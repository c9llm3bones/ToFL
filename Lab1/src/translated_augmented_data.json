[
    {
        "question": "What is Glushkov's construction algorithm?",
        "answer": "\nIn formal language theory, Glushkov's algorithm transforms a given regular expression into an equivalent nondeterministic finite automaton (NFA).\nThe NFA can then be made deterministic and then minimized to obtain an optimal automaton corresponding to the given regular expression.\nGlushkov's algorithm works as follows:\n1) Linearize the expression. Each letter of the alphabet appearing in the expression e is renamed so that each letter appears at most once in the new expression.\n2) Create a starting state, which is a transition for each symbol of the alphabet of the language.\n3) For each state and each symbol of the alphabet of the language, a transition to the next state is determined, if this is possible according to the language.\nIf the transition is not possible, a new transition to the new state is created.\nIf the transition to the new state is possible, a transition to it is created.\n4) The algorithm continues until a final state is reached, which is the acceptance state for the language."
    },
    {
        "question": "What is Glushkov's construction algorithm?",
        "answer": "\nIn formal language theory, Glushkov's algorithm transforms a given regular expression into an equivalent nondeterministic finite automaton (NFA).\nThe NFA can then be made deterministic and then minimized to obtain an optimal automaton corresponding to the given regular expression.\nGlushkov's algorithm works as follows:\n1) Linearize the expression. Each letter of the alphabet occurring in the expression e is renamed such that each letter occurs at most once in the new expression.\n2) Create an initial state that is a transition for each symbol of the alphabet of the language.\n3) For each state and each symbol of the alphabet of the language, a transition to the next state is determined if this is possible according to the language.\nIf the transition is not possible, a new transition to the new state is created.\nIf the transition to the new state is possible, a transition to it is created.\n4) The algorithm continues until a final state is reached that is an acceptable state for the language."
    },
    {
        "question": "What is Glushkov's construction algorithm?",
        "answer": "\nIn formal language theory, Glushkov's algorithm transforms a given regular expression into an equivalent nondeterministic finite automaton (NFA).\nThe NFA can then be made deterministic and then minimized to obtain an optimal automaton corresponding to the given regular expression.\nGlushkov's algorithm works as follows:\n1) Linearize the expression. Each letter of the alphabet in the expression e is renamed such that each letter appears in the new expression at most once.\n2) Create an initial state that is a transition for each symbol of the alphabet of the language.\n3) For each state and each character of the alphabet of the language, a transition to the next state is determined, if this is possible according to the language.\nIf the transition is impossible, a new transition to the new state is created.\nIf the transition to the new state is possible, a transition to it is created.\n4) The algorithm continues until a final state is reached that is acceptable for the language."
    },
    {
        "question": "What is Glushkov's construction algorithm?",
        "answer": "\nIn formal language theory, Glushkov's algorithm transforms a given regular expression into an equivalent nondeterministic finite automaton (NFA).\nThe AFN can then be made deterministic and then minimized to obtain an optimal automaton corresponding to the given regular expression.\nGlushkov's algorithm works as follows:\n1) Linearize the expression. Each letter of the alphabet in the expression e is renamed such that each letter appears in the new expression at most once.\n2) Create an initial state that is a transition for each symbol of the language alphabet.\n3) For each state and each symbol of the language alphabet, a transition to the next state is determined, if possible depending on the language.\nIf the transition is impossible, a new transition to the new state is created.\nIf the transition to the new state is possible, a transition to it is created.\n4) The algorithm continues until the final state is reached, which is the acceptance state of the language."
    },
    {
        "question": "Give a definition of a finite deterministic recognizer",
        "answer": "\nA finite deterministic recognizer (FDR) is a finite automaton that accepts or rejects a given string of symbols by passing through a sequence of states defined by the string.\n\nA deterministic recognizer is defined by a quintuplet M = (Q, Σ, δ, q0, F), where:\n1. Q = {q0,…, qm} is a finite set of states;\n2. Σ = {a0,…, an} is a finite set of input symbols (input alphabet);\n3. δ(x,y) is the transition function δ: QХΣ→Q;\n4. q0 ∈ Q is the initial state;\n5. F ⊂ Q is a subset of the final states.\n\nIn a deterministic automaton, there can be only one transition from state qi to letter a.\n\nThe DKR functions as follows:\n1) At the initial moment of time, being in the state q0, it examines the leftmost symbol of the input string.\n2) The string is read from left to right, without returns, shifting to the right by one symbol for each step of the automaton's operation.\n3) In accordance with the specified transition function, the automaton changes its internal states in the process of reading the input string.\n4) Having read it to the end, the automaton ends up either in one of the final states (in which case the string will be accepted or recognized by the automaton), or in some other, non-final state (in which case the string will be rejected)."
    },
    {
        "question": "Give the definition of a finite deterministic recognizer.",
        "answer": "\nA finite deterministic recognizer (FDR) is a finite automaton that accepts or rejects a given string of symbols by going through a sequence of states determined by the string.\n\nA deterministic recognizer is defined by a quinteplet M = (Q, Σ, δ, q0, F), where:\n1. Q = {q0,…, qm} is a finite set of states;\n2. Σ = {a0,…, an} is a finite set of input symbols (input alphabet);\n3. δ(x,y) is the transition function δ: QХΣ→Q;\n4. q0 ∈ Q is the initial state;\n5. F ⊂ Q is a subset of the final states.\n\nThere can be only one transition from state qi to letter a in a deterministic automaton.\n\nThe DKR functions as follows:\n1) At the initial moment of time, being in the state q0, it looks at the leftmost symbol of the input string.\n2) The string is read from left to right, without returns, shifting to the right by one symbol for each step of the automaton's operation.\n3) In accordance with the specified transition function, the automaton changes its internal states in the process of reading the input string.\n4) Having read it to the end, the automaton either gets into one of the final states (in this case, the string will be accepted or recognized by the automaton), or into some other, non-final state (in this case, the string will be rejected)."
    },
    {
        "question": "Define a finite deterministic recognizer.",
        "answer": "\nA finite deterministic resolver (FDR) is a finite state machine that accepts or rejects a given string by passing through a sequence of states defined by that string.\n\nA deterministic resolver is defined by a quintet M = (Q, Σ, δ, q0, F), where:\n1. Q = {q0,…, qm} is a finite set of states;\n2. Σ = {a0,…, an} is a finite set of input symbols (input alphabet);\n3. δ(x,y) is the transition function δ: QХΣ→Q;\n4. q0 ∈ Q is the initial state;\n5. F ⊂ Q is a subset of the final states.\n\nThere can only be one transition from the state qi to the letter a in a deterministic automaton.\n\nThe DKR works as follows:\n1) At the initial moment of time, being in the state q0, it checks the leftmost symbol of the input string.\n2) The string is read from left to right, without returns, shifting to the right by one symbol for each step of the machine's operation.\n3) According to the given transition function, the machine changes its internal states in the process of reading the input string.\n4) Having read it to the end, the machine ends up either in one of the final states (in this case, the string will be accepted or recognized by the machine), or in another non-final state (in this case, the string will be accepted or recognized by the machine). the string will be rejected)."
    },
    {
        "question": "Give a definition of a finite deterministic recognition device.",
        "answer": "\nA finite deterministic recognizer (FDR) is a finite automaton that accepts or rejects a given string of symbols by going through a sequence of states defined by that string.\n\nA FDR is defined by a quintuplet M = (Q, Σ, δ, q0, F), where:\n1. Q = {q0,…, qm} is a finite set of states;\n2. Σ = {a0,…, an} is a finite set of input symbols (input alphabet);\n3. δ(x,y) is the transition function δ: QХΣ→Q;\n4. q0 ∈ Q is the initial state;\n5. F ⊂ Q is a subset of the final states.\n\nThere can only be one transition from the state qi to the letter a in a deterministic automaton.\n\nThe DKR works as follows:\n1) At the initial moment of time, being in the state q0, it checks the leftmost symbol of the input string.\n2) The string is read from left to right, without returning, shifting to the right by one symbol at each stage of the automaton's operation.\n3) According to the specified transition function, the automaton changes its internal states in the process of reading the input string.\n4) Having read it to the end, the automaton either gets into one of the final states (in this case, the string will be accepted or recognized by the automaton), or into another non-final state (in this case, the channel will be rejected)."
    },
    {
        "question": "Give a definition of a finite non-deterministic recognizer",
        "answer": "\nA non-deterministic finite recognizer (NFR) is a deterministic finite automaton that does not satisfy the following conditions:\n1) Any of its transitions is uniquely determined by the current state and the input symbol\n2) Reading the input symbol is required for each change of state.\nIn particular, any DFA is also a NFA.\n\nA non-deterministic recognizer is defined by a quintuplet M = (Q, Σ, δ, q0, F), where:\n1. Q = {q0,…, qm} is a finite set of states;\n2. Σ = {a0,…, an} is a finite set of input symbols (input alphabet);\n3. δ(x,y) is the transition function δ: QХΣ→P(Q), where P(Q) is the set of all subsets of states;\n4. q0 ∈ Q is the initial state;\n5. F ⊂ Q is a subset of final states.\n\nIn a non-deterministic automaton, there can be several transitions from state qi to letter a.\n\nThe NCR functions as follows:\n1) The machine starts from a certain initial state q0 and reads a string of symbols.\n2) For each input symbol, the recognizer moves to a new state until it processes all input symbols.\n3) At each step, the automaton randomly selects one of the possible transitions.\n4) If there is a “lucky pass”, that is, some sequence of choices that leads to the final state after the complete selection of the input string, then the string is accepted.\n\nIf there is no sequence that leads the automaton to the final state after processing the entire input string, then the input string is rejected."
    },
    {
        "question": "Give a definition of a finite non-deterministic recognizer.",
        "answer": "\nA non-deterministic finite recognizer (NFA) is a deterministic finite automaton that does not satisfy the following conditions:\n1) Any of its transitions is uniquely determined by the current state and the input symbol\n2) Each change of state requires reading the input symbol.\nIn particular, any DFA is also an NFA.\n\nA non-deterministic recognizer is defined by a quintuplet M = (Q, Σ, δ, q0, F), where:\n1. Q = {q0,…, qm} is a finite set of states;\n2. Σ = {a0,…, an} is a finite set of input symbols (input alphabet);\n3. δ(x,y) is the transition function δ: QХΣ→P(Q), where P(Q) is the set of all subsets of states;\n4. q0 ∈ Q is the initial state;\n5. F ⊂ Q is a subset of final states.\n\nThere can be several transitions from state qi to letter a in a non-deterministic automaton.\n\nNCR functions as follows:\n1) The machine starts from some initial state q0 and reads a string of symbols.\n2) For each input symbol, the recognizer moves to a new state until it processes all input symbols.\n3) At each step, the automaton randomly selects one of the possible transitions.\n4) If there is a \"lucky pass\", that is, some sequence of choices that leads to the final state after the entire input string has been selected, then the string is accepted.\n\nIf there is no sequence that leads the automaton to the final state after processing the entire input string, then the input string is rejected."
    },
    {
        "question": "Define a nondeterministic finite recognizer.",
        "answer": "\nA nondeterministic finite state recognition system (NFR) is a deterministic finite automaton that does not satisfy the following conditions:\n1) Every transition is uniquely determined by the current state and the input symbol\n2) Every change of state requires reading the input symbol.\nIn particular, any DFA is also an NFA.\n\nA nondeterministic recognizer is defined by a quintuplet M = (Q, Σ, δ, q0, F), where:\n1. Q = {q0,…, qm} is a finite set of states;\n2. Σ = {a0,…, an} is a finite set of input symbols (input alphabet);\n3. δ(x,y) is the transition function δ: QХΣ→P(Q), where P(Q) is the set of all subsets of states;\n4. q0 ∈ Q is the initial state;\n5. F ⊂ Q is a subset of final states.\n\nThere can be several transitions from state qi to letter a in a non-deterministic automaton.\n\nThe NCR works as follows:\n1) The machine starts from the initial state q0 and reads a string of characters.\n2) For each input character, the recognizer moves to a new state until it processes all input characters.\n3) At each step, the machine randomly chooses one of the possible transitions.\n4) If there is a \"lucky pass\", that is, a sequence of choices that leads to the final state after the input string has been completely chosen, then the string is accepted.\n\nIf there is no sequence that leads the machine to the final state after processing the entire input string, then the input string is discarded."
    },
    {
        "question": "Give a definition of a finite non-deterministic recognition device.",
        "answer": "\nA nondeterministic finite automaton (NFA) is a deterministic finite automaton that does not satisfy the following conditions:\n1) Each of its transitions is uniquely determined by the current state and the input symbol\n2) Reading the input symbol is required for each state change.\nIn particular, any DFA is also an NFA.\n\nA nondeterministic finite automaton is defined by a quintuplet M = (Q, Σ, δ, q0, F), where:\n1. Q = {q0,…, qm} is a finite set of states;\n2. Σ = {a0,…, an} is a finite set of input symbols (input alphabet);\n3. δ(x,y) is the transition function δ: QХΣ→P(Q), where P(Q) is the set of all subsets of states;\n4. q0 ∈ Q is the initial state;\n5. F ⊂ Q is a subset of final states.\n\nThere can be several transitions from state qi to letter a in a non-deterministic automaton.\n\nThe NCR works as follows:\n1) The machine starts from some initial state q0 and reads a string of symbols.\n2) For each input symbol, the recognizer moves to a new state until it processes all input symbols.\n3) At each step, the automaton randomly selects one of the possible transitions.\n4) If there is a \"randomness\", i.e. a sequence of choices leading to a final state after the entire input string has been selected, then the string is accepted.\n\nIf there is no sequence that takes the automaton to a final state after processing the entire input string, then the input string is discarded."
    },
    {
        "question": "Prove the following theorem: If a language L is recognized by a non-deterministic recognizer M, then there is a deterministic recognizer M’ that recognizes the language L.",
        "answer": "\nProof:\nLet M=(Q,∑,δ,q0,F) be a non-deterministic recognizer. Given a non-deterministic recognizer M, we construct a deterministic recognizer M’=( Q’ ,∑, δ’, q’0, F) as follows:\n1. Q’ is the set of all subsets of Q;\n2. The input alphabet M’ coincides with the alphabet of M, that is, ∑;\n3. The transition function δ’ is defined as follows: δ’(O,a)= {q’| δ(q,a) q∈O, where O∈ Q’}. Obviously, the function δ’ is defined on all subsets O⊆Q;\n4. q’0 ={ q’0} – initial state\n5. final states are a subset O∈Q’ such that if q∈O then q∈F. which we denote\nas 2Q;\n\nThe transition function for any value of the arguments has one value δ'(O,a), which is a subset of states. Let us show that the constructed deterministic automaton M’ recognizes the language L.\n\nFirst, let us prove a more general fact: the recognizer M from the state qi in n cycles goes to the final state qf(qiw├n qfe), if and only if the recognizer M’ from some state Oi such that qi∈Oi goes to the final state Of in n cycles. We write this statement as follows:\n(qiw├n qfe) ⇔ Oiw├nOfe and qi∈Oi, qf ∈ Of.\n\nWe will prove it by induction on the length of the word w.\nLet the length of the word |w|=1 (a one-letter word w=a). Suppose that the recognizer M comes to the final state in one cycle for some input word consisting of one symbol a, which means that qa├qfe, i.e. qf ∈δ(qi,a). Let us consider how M’ will work on the same word. Let us take a subset {q}.\n\nAccording to our definition of the transition function, qf∈δ'({q},a)=O. According to the definition of final states in M’, the state δ'({q},a) is a final state. That is, the word is recognized.\n\nLet the statement be true for words of length |w|=n, let us show that it is true for words of length n+1.\nSuppose that M recognizes the word |aw|= n+1, that is, after n+1 steps it comes to the final state qiaw├n+1 qfe. Consider w. Its length is n. According to the inductive hypothesis, M’ comes to the final state from some configuration <Ok,w>, since qkw├n qfe. Moreover, qk ∈ Ok. Let us take the preimage Ok. If the automaton M recognizes it, then by the inductive hypothesis it is also recognized by M’.\nOkw├n Ofe and Oiaw├Okw and Okw├n Ofe, which is what was required to be proved.\n\nLet us prove the statement in the opposite direction.\nLet the word w=a be recognized by the automaton M’ in one step, Oiа├ Ofe, then in qf∈Of , then there is q∈Qi such that qf∈δ(q,a) and qa├n qfe\n\nLet the word |aw|=n+1 and be recognized by the automaton M’ in n+1 steps Oiаw├n+1 Ofe, then Okw├ nOfe. Then, according to the inductive hypothesis, it is true that qkw├n qfe for some qk∈Ok. According to the definition of δ’, we have that Ok=δ’(Oi,a) and qk = δ’(qi,a). Then qiaw├ qkw and qkw├n qfe , which corresponds to qiaw├n+1 qfe.\nAs qi, we can choose q0, therefore, the theorem is proved."
    },
    {
        "question": "Prove the following theorem: If a language L is recognized by a non-deterministic recognizer M, then there exists a deterministic recognizer M' that recognizes the language L.",
        "answer": "\nProof:\nLet M=(Q,∑,δ,q0,F) be a non-deterministic recognizer. For a non-deterministic recognizer M, we construct a deterministic recognizer M’=( Q’ ,∑, δ’, q’0, F) as follows:\n1. Q’ is the set of all subsets of Q;\n2. The input alphabet of M’ coincides with the alphabet of M, that is, ∑;\n3. The transition function δ’ is defined as follows: δ’(O,a)= {q’| δ(q,a) q∈O, where O∈ Q’}. Obviously, the function δ’ is defined on all subsets O⊆Q;\n4. q’0 ={ q’0} – initial state\n5. final states are a subset O∈Q’ such that if q∈O, then q∈F. which we denote\nas 2Q;\n\nThe transition function for any value of the arguments has one value δ'(O,a), which is a subset of states. Let us show that the constructed deterministic automaton M’ recognizes the language L.\n\nFirst, we will prove a more general fact: the recognizer M from the state qi in n cycles goes to the final state qf(qiwÜn qfe), if and only if the recognizer M’ from some state Oi such that qi∈Oi goes to the final state Of in n cycles. We write this statement as follows:\n(qiw├n qfe) ⇔ Oiw├nOfe and qi∈Oi, qf ∈ Of.\n\nWe prove by induction on the length of the word w.\nLet the length of the word |w|=1 (a one-letter word w=a). Suppose that the recognizer M comes to the final state in one cycle for some input word consisting of one symbol a, which means that qa├qfe, i.e. qf ∈δ(qi,a). Let's consider how M' will work on the same word. Let's take a subset of {q}.\n\nAccording to our definition of the transition function, qf∈δ'({q},a)=O. According to the definition of final states in M', the state δ'({q},a) is a final state. That is, the word is recognized.\n\nLet's say the statement is true for words of length |w|=n, we will show that it is true for words of length n+1.\nSuppose that M recognizes the word |aw|= n+1, i.e. in n+1 steps it arrives at the final state qiaw├n+1 qfe. Consider w. Its length is n. According to the inductive hypothesis, M’ arrives at the final state from some configuration <Ok,w>, since qkw├n qfe. Moreover, qk ∈ Ok. Take the preimage of Ok. If the automaton M recognizes it, then by the inductive hypothesis it is also recognized by M’.\nOkw├n Ofe and Oiaw├Okw and Okw├n Ofe, which is what was required to be proved.\n\nLet us prove the statement in the opposite direction.\nLet the word w=a be recognized by the automaton M’ in one step, Oiа├ Ofe, then in qf∈Of , then there exists q∈Qi such that qf∈δ(q,a) and qa├n qfe\n\nLet the word |aw|=n+1 and be recognized by the automaton M’ in n+1 steps Oiаw├n+1 Ofe, then Okw├ nOfe. Then, according to the inductive hypothesis, it is true that qkw├n qfe for some qk∈Ok. According to the definition of δ’, we have Ok=δ’(Oi,a) and qk = δ’(qi,a). Then qiaw├ qkw and qkw├n qfe , which corresponds to qiaw├n+1 qfe.\nAs qi, we can choose q0, therefore, the theorem is proved."
    },
    {
        "question": "Prove the following theorem: If a language L is recognized by a non-deterministic recognizer M, then there exists a deterministic recognizer M' that recognizes the language L.",
        "answer": "\nProof:\nLet M=(Q,∑,δ,q0,F) be a non-deterministic recognizer. For a non-deterministic recognizer M, we construct a deterministic recognizer M’=( Q’, ∑, δ’, q’0, F) as follows:\n1. Q’ is the set of all subsets of Q;\n2. The input alphabet M’ coincides with the alphabet of M, i.e. ∑;\n3. The transition function δ’ is defined as follows: δ’(O,a)= {q’| δ(q,a) q ∈ O, where O ∈ Q’}. Obviously, the function δ’ is defined on all subsets O⊆Q;\n4. q’0 = { q’0} – initial state\n5. final states are a subset OεQ’ such that if qεO, then qεF. which we denote\nas the 2nd quarter;\n\nThe transition function for any value of the argument has the value δ'(O,a), which is a subset of states. Let us show that the constructed deterministic automaton M’ recognizes the language L.\n\nFirst, we will prove a more general fact: the recognizer M of the state qi in n cycles goes to the final state qf(qiwÜn qfe) if and only if the recognizer M' of the state Oi such that qiεOi goes to the final state in n beats. We will write this statement as follows:\n(qiw├n qfe) ⇔ Oiw├nOfe and qiεOi, qf ∈ Of.\n\nWe will prove this by induction on the length of the word w.\nLet the length of a word be |w|=1 (a one-letter word w=a). Suppose that the recognition module M reaches the final state in one cycle for an input word consisting of the symbol a, which means that qaÃqfe, i.e., qf ∈δ(qi,a ). Let us see how M' will work with the same word. Consider the subset {q}.\n\nBy our definition of the transition function, qfεδ'({q},a)=O. By the definition of final states in M', the state δ'({q},a) is a final state. In other words, the word is recognizable.\n\nSuppose that the statement is true for words of length |w|=n, we will show that it is true for words of length n+1.\nSuppose that M recognizes the word |aw|= n+1, i.e., in n+1 steps it reaches the final state qiawÃn+1 qfe. Consider w. Its length is n. By the inductive hypothesis, M' comes to the final state from the configuration <Ok,w>, since qkwÃn qfe. Moreover, qk ∈ Ok. Let's take a prototype Ok. If the automaton M recognizes it, then by the inductive hypothesis, so does M'.\nOkw├n Ofe and Oiaw├Okw and Okw├n Ofe, that's what needed to be proved.\n\nLet's prove the converse.\nLet the word w=a be recognized by the automaton M' in one step, OiaÃ Ofe, then in qfεOf there exists qεQi such that qfεδ(q,a) and qaÜn qfe\n\nLet the word |aw|=n+1 be recognized by the automaton M' in n+1, and not in OiawÃn+1 Ofe, then OkwÃ nOfe. Then by the inductive hypothesis it is true that qkwÃn qfe for some qkεOk. By the definition of δ’ we have Ok=δ’(Oi,a) and qk = δ’(qi,a). Then qiawÃ qkw and qkwÃn qfe, which corresponds to qiawÃn+1 qfe.\nWe can choose q0 as qi, thus proving the theorem."
    },
    {
        "question": "Prove the following theorem: If a language L is recognized by a non-deterministic recognizer M, then there exists a deterministic recognizer M’ that recognizes the language L.",
        "answer": "\nProof:\nLet M=(Q,∑,δ,q0,F) be a non-deterministic recognizer. Given a non-deterministic recognizer M, we construct a deterministic recognizer M’=( Q’, ∑, δ’, q’0, F) as follows:\n1. Q’ is the set of all subsets of Q;\n2. The input alphabet M’ coincides with the alphabet of M, i.e. ∑;\n3. The transition function δ’ is defined as follows: δ’(O,a)= {q’| δ(q,a) q ∈ O, where O ∈ Q’}. Obviously, the function δ’ is defined on all subsets O⊆Q;\n4. q’0 = { q’0} – initial state\n5. final states are a subset OεQ’ such that if qεO, then qεF. which we denote\nby the 2nd quarter;\n\nThe transition function for any value of the arguments has the value δ'(O,a), which is a subset of states. Let us show that the constructed deterministic automaton M’ recognizes the language L.\n\nFirst of all, let us prove a more general fact: the recognizer M of the state qi in n cycles goes to the final state qf(qiwÃn qfe) if and only if the recognizer M' of the state Oi is such that qi ∈Oi goes to the final state Of in n cycles. We write this statement as follows:\n(qiw├n qfe) ⇔ Oiw├nOfe and qiεOi, qf ∈ Of.\n\nWe prove this by induction on the length of the word w.\nLet the length of the word be |w|=1 (a one-letter word w=a). Suppose that the recognizer M arrives at the final state in one cycle for an input word consisting of the symbol a, which means that qaÃqfe, i.e., qf ∈δ(qi, has). Let's see how M' will work with the same word. Take a subset {q}.\n\nBy our definition of the transition function, qfεδ'({q},a)=O. By the definition of final states in M', the state δ'({q},a) is a final state. That is, the word is recognizable.\n\nLet the statement be true for words of length |w|=n, we will show that it is true for words of length n+1.\nSuppose that M recognizes the word |aw|= n+1, that is, after n+1 steps it reaches the final state qiawÃn+1 qfe. Consider w. Its length is n. According to the inductive hypothesis, M' comes to the final state from the configuration <Ok,w>, since qkwÃn qfe. Moreover, qk ∈ Ok. Take the preimage Ok. If the automaton M recognizes it, then by the inductive hypothesis M' recognizes it too.\nOkwÃn Ofe and OiawÃOkw and OkwÃn Ofe, which is what was required to be proved.\n\nLet us prove the opposite statement.\nLet the word w=a be recognized by the automaton M' in one step OiаÃ Ofe, then in qfεOf there exists qεQi such that qfεδ(q,a) and qaÜn qfe\n\nLet the word |aw|=n+1 be recognized by the automaton M' in n+1 steps OiаwÃn+1 Ofe, then OkwÃ nOfe. Then by the inductive hypothesis it is true that qkwÃn qfe for qkεOk. By the definition of δ’ we have Ok=δ’(Oi,a) and qk = δ’(qi,a). Then qiawÃ qkw and qkwÃn qfe , which corresponds to qiawÃn+1 qfe.\nLike qi, we can choose q0, so the theorem is proved."
    },
    {
        "question": "Give a definition of two-level grammar",
        "answer": "\nA two-level grammar is a formal grammar that is used to generate another formal grammar, for example one with an infinite set of rules.\n\nA two-level grammar is a triple <G1, G2, z>\nG1 = <N1, Σ1, P1>\nG2 = <N2, Σ2, P2>\nN2 has the form ⟨h⟩, where h ∈ (N1 ∪ Σ1)* and ⟨z⟩ ∈ Σ1*\n\nMetasystem:\nG1= <N1, Σ1, P1> is a wide system that includes the system under study as a component/system that is a superstructure, a description for another system, where\nN1 is a set of meta-non-terminals\nΣ1 is a set of meta-non-terminals\nP1 is CS – meta-rules\n\nObject system G2= <N2, Σ2, P2>, where\nN2 is a set of object terminals\nΣ2 is a set of object non-terminals ⟨𝑤⟩where 𝑤 ∈ (𝑁1 ∪ 𝛴1)*\nP2 - KS –object rules\nLet the rule from P2 Y1, Y1 ∈ N1\n⟨w⟩ → h1. . . hn\nStructural rules:\nF(Y1) ∈ L(G1, Y1)\nF(⟨w⟩) → F(h1. . . hn)\n\nConclusion: X ⇒ Yiff ∃P,Q(N2 ∪ Σ2)*: X = PXQ , Y = PYQ\n\nExample:\n1) G1 ={A, BI, {a,s}, {A → aA|a, B → A},\n2) G2={<s>, <A>, <B>}, {a}, {<s> → < A>) ,< A> → a, <B> → a }"
    },
    {
        "question": "Give a definition of two-level grammar",
        "answer": "\nA two-level grammar is a formal grammar that is used to generate another formal grammar, for example one with an infinite set of rules.\n\nA two-level grammar is a triple <G1, G2, z>\nG1 = <N1, Σ1, P1>\nG2 = <N2, Σ2, P2>\nN2 is of the form ⟨h⟩, where h ∈ (N1 ∪ Σ1)* and ⟨z⟩ ∈ Σ1*\n\nMetasystem:\nG1= <N1, Σ1, P1> is a wide system that includes the system under study as a component/system that is a superstructure, a description for another system, where\nN1 is a set of meta-non-terminals\nΣ1 is a set of meta-non-terminals\nP1 is CS — metarules\n\nObject system G2= <N2, Σ2, P2>, where\nN2 is a set of object terminals\nΣ2 is a set of object non-terminals ⟨𝑤⟩where 𝑤 ∈ (𝑁1 ∪ 𝛴1)*\nP2 - KS –object rules\nLet the rule from P2 Y1, Y1 ∈ N1\n⟨w⟩ → h1. . . hn\nStructural rules:\nF(Y1) ∈ L(G1, Y1)\nF(⟨w⟩) → F(h1. . . hn)\n\nConclusion: X ⇒ Y if ∃P,Q(N2 ∪ Σ2)*: X = PXQ , Y = PYQ\n\nExample:\n1) G1 ={A, BI, {a,s}, {A → aA|a, B → A},\n2) G2={<s>, <A>, <B>}, {a}, {<s> → < A>) ,< A> → a, <B> → a }"
    },
    {
        "question": "Definition of a two-tier grammar",
        "answer": "\nA two-level grammar is a formal grammar that is used to construct another formal grammar, such as a grammar with an infinite set of rules.\n\nA two-level grammar is a triple <G1, G2, z>\nG1 = <N1, Σ1, P1>\nG2 = <N2, Σ2, P2>\nN2 has the form ⟨h⟩, where h ∈ (N1 ∪ Σ1)* and ⟨z⟩ ∈ Σ1*\n\nMetasystem:\nG1= <N1, Σ1, P1> is a wide system that includes the system under study as a component/system that is a superstructure, a description of another system, where\nN1 is a set of meta-terminals\nΣ1 is a set of meta-terminals\nP1 is CS – meta-rules\n\nObject system G2= <N2, Σ2, P2>, where\nN2 is a set of object terminals\nΣ2 is a set of non-terminals of objects ⟨𝑤⟩, where 𝑤 ∈ (𝑁1 ∪ 𝛴1)*\nP2 - KS – rules of the object\nLet the rule P2 Y1, Y1 ∈ N1\n⟨w⟩ → h1. . . хн\nStructural rules:\nF(Y1) ∈ L(G1, Y1)\nF(⟨w⟩) → F(h1...hn)\n\nConclusion: X ⇒ Y if ∃P,Q(N2 ∪ Σ2)*:\n\nExample:\n1) G1 = {A, BI, {a,s}, {A → aA|a, B → A},\n2) G2={<s>, <A>, <B>}, {a}, {<s> → <A>), <A> →a, <B> →a}"
    },
    {
        "question": "Define grammar on two levels.",
        "answer": "\nA two-level grammar is a formal grammar that is used to construct another formal grammar, such as a grammar with an infinite set of rules.\n\nA two-level grammar is a triple <G1, G2, z>\nG1 = <N1, Σ1, P1>\nG2 = <N2, Σ2, P2>\nN2 is of the form ⟨h⟩, where h ∈ (N1 ∪ Σ1)* and ⟨z⟩ ∈ Σ1*\n\nMetasystem:\nG1= <N1, Σ1, P1> is a wide system that includes the system under study as a component/system that is a superstructure, a description for another system, where\nN1 is a set of meta-nonterminals\nΣ1 is a set of meta-nonterminals\nP1 – CS – metarules\n\nObject system G2= <N2, Σ2, P2>, where\nN2 is a set of object terminals\nΣ2 is a set of object nonterminals ⟨𝑤⟩, where 𝑤 ∈ (𝑁1 ∪ 𝛴1)*\nP2 - KS – rules of the object\nLet the rule P2 — Y1, Y1 ∈ N1\n⟨w⟩ → h1. . . хн\nStructural rules:\nF(Y1) ∈ L(G1, Y1)\nF(⟨w⟩) → F(h1...hn)\n\nOutput: X ⇒ Yiff ∃P,Q(N2 ∪ Σ2)*:\n\nExample:\n1) G1 = {A, BI, {a,s}, {A → aA|a, B → A},\n2) G2={<s>, <A>, <B>}, {a}, {<s> → <A>), <A> → a, <B> → a}"
    },
    {
        "question": "The expansion theorem for regular languages ​​is",
        "answer": "\nThe expansion theorem, also known as the pumping lemma for regular languages, states that for any regular language L there exists a constant p >= 1 such that any word w in L of length at least p can be written as:\nw = xyz, where:\n1. x, y, z ∈ ∑*,\n2. |xy| <= p,\n3. y ≠ ε,\n4. For any k >= 0, the string x(y^k)z is also in L.\n\nPurpose of the theorem:\nThis theorem is used to prove that a certain language is not regular. For example, if it can be shown that no decomposition of w into xyz satisfies the conditions of the pumping lemma, then L cannot be regular.\n- The expansion theorem is concerned with the conditions under which a string can be \"pumped\" while still being in the language.\n\nThere is a simple method for checking whether a given language is regular. This method is based on checking the Lemma of Expansion of a Regular Language. It is proved that if for some given \nlanguage the Lemma of Expansion of a Regular Language holds, then this language is regular; if the lemma does not hold, then the language is not regular. \nThe Lemma of Expansion or Pumping (Pumping Lemma) for regular languages ​​is formulated as follows: if L is a regular language and α is a sufficiently long chain of symbols belonging to this language, then in this chain one can find a non-empty subchain that can be repeated arbitrarily many times, and all new chains obtained in this way will belong to the same regular language, the original chain \"expands\" - hence the name \"Pumping Lemma\"). The Lemma of Expansion makes sense only for infinite regular languages."
    },
    {
        "question": "The extension theorem for regular languages ​​is",
        "answer": "\nThe extension theorem, also known as the pumping lemma for regular languages, states that for any regular language L there is a constant p >= 1 such that any word w in L of length at least p can be written as:\nw = xyz, where:\n1. x, y, z ∈ ∑*,\n2. |xy| <= p,\n3. y ≠ ε,\n4. For any k >= 0, the string x(y^k)z also belongs to L.\n\nGoal of the theorem:\nThis theorem is used to prove that a certain language is not regular. For example, if it can be shown that no decomposition of w in xyz satisfies the conditions of the pumping lemma, then L cannot be regular.\n- The extension theorem concerns conditions under which a string can be \"pumped\" while still being in the language.\n\nThere is a simple method for testing whether a given language is regular. This method is based on checking the lemma on the extension of a regular language. It is proved that if for some given language the lemma on the extension of a regular language is true, then this language is regular; if the lemma is not true, then the language is not regular. \nThe lemma on the extension or pumping (the lemma on the pumping) for regular languages ​​is formulated as follows: if L is a regular language and α is a sufficiently long chain of symbols belonging to this language, then in this chain one can find a non-empty subchain that can be repeated arbitrarily many times, and all new chains obtained in this way will belong to the same regular language, the original chain is \"extended\" - hence the name \"the lemma on the pumping\"). The lemma on the extension makes sense only for infinite regular languages."
    },
    {
        "question": "The extension theorem for regular languages ​​is",
        "answer": "\nThe extension theorem, also known as the pumping lemma for regular languages, states that for any regular language L there exists a constant p >= 1 such that every word w in L of length at least p can be written as:\nw = xyz, where:\n1. x, y, z ∈ ∑*,\n2. |ху| <= р,\n3. y ≠ ε,\n4. For all k >= 0, the line x(y^k)z also belongs to L.\n\nGoal of the theorem:\nThis theorem is used to prove that a certain language is not regular. For example, if it can be shown that no decomposition of w in xyz satisfies the conditions of the pumping lemma, then L cannot be regular.\n— The extension theorem concerns conditions under which a string can be \"pumped\" and still remain in the language.\n\nThere is a simple way to check whether a given language is regular. This method is based on checking the lemma on the extension of a regular language. It is proved that if the lemma on the extension of a regular language is true for a given language, then this language is regular; if the lemma is false, then the language is not regular. \nThe lemma on the extension or pumping (the lemma on the pumping) for regular languages ​​is formulated as follows: if L is a regular language and α is a sufficiently long chain of symbols belonging to this language, then in this chain one can find a non-empty substring that can be repeated any number of times, and all new strings obtained in this way will belong to the same regular language, the original string is \"extended\" - hence the name \"the lemma on the pumping\"). The lemma on the extension makes sense only for infinite regular languages."
    },
    {
        "question": "The decomposition theorem for regular languages ​​is:",
        "answer": "\nThe extension theorem, also known as the pumping lemma for regular languages, states that for any regular language L there is a constant p >= 1 such that every word w in L of length at least p can be written as:\nw = xyz, where:\n1. x, y, z ∈ ∑*,\n2. |хy| <= р,\n3. y ≠ ε,\n4. For all k >= 0, the string x(y^k)z is also in L.\n\nGoal of the theorem:\nThis theorem is used to prove that a certain language is not regular. For example, if it can be shown that no decomposition of w into xyz satisfies the conditions of the pumping lemma, then L cannot be regular.\n— The decomposition theorem concerns conditions under which a string can be \"pumped\" and still be in the language.\n\nThere is a simple way to check whether a given language is regular. This method is based on the verification of the expansion lemma of a regular language. It is proved that if the decomposition lemma of a regular language is true for a given language, then this language is regular; if the lemma is false, then the language is not regular. The expansion or pumping lemma for regular languages ​​is formulated as follows: if L is a regular language and α is a sufficiently long chain of symbols belonging to this language, then in this chain one can find a sub-nonempty string that can be repeated arbitrarily many times, and all new strings obtained in this way will belong to the same regular language, the original string is \"expanded\" - hence the name \"Pumping Lemma\"). The expansion lemma makes sense only for infinite regular languages."
    },
    {
        "question": "Simple precedence is",
        "answer": "\nA simple precedence grammar is a class of context-free grammars for which a bottom-up parser can be built using precedence relations between the symbols of the grammar. The basic idea of ​​this method is to use binary relations between symbols to determine the order in which the output rules are applied.\n\nPrecedence relations:\nFor a simple precedence grammar, three key relations are defined between terminal symbols:\n1. Less than (≺): a ≺ b if terminal a immediately precedes b in the output.\n2. Equal to (=): a = b if the two symbols are adjacent and part of the same subexpression.\n3. Greater than (≻): a ≻ b if symbol a terminates a part of the output and symbol b follows.\n\nThe base of a chain in a simple precedence grammar is the smallest subexpression that respects the ordering relations, on which other parts of the chain can be \"pumped\". For example, for an arithmetic expression like a+b×c , the subexpression b×c can be considered the base of the chain, since the multiplication operation (with higher precedence) precedes the addition. This subexpression is preserved in any subsequent part of the expression and determines how the remaining components will be built.\n\nRelations are used to build a precedence table, which helps in the process of syntactic analysis.\n\nBuilding a simple precedence grammar\n1) The grammar must be simplified - in particular, left-recursive rules must be removed. Left-recursion creates cycles in the grammar, which makes the use of precedence relations impossible, since such cycles violate the strict ordering between symbols.\n2) All pairs of adjacent terminals between which a relation must be established are determined. 3) Based on these pairs, a precedence table is built.\n\nSimple precedence grammars are a powerful tool for creating parsers. They make it easier to build bottom-up parsers by exploiting the relationships between symbols, but are only suitable for a certain type of grammar."
    },
    {
        "question": "Simple priority",
        "answer": "\nA simple precedence grammar is a class of context-free grammars for which a bottom-up parser can be built using precedence relations between the symbols of the grammar. The basic idea of ​​this method is to use binary relations between symbols to determine the order in which the output rules are applied.\n\nPrecedence relations:\nFor a simple precedence grammar, three key relations between terminal symbols are defined:\n1. Less than (≺): a ≺ b if the terminal a immediately precedes b in the output.\n2. Equal to (=): a = b if the two symbols are adjacent and are part of the same subexpression.\n3. Greater than (≻): a ≻ b if the symbol a terminates a part of the output and the symbol b follows it.\n\nThe base of a chain in a simple precedence grammar is the smallest subexpression that respects the ordering relations, on which other parts of the chain can be \"pumped\". For example, for an arithmetic expression of the form a + b × c, the subexpression b × c can be considered the base of the chain, since the multiplication operation (with higher precedence) precedes the addition. This subexpression is preserved in any subsequent part of the expression and determines how the remaining components are built. \n\nRelations are used to build a precedence table, which helps in the parsing process. \n\nBuilding a simple precedence grammar \n1) The grammar must be simplified - in particular, left-recursive rules must be removed. Left-recursion creates cycles in the grammar, which makes it impossible to use precedence relations, since such cycles violate the strict order between symbols. \n2) All pairs of adjacent terminals between which a relationship must be established are determined. 3) A precedence table is built based on these pairs. \n\nSimple priority grammars are a powerful tool for creating parsers. They simplify the creation of bottom-up parsers by exploiting the relationships between symbols, but are only suitable for a certain type of grammar."
    },
    {
        "question": "Single priority",
        "answer": "\nA simple precedence grammar is a class of context-free grammars for which a bottom-up parser can be constructed using precedence relations between symbols in the grammar. The basic idea of ​​this method is to use binary relations between symbols to determine the order in which output rules are applied.\n\nPrecedence relations:\nFor a simple precedence grammar, three key relations between terminal symbols are defined:\n1. Less than (≺): a ≺ b if terminal a immediately precedes b in the output.\n2. Equal to (=): a = b if the two symbols are adjacent and part of the same subexpression.\n3. Greater than (≻): a ≻ b if symbol a terminates a part of the output and is followed by symbol b.\n\nThe basis of a string in a simple precedence grammar is the smallest subexpression that respects the order relation into which other parts of the string can be \"pumped\". For example, for an arithmetic expression like a+b×c, the subexpression b×c can be considered the base of the string, since the addition is preceded by a multiplication operation (with higher precedence). This subexpression is preserved in any subsequent part of the expression and determines how the remaining components are created.\n\nRelations are used to build a precedence table, which facilitates the parsing process.\n\nBuilding a simple precedence grammar\n1) The grammar should be simplified - in particular, left-recursive rules should be removed. Left recursion creates cycles in the grammar, making it impossible to use precedence relations, since such cycles violate the strict order between symbols.\n2) All pairs of adjacent terminals between which a connection must be established are determined. 3) A precedence table is built based on these pairs.\n\nSimple precedence grammars are a powerful tool for creating parsers. They simplify the creation of bottom-up parsers using relations between symbols, but are suitable only for a certain type of grammar."
    },
    {
        "question": "Simple priority -",
        "answer": "\nA simple precedence grammar is a class of context-free grammars for which a bottom-up parser can be constructed using precedence relations between symbols in the grammar. The basic idea of ​​this method is to use binary relations between symbols to determine the order in which output rules are applied.\n\nPrecedence relations:\nFor a simple precedence grammar, three key relations are defined between terminal symbols:\n1. Less than (≺): a ≺ b if terminal a immediately precedes b in the output.\n2. Equal to (=): a = b if the two symbols are adjacent and part of the same subexpression.\n3. Greater than (≻): a ≻ b if symbol a terminates a part of the output and is followed by symbol b.\n\nThe basis of a string in a simple precedence grammar is the smallest subexpression that respects the ordering relations into which other parts of the string can be \"pumped\". For example, for an arithmetic expression like a+b×c, the subexpression b×c can be considered the root of the string, since the addition is preceded by a multiplication operation (with higher precedence). This subexpression is preserved in any subsequent part of the expression and determines how the remaining components are constructed.\n\nRelations are used to build a precedence table, which helps in the parsing process.\n\nBuilding a simple precedence grammar\n1) The grammar should be simplified - in particular, left-recursive rules should be removed. Left recursion creates cycles in the grammar, making it impossible to use precedence relations, since such cycles violate the strict order between symbols.\n2) All pairs of adjacent terminals between which a connection should be established are determined. 3) A precedence table is built based on these pairs.\n\nSimple precedence grammars are a powerful tool for creating parsers. They facilitate the creation of bottom-up parsers using relations between symbols, but are suitable only for a certain type of grammar."
    },
    {
        "question": "Normal form Greibach is",
        "answer": "\nGreibach Normal Form (GNF) was proposed by Sheila Greibach in 1965. It is a representation of a context-free grammar in which each output rule has a certain strict structure. GNF is used in syntactic analysis and grammar theory to simplify grammars and provide properties important for building parsers such as LL(1) parsers.\n\nDefinition of Greibach Normal Form:\n\nA grammar G = (N, Σ, P, S) is in Greibach Normal Form if all output rules have the following form:\n\nA → aα\n\nwhere:\n- A ∈ N is a nonterminal (the left symbol of a rule).\n- a ∈ Σ is a terminal (the first symbol of the right side of a rule).\n- α ∈ V* is a (possibly empty) sequence of nonterminals.\n\nExample of a grammar in Greibach normal form\nConsider a grammar for a language L = {a b^n | n ≥ 0}.\n\nRules in Greibach normal form:\n\n1. S → aS (says that any string in L must start with a terminal a, followed by a nonterminal A)\n2. S → bA (indicates that a may be followed by one or more b)\n3. A - ϵ (allows the string to end after a or after a sequence of b)\n\nEach rule starts with a terminal, followed by (possibly) nonterminals."
    },
    {
        "question": "Greibach normal form is",
        "answer": "\nGreibach Normal Form (GNF) was proposed by Sheila Greibach in 1965. It is a representation of a context-free grammar in which each output rule has a certain strict structure. GNF is used in syntactic analysis and grammar theory to simplify grammars and provide properties important for building parsers such as LL(1) parsers.\n\nDefinition of Greibach Normal Form:\n\nA grammar G = (N, Σ, P, S) is in Greibach Normal Form if all output rules have the following form:\n\nA → aα\n\nwhere:\n- A ∈ N is a nonterminal (the left symbol of a rule).\n- a ∈ Σ is a terminal (the first symbol of the right side of a rule).\n- α ∈ V* is a (possibly empty) sequence of nonterminals.\n\nExample of a grammar in Greibach normal form\nConsider a grammar for a language L = {a b^n | n ≥ 0}.\n\nRules in Greibach normal form:\n\n1. S → aS (says that any string in L must start with a terminal a followed by a nonterminal A)\n2. S → bA (indicates that a may be followed by one or more b)\n3. A - ϵ (allows a string to end after a or after a sequence of b)\n\nEach rule starts with a terminal followed by (possibly) nonterminals."
    },
    {
        "question": "Greibach normal form:",
        "answer": "\nGreibach Normal Form (GNF) was proposed by Sheila Greibach in 1965. It is a representation of a context-free grammar in which each output rule has a certain strict structure. GNF is used in syntactic analysis and grammar theory to simplify grammars and provide important properties for building parsers such as LL(1) parsers.\n\nDefinition of Greibach Normal Form:\n\nA grammar G = (N, Σ, P, S) is in Greibach Normal Form if all output rules have the following form:\n\nA → aα\n\nOr:\n- A ∈ N is a nonterminal (the left symbol of a rule).\n- a ∈ Σ is terminal (the first symbol on the right side of the ruler).\n- α ∈ V* is a (possibly empty) sequence of nonterminals.\n\nExample of a grammar in Greibach normal form\nConsider the grammar of a language L = {a b^n | п ≥ 0}.\n\nThe rules in Greibach normal form are:\n\n1. S → aS (says that each line in L must begin with a terminal a followed by a nonterminal A)\n2. S → bA (indicates that a may be followed by one or more b)\n3. A - ϵ (allows a line to end after a or after a sequence of b)\n\nEach rule begins with a terminal optionally followed by nonterminals."
    },
    {
        "question": "Greibach normal form:",
        "answer": "\nGreibach Normal Form (GNF) was proposed by Sheila Greibach in 1965. It is a representation of a context-free grammar in which each output rule has a certain strict structure. GNF is used in syntactic analysis and grammar theory to simplify grammars and provide important properties for building parsers such as LL(1) parsers.\n\nDefinition of Greibach Normal Form:\n\nA grammar G = (N, Σ, P, S) is in Greibach Normal Form if all output rules have the following form:\n\nA → aα\n\nOr :\n- A ∈ N is a nonterminal (the left symbol of a rule).\n- a ∈ Σ is a terminal (the first symbol on the right side of the ruler).\n- α ∈ V* is a (possibly empty) sequence of nonterminals.\n\nExample of Greibach Normal Form Grammar\nConsider the grammar of a language L = {a b^n | п ≥ 0}.\n\nGreibach Normal Form rules:\n\n1. S → aS (means that any string in L must start with a terminal a followed by a nonterminal A)\n2. S → bA (indicates that a may be followed by one or more b)\n3. A - ϵ (allows the string to end after a or after a sequence of b)\n\nEach rule starts with a terminal optionally followed by nonterminals."
    },
    {
        "question": "The Cock-Younger-Kasami (CYK) algorithm is",
        "answer": "\nThe Cock-Younger-Kasami (CYK) algorithm is a dynamic algorithm used to recognize strings belonging to context-free languages.\n\nIt determines whether a given string can be generated using a certain context-free grammar (CFG), \nwhich is represented in Chomsky Normal Form (CNF).\n\nThe essence of the algorithm:\nCYK checks whether a string can be constructed from terminals using the rules of the grammar. The algorithm is especially useful for building parsers and has a running time of O(n^3), where n is the length of the string.\n\nSteps of the CYK algorithm:\n\n1. Preliminary preparation:\nThe grammar must be represented in Chomsky normal form:\n- Each output rule has the form:\nA → BC or A → a\nwhere A, B, C are nonterminals, a is a terminal.\n\n2. Construction of a triangular table:\nCYK uses a table of size n × n, where n is the length of the string.\n\n- All nonterminals that can output the substring w[i ... j] are written in the cell (i, j) of the table.\n\n3. Filling the table:\n- On the first level (the diagonal of the table), nonterminals are indicated that can output individual characters of the string.\n- On the following levels, the algorithm checks all possible ways of splitting the string and fills the table, checking which rules can be applied to combine substrings.\n\n4. Result:\nIf the upper right corner of the table contains the start symbol of the grammar, then the string belongs to the language described by the grammar.\n\nUse example:\nLet us have a string w = \"baaa\" and a grammar in Chomsky normal form, which contains the following rules:\n- S → AB | BC \n- A → BA | a \n- B → CC | b \n- C → AB | a\n\nExample of filling in the table for the string w = \"baaa\":\n\nThe table is filled gradually, starting with the diagonal cells, where each cell on the first level contains nonterminals that can output individual symbols of the string. That is, the first level (diagonal cells) indicates which nonterminals can output the symbols b and a.\n\n| | b | a | a | a |\n|-----|--------|-------|-------|-------|\n| b | B | | | |\n| a | | A, C | | |\n| a | | | A, C | |\n| a | | | | A, C |\n\nAt the second level (or iteration), we consider all possible partitions of substrings of length 2 and write down nonterminals that can output each such substring. This process continues for substrings of length 3, 4, and so on, until the entire table is filled.\n\nRunning time:\n- Time complexity: O(n^3)\n- Memory: O(n^2)\n\nApplications:\n- Building parsers for context-free languages.\n- Checking whether a string belongs to a certain language.\n- Used in compilers and natural language processing systems.\n\nCYK is a powerful algorithm for string analysis based on context-free grammars, which finds applications in the fields of compilers and theoretical computer science."
    },
    {
        "question": "Cock-Younger-Kasami (CYK) algorithm",
        "answer": "\nThe Cock-Younger-Kasami (CYK) algorithm is a dynamic algorithm used to recognize strings belonging to context-free languages.\n\nIt determines whether a given string can be generated using a certain context-free grammar (CFG),\n\nwhich is represented in Chomsky normal form (CNF).\n\nThe essence of the algorithm:\nCYK checks whether a string can be constructed from terminals using the rules of the grammar. The algorithm is especially useful for building parsers and has a running time of O(n^3), where n is the length of the string.\n\nThe steps of the CYK algorithm:\n\n1. Preliminary preparation:\nThe grammar must be represented in Chomsky normal form:\n- Each output rule has the form:\nA → BC or A → a\nwhere A, B, C are non-terminals, a is a terminal.\n\n2. Building a triangular table:\nCYK uses a table of size n × n, where n is the length of the string.\n\n- All nonterminals that can output a substring w[i ... j] are written to cell (i, j) of the table.\n\n3. Filling the table:\n- The first level (the diagonal of the table) specifies nonterminals that can output individual characters of the string.\n- At the next levels, the algorithm checks all possible ways to split the string and fills the table by checking which rules can be applied to combine the substrings.\n\n4. Result:\nIf the upper right corner of the table contains the initial symbol of the grammar, then the string belongs to the language described by the grammar.\n\nExample of use:\nLet us have a string w = \"baaa\" and a grammar in Chomsky normal form that contains the following rules:\n- S → AB | BC \n- A → BA | a \n- B → CC | b \n- C → AB | a\n\nExample of filling the table for the string w = \"baaa\":\n\nThe table is filled gradually, starting from the diagonal cells, where each cell at the first level contains nonterminals that can output individual characters of the string. That is, the first level (diagonal cells) specifies which nonterminals can output the characters b and a.\n\n| | b | a | a | a |\n|-----|--------|-------|--------|\n| b | B | | | |\n| a | | A, C | | |\n| a | | | A, C | |\n| a | | | A, C |\n\nAt the second level (or iteration), we consider all possible partitions of substrings of length 2 and write down the nonterminals that can output each such substring. This process continues for substrings of length 3, 4, etc., until the entire table is filled.\n\nRunning time:\n- Time complexity: O(n^3)\n- Memory: O(n^2)\n\nUsage:\n- Creating parsers for context-free languages.\n- Checking whether a string belongs to a certain language.\n- Used in compilers and natural language processing systems.\n\nCYK is a powerful string parsing algorithm based on context-free grammars, which finds applications in the field of compilers and theoretical computer science."
    },
    {
        "question": "Kok-Younger-Kasami (CYK) algorithm",
        "answer": "\nThe Cock-Younger-Kasami (CYK) algorithm is a dynamic algorithm used to recognize strings belonging to context-free languages.\n\nIt determines whether a given string can be generated using a certain context-free grammar (CFG).\n\nwhich is represented in Chomsky normal form (CNF).\n\nThe essence of the algorithm:\nCYK checks whether a string can be constructed from terminals using the rules of the grammar. The algorithm is especially useful for creating parsers and has a running time of O(n^3), where n is the length of the string.\n\nThe steps of the CYK algorithm:\n\n1. Preliminary preparation:\nThe grammar must be represented in Chomsky normal form:\n- Each output rule has the form:\nA → BC or A → a\nwhere A, B, C are non-terminals, a is a terminal.\n\n2. Building a triangular table:\nCYK uses an n × n array, where n is the length of the string.\n\n— All nonterminals that can generate a substring w[i...j] are written to cell (i,j) of the table.\n\n3. Fill in the table:\n- The first level (the diagonal of the table) contains nonterminals that can represent individual characters of the string.\n- At subsequent levels, the algorithm checks all possible ways to split the string and fills in the table by checking which rules can be applied to combine substrings.\n\n4. Result:\nIf the initial symbol of the grammar is in the upper right corner of the table, then the string belongs to the language described by the grammar.\n\nExample of use:\nLet's say we have a string w = \"baaa\" and a Chomsky Normal Form grammar containing the following rules:\n-C → AB | British Columbia\n- A → Bachelor | A\n-B → CC | b \n-C → AB | A\n\nExample of filling a table for the string w=\"baaa\":\n\nThe table is filled gradually, starting from the diagonal cells, where each cell of the first level contains nonterminals capable of displaying individual characters of the string. That is, the first level (diagonal cells) determines which nonterminals can display the characters b and a.\n\n| | b | a | a | a |\n|-------------|--------|--------|--------|\n| b | B | | | |\n| a | | A, C | | |\n| a | | | A, C | |\n| a | | | A, C |\n\nAt the second level (or iteration), we consider all possible divisions of substrings of length 2 and write nonterminals capable of generating each of these substrings. This process continues for substrings of length 3, 4, etc., until the entire array is filled.\n\nRuntime:\n- Time complexity: O(n^3)\n- Memory: O(n^2)\n\nApplication:\n- Creating parsers for context-free languages.\n- Check if a channel belongs to a certain language.\n- Used in compilers and natural language processing systems.\n\nCYK is a powerful string parsing algorithm based on context-free grammars, which finds applications in the fields of compilers and theoretical computer science."
    },
    {
        "question": "Cock-Younger-Kasami (CYK) algorithm",
        "answer": "\nThe Cock-Younger-Kasami (CYK) algorithm is a dynamic algorithm used to recognize strings belonging to context-free languages.\n\nIt determines whether a given string can be generated using a certain context-free grammar (CFG).\nwhich is represented in Chomsky normal form (CNF).\n\nThe essence of the algorithm:\nCYK checks whether a string can be constructed from terminals using the rules of the grammar. The algorithm is especially useful for constructing parsers and has a running time of O(n^3), where n is the length of the string.\n\nThe steps of the CYK algorithm:\n\n1. Preliminary preparation:\nThe grammar must be represented in Chomsky normal form:\n- Each output rule has the form:\nA → BC or A → a\nwhere A, B, C are non-terminals, a is a terminal.\n\n2. Building a triangular table:\nCYK uses an n × n array, where n is the length of the string.\n\n- All nonterminals capable of producing a substring w[i...j] are written to cell (i, j) of the table.\n\n3. Filling the table:\n- The first level (table diagonal) contains nonterminals capable of producing individual characters of the string.\n- At subsequent levels, the algorithm checks all possible ways to split the string and fills the table by checking which rules can be applied to combine substrings.\n\n4. Result:\nIf the upper right corner of the table contains the initial symbol of the grammar, then the string belongs to the language described by the grammar.\n\nUsage example:\nLet us have a string w = \"baaa\" and a grammar in Chomsky normal form that contains the following rules:\n-C → AB | BC\n- A → Bachelor | has\n-B → CC | b \n-C → AB | has\n\nAn example of filling an array for the string w=\"baaa\":\n\nThe table is filled gradually, starting from the diagonal cells, where each first-level cell contains nonterminals capable of generating individual characters of the string. In other words, the first level (diagonal cells) indicates which nonterminals can generate the characters b and a.\n\n| | b | a | a | a |\n|-----|--------|-------|-------|\n| b | B | | | |\n| a | | A, C | | |\n| a | | | | A, C | |\n\nAt the second level (or iteration), we consider all possible partitions of substrings of length 2 and mark the nonterminals capable of generating each of these substrings. This process continues for substrings of length 3, 4, etc., until the entire array is filled.\n\nRuntime:\n- Time complexity: O(n^3)\n- Memory: O(n^2)\n\nApplications:\n- Creating parsers for context-free languages.\n- Checking whether a string belongs to a certain language.\n- Used in compilers and natural language processing systems.\n\nCYK is a powerful string parsing algorithm based on context-free grammars, which finds applications in the field of compilers and theoretical computer science."
    },
    {
        "question": "Recursive Descent Method",
        "answer": "\nThe recursive descent method is a method of parsing context-free grammars that uses recursive functions to analyze the structure of the input string. This method is convenient for parsing expressions, such as arithmetic expressions, and is often used when writing compilers and interpreters.\nThe main characteristics of the recursive descent method are:\n1. Recursive calls: For each non-terminal of the grammar, a separate function is written that is responsible for parsing derivatives for this non-terminal. The functions call each other recursively, following the rules of the grammar.\n2. Simplicity: The method is clear and relatively simple to implement, but can only work with LL(1) grammars - these are grammars that can be parsed by looking only one symbol ahead.\n3. Implementation: The parser is implemented as a set of mutually recursive functions, each of which checks a certain part of the grammar.\n\nAn example of the recursive descent method:\nConsider a grammar in Extended Backus-Naur Form (EBNF): for a simple arithmetic expression such as 1 + 2 * 3, which has the following rules:\nExpr -> Term ( (\"+\" | \"-\") Term )*\nTerm -> Factor ( (\"*\" | \"/\") Factor )*\nFactor -> \"(\" Expr \")\" | NUM\n\nFor such a grammar, we can create a recursive parser consisting of functions Expr, Term, Factor, which will follow the rules of the grammar and call functions for each derived expression.\nThe main stages of implementing a recursive descent parser:\n1. Input processing: The parser starts analyzing from the top of the grammar. For example, for the expression Expr, the parser starts by calling the function Expr.\n2. Recursive function calls: Depending on the characters in the string and the current rule, the functions call each other. For example, when parsing Expr, the Term function is called, and then checked to see if it is followed by + or -, so that Term can be called again.\n3. Checking terminals and terminating recursion: When a terminal symbol (such as NUM or +) is encountered, the function compares it with the next symbol in the string. If the symbol matches, the parser moves forward one symbol; if not, it returns an error.\n4. Terminating processing: If the parser successfully completes all function calls, the string matches the grammar.\n\nAdvantages:\n- Ease of implementation for LL(1) grammars.\n- Readability and logicality of code: each non-terminal of the grammar corresponds to a function.\n- Direct construction of the syntax tree, which is convenient for creating compilers and interpreters.\nDisadvantages:\n- Limited to LL(1) grammars for iterative implementation, but recursive descent can be used for other grammars (in the absence of left recursion), although they will require backtracking.\n- There may be performance and recursion depth issues with complex grammars or very long input strings, especially when using returns.\n\nThe recursive descent method is convenient for small languages ​​and expressions and is often used when creating compilers for simple programming languages."
    },
    {
        "question": "Recursive Descent Method",
        "answer": "\nThe recursive descent method is a method for parsing context-free grammars that uses recursive functions to analyze the structure of the input string. This method is convenient for parsing expressions such as arithmetic expressions, and is often used when writing compilers and interpreters.\nThe main characteristics of the recursive descent method are:\n1. Recursive calls: for each non-terminal of the grammar, a separate function is written that is responsible for parsing derivatives for this non-terminal. The functions call each other recursively, following the rules of the grammar.\n2. Simplicity: the method is clear and relatively easy to implement, but can only work with LL(1) grammars - these are grammars that can be parsed by looking ahead only one symbol.\n3. Implementation: The parser is implemented as a set of mutually recursive functions, each of which checks a certain part of the grammar.\n\nAn example of the recursive descent method:\nConsider a grammar in Extended Backus-Naur Form (EBNF): for a simple arithmetic expression such as 1 + 2 * 3, which has the following rules:\nExpr -> Term ( (\"+\" | \"-\") Term )*\nTerm -> Factor ( (\"*\" | \"/\") Factor )*\nFactor -> \"(\" Expr \")\" | NUM\n\nFor such a grammar, we can create a recursive parser consisting of functions Expr, Term, Factor, which will follow the rules of the grammar and call functions for each derived expression.\nThe main steps of implementing a recursive descent parser are:\n1. Processing input data: The parser starts analyzing from the top of the grammar. For example, for the expression Expr, the parser starts by calling the function Expr.\n2. Recursive function calls: Depending on the characters in the string and the current rule, the functions call each other. For example, parsing Expr calls Term and then checks whether it is followed by + or - so that Term can be called again.\n3. Terminal checking and recursion termination: When a terminal symbol (such as NUM or +) is encountered, the function compares it with the next symbol in the string. If the symbol matches, the parser moves forward one symbol; if not, it returns an error.\n4. Termination: If the parser successfully completes all function calls, the string conforms to the grammar.\n\nAdvantages:\n- Easy to implement for LL(1) grammars.\n- Readable and logical code: each non-terminal in the grammar corresponds to a function.\n- Direct construction of the syntax tree, which is convenient for writing compilers and interpreters.\nDisadvantages:\n- Limited to LL(1) grammars for iterative implementation, but recursive descent can be used for other grammars (in the absence of left recursion), although they will require backtracking. - There may be performance and recursion depth issues with complex grammars or very long input strings, especially when using returns.\n\nThe recursive descent method is convenient for small languages ​​and expressions and is often used when creating compilers for simple programming languages."
    },
    {
        "question": "Recursive Descent Method",
        "answer": "\nRecursive descent is a context-free parsing technique that uses recursive functions to analyze the structure of an input string. This technique is useful for parsing expressions such as arithmetic expressions, and is often used when writing compilers and interpreters.\nThe main features of the recursive descent technique are:\n1. Recursive calls: for each non-terminal in the grammar, a separate function is written that is responsible for parsing derivatives of this non-terminal. The functions are called recursively, following the rules of the grammar.\n2. Simplicity: the technique is clear and relatively easy to implement, but can only work with LL(1) grammars - these are grammars that can be parsed by looking at one symbol ahead.\n3. Implementation: the parser is implemented as a set of mutually recursive functions, each of which checks a certain part of the grammar.\n\nAn example of the recursive descent method:\nConsider a grammar in Extended Backus-Naur Form (EBNF): for a simple arithmetic expression such as 1 + 2 * 3, which has the following rules:\nExpr -> Term ((((((( «+» | «-)) Term )*\nTerm -> Factor ( (\"*\" | \"/\") Factor )*\nFactor -> \"(\"Expr))\" | NUMBER\n\nFor such a grammar, we can create a recursive parser consisting of functions Expr, Term, Factor, which will follow the rules of the grammar and call functions for each derived expression.\nThe main steps of implementing a recursive descent parser:\n1. Input processing: the parser starts parsing from the beginning of the grammar. For example, for the expression Expr, the parser starts by calling the function Expr.\n2. Recursive function calls: depending on the characters in the string and the current rule, the functions call each other. For example, parse Expr calls Term, then checks whether it is followed by + or - so that Term can be called again.\n3. Terminal check and recursion termination. When a terminal symbol (such as NUM or +) is encountered, the function compares it with the next symbol in the string. If the symbol matches, the parser moves forward one symbol; otherwise, an error is returned.\n4. Terminate processing. If the parser successfully completes all function calls, the string conforms to the grammar.\n\nAdvantages:\n- LL(1) grammars are easy to implement.\n- Code readability and consistency: each non-terminal in the grammar corresponds to a function.\n- Direct construction of the syntax tree, convenient for writing compilers and interpreters.\nBugs:\n- Limited to LL(1) grammars for iterative implementation, but recursive descent can be used for other grammars (in the absence of left recursion), although they require backtracking.\n— There may be performance and recursion depth issues when using complex grammars or very long input strings, especially when using returns.\n\nThe recursive descent method is convenient for small languages ​​and expressions and is often used to create compilers for simple programming languages."
    },
    {
        "question": "Recursive Descent Method",
        "answer": "\nThe recursive descent method is a context-free grammar parsing method that uses recursive functions to analyze the structure of an input string. This method is useful for parsing expressions, such as arithmetic expressions, and is often used when writing compilers and interpreters.\nThe main features of the recursive descent method are:\n1. Recursive calls. For each non-terminal in the grammar, a separate function is written that is responsible for parsing the derivatives of this non-terminal. The functions call each other recursively, following the rules of the grammar.\n\n2. Simplicity: the method is clear and relatively easy to implement, but can only work with LL(1) grammars - these are grammars that can be parsed by looking ahead just one symbol.\n3. Implementation: The parser is implemented as a set of mutually recursive functions, each of which checks a certain part of the grammar.\n\nAn example of the recursive descent method:\nConsider a grammar in Extended Backus-Naur Form (EBNF): for a simple arithmetic expression such as 1 + 2 * 3, which has the following rules:\nExpr -> Term ((((((( «+» | «-)) Term )*\nTerm -> Factor ( (\"*\" | \"/\") Factor )*\nFactor -> \"(\"Expr))\" | NUMBER\n\nFor such a grammar, we can create a recursive parser consisting of functions Expr, Term, Factor, which will follow the rules of the grammar and call functions for each derived expression.\nThe main steps of implementing a recursive descent parser:\n1. Input processing: the parser starts parsing from the beginning of the grammar. For example, for the expression Expr, the parser starts by calling the function Expr.\n2. Recursive function calls. Depending on the characters in the string and the current rule, the functions call each other. For example, when parsing Expr calls Term, then checks whether it is followed by + or - so that Term can be called again.\n3. Checking terminals and terminating recursion. When a terminal symbol (such as NUM or +) is encountered, the function compares it to the next symbol in the string. If the symbol matches, the parser moves forward one symbol; otherwise, an error is returned.\n4. Terminating processing: If the parser successfully completes all function calls, the string matches the grammar.\n\nAdvantages:\n- LL(1) grammars are easy to implement.\n- Code is readable and logical: each non-terminal in the grammar corresponds to a function.\n- Direct construction of the syntax tree, convenient for creating compilers and interpreters.\nDisadvantages:\n- Limited to LL(1) grammars for iterative implementation, but recursive descent can be used for other grammars (in the absence of left recursion), although they require backtracking. - There may be performance and recursion depth issues when using complex grammars or very long input strings, especially when using returns.\n\nThe recursive descent method is convenient for small languages ​​and expressions and is often used when creating compilers for simple programming languages."
    },
    {
        "question": "The Problem of Post Correspondences",
        "answer": "\nThe Post Correspondence Problem (PCP) is formulated as follows: given two sets of strings of the same length over some alphabet, it is necessary to determine whether there exists a sequence of indices of strings in the sets such that the concatenation of strings from the first set coincides with the concatenation of strings from the second set.\nFormally, the problem is described as follows:\nGiven two sets of strings A = {a_1, a_2, …, a_n} and B = {b_1, b_2, …, b_n}, where a_i and b_i are strings over the alphabet Σ. It is necessary to find a sequence of indices i_1,i_2,…,i_k such that the equality a_i1 a_i2…a_ik=b_i1 b_i2…b_ik is satisfied.\n\nProperties and complexity of the Post Correspondence Problem:\n- PCP is an undecidable problem in the general case, i.e. there is no algorithm that can solve it for any input.\n- PCP is not a regular or context-free language and cannot be solved by automata.\n- This problem is fundamental in theoretical computer science, since it is used to prove the undecidability of other problems, but it is not applicable to problems related to regular languages ​​and regular expressions."
    },
    {
        "question": "The problem of postal correspondence",
        "answer": "\nThe Postal Correspondence Problem (PCP) is formulated as follows: given two sets of strings of the same length over some alphabet, it is necessary to determine whether there exists a sequence of indices of strings in the sets such that the concatenation of strings from the first set coincides with the concatenation of strings from the second set.\nFormally, the problem is described as follows:\nGiven two sets of strings A = {a_1, a_2, …, a_n} and B = {b_1, b_2, …, b_n}, where a_i and b_i are strings over the alphabet Σ. It is necessary to find a sequence of indices i_1,i_2,…,i_k such that the equality a_i1 a_i2…a_ik=b_i1 b_i2…b_ik is satisfied.\n\nProperties and complexity of the Post Correspondence Problem:\n- PCP is an undecidable problem in the general case, i.e. there is no algorithm that can solve it for any input.\n- PCP is not a regular or context-free language and cannot be solved by automata.\n- This problem is fundamental in theoretical computer science because it is used to prove the undecidability of other problems, but it is not applicable to problems involving regular languages ​​and regular expressions."
    },
    {
        "question": "Problem with mail",
        "answer": "\nThe Postal Correspondence Problem (PCP) is formulated as follows: given two sets of strings of the same length in a certain alphabet, it is necessary to determine whether there exists a sequence of string indices in the sets such that the concatenation of strings from the first set is similar to the concatenation of strings from the second set.\nThe problem is formally described as follows:\nGiven two sets of strings A = {a_1, a_2, …, a_n} and B = {b_1, b_2, …, b_n}, where a_i and b_i are strings of the alphabet Σ. We need to find a sequence of indices i_1,i_2,…,i_k such that the equality a_i1 a_i2…a_ik=b_i1 b_i2…b_ik is satisfied.\n\nProperties and complexity of the Postal Correspondence Problem:\n— PCP is a general unsolvable problem, that is, there is no algorithm capable of solving it for the input data.\n- PCP is not a regular or context-free language and cannot be solved by automata.\n- This problem is fundamental in theoretical computer science, since it is used to prove the undecidability of other problems, but it does not apply to problems involving regular languages ​​and regular expressions."
    },
    {
        "question": "The problem of postal correspondence",
        "answer": "\nThe Postal Correspondence Problem (PCP) is formulated as follows: given two sets of strings of the same length in an alphabet, it is necessary to determine whether there exists a sequence of string indices in the sets such that the concatenation of the strings from the first set coincides with the concatenation of the strings from the second set.\nFormally, the problem is described as follows:\nGiven two sets of strings A = {a_1, a_2, …, a_n} and B = {b_1, b_2, …, b_n}, where a_i and b_i are strings of the alphabet Σ. It is necessary to find a sequence of indices i_1,i_2,…,i_k such that the equality a_i1 a_i2…a_ik=b_i1 b_i2…b_ik is satisfied.\n\nProperties and complexity of the Postal Correspondence Problem:\n— PCP is an unsolvable problem in the general case, that is, there is no algorithm capable of solving it for any input.\n- PCP is not a regular or context-free language and cannot be solved by automata.\n- This problem is fundamental in theoretical computer science, since it is used to prove the undecidability of other problems, but it does not apply to problems involving regular languages ​​and regular expressions."
    },
    {
        "question": "Simple Precedence Grammars",
        "answer": "\nSimple precedence grammars are a special class of context-free grammars for which order relations between symbols can be constructed based on the structure of the grammar, making parsing easier. Simple precedence grammars use three types of relations between terminal symbols:\n\n1. The \"less than\" relation - when one symbol immediately precedes another in the grammar.\n2. The \"equal to\" relation - when two symbols can appear next to each other in the same grammar rule.\n3. The \"greater than\" relation - when one symbol ends a structure, and the next symbol begins another structure.\n\nBuilding simple precedence relations: \nThe relations between symbols in simple precedence grammars can be defined using the following steps:\n\n- Defining direct precedence: For each rule of the form A that maps to α and X from β, where X and β are symbols or substrings, it can be established that X follows α.\n- Defining right precedence: If X is a terminal that follows a substring, then the relation X is established.\n- Transitive relations: Used to complete the construction of the precedence relation by defining the order of symbols.\n\nApplication: \nSimple precedence grammars are useful for automatically building top-down and bottom-up parsers (e.g., the shift-reduce algorithm). With their help, it is possible to unambiguously determine the sequence of operations for a string in the grammar using precedence matrices.\n\nExamples of use: \nOne of the famous examples of simple precedence grammar is arithmetic expressions, where addition and multiplication operations follow the rules of mathematical precedence. For example:\n- The addition operation has low precedence compared to multiplication, i.e. multiplication has precedence over addition, which is expressed by the precedence relation + < *. This means that multiplication is performed before addition if both symbols are in the same expression.\n- An opening parenthesis has high precedence for any symbol following it, and a closing parenthesis terminates a group of operators.\n\nThis approach allows for the development of efficient parsers that build a parse tree for expressions, minimizing the need for additional checking of the inference rules."
    },
    {
        "question": "Simple priority grammars",
        "answer": "\nSimple precedence grammars are a special class of context-free grammars for which order relations between symbols can be constructed based on the structure of the grammar, which simplifies syntactic analysis. Simple precedence grammars use three types of relations between terminal symbols:\n\n1. The \"less than\" relation - when one symbol immediately precedes another in the grammar.\n2. The \"equal to\" relation - when two symbols can appear next to each other in the same grammar rule.\n3. The \"greater than\" relation - when one symbol ends a structure and the next symbol begins another structure.\n\nConstructing simple precedence relations:\n\nThe relations between symbols in simple precedence grammars can be defined using the following steps:\n\n- Defining direct precedence: For each rule of the form A that maps to α and X from β, where X and β are symbols or substrings, it can be established that X follows α.\n\n- Defining right precedence: If X is a terminal that follows a substring, then the relation X is established.\n\n- Transitive relations: Used to complete the construction of the precedence relation by determining the order of the symbols.\n\nUsage:\n\nSimple precedence grammars are useful for automatically constructing top-down and bottom-up parsers (e.g., the shift-reduce algorithm). With their help, it is possible to unambiguously determine the sequence of operations for a string in the grammar using precedence matrices.\n\nUse examples:\n\nOne of the famous examples of a simple precedence grammar is arithmetic expressions, where the addition and multiplication operations follow the rules of mathematical precedence. For example:\n- The addition operation has low precedence compared to multiplication, i.e. multiplication takes precedence over addition, which is expressed by the precedence relation + < *. This means that multiplication is performed before addition if both symbols are in the same expression.\n- An opening parenthesis has high precedence for any symbol following it, and a closing parenthesis terminates a group of operators.\n\nThis approach allows for the development of efficient parsers that build a parse tree for expressions, minimizing the need for additional checking of inference rules."
    },
    {
        "question": "Simple Priority Grammars",
        "answer": "\nSimple precedence grammars are a special class of context-free grammars for which order relations between symbols can be constructed based on the structure of the grammar, thereby simplifying the analysis. Simple precedence grammars use three types of relations between terminal symbols:\n\n1. The less-than relation occurs when one symbol in the grammar immediately precedes another.\n2. The equal relation is when two symbols can appear next to each other in the same grammatical rule.\n3. The greater-than relation occurs when one symbol terminates one structure and the next symbol begins another structure.\n\nConstructing simple precedence relations:\n\nRelations between symbols in simple precedence grammars can be defined by the following steps:\n\n- Definition of forward precedence: For each rule of the form A that matches α and X of β, where X and β are symbols or substrings, we can define that X follows α.\n- Determining the correct precedence: if X is a terminal following a substring, then the relationship X is established.\n\n- Transitive relations: allow the construction of precedence relations to be completed by determining the order of symbols.\n\nApplication: \nSimple precedence grammars are useful for automatically constructing top-down and bottom-up parsers (e.g., the Shift-Reduce algorithm). They allow one to unambiguously determine the sequence of operations on a string in the grammar using precedence matrices.\n\nUsage examples: \nA well-known example of a simple precedence grammar is arithmetic expressions, in which the addition and multiplication operations follow the rules of mathematical precedence. For example:\n- The addition operation has low precedence compared to multiplication, i.e. multiplication takes precedence over addition, which is expressed by the precedence relation + < *. This means that multiplication is performed before addition if both symbols are in the same expression.\n- An opening parenthesis has high precedence for any character following it, and a closing parenthesis terminates a group of instructions.\n\nThis approach allows for the development of efficient parsers that build a parse tree for expressions, thereby minimizing the need for additional checking of output rules."
    },
    {
        "question": "Simple priority grammars",
        "answer": "\nSimple priority grammars are a special class of context-free grammars for which order relations between symbols can be constructed based on the structure of the grammar, making analysis easier. Simple priority grammars use three types of relations between terminal symbols:\n\n1. Less-than relation: when in the grammar one symbol immediately precedes another.\n2. Equal-to relation: when two symbols can appear next to each other in the same grammar rule.\n3. Greater-than relation: when one symbol terminates one structure and the next symbol begins another.\n\nConstructing simple priority relations:\nRelations between symbols in simple priority grammars can be determined using the following steps:\n\n- Defining forward precedence: For each rule of the form A that matches α and X from β, where X and β are symbols or substrings, it can be established that X follows α.\n- Right-hand precedence: If X is the terminal following a substring, then the relation X is established.\n- Transitive relations: Used to complete the construction of precedence relations by defining the order of symbols.\n\nApplication : \nSimple precedence grammars are useful for automatically constructing top-down and bottom-up parsers (e.g., the shift-reduce algorithm). They can be used to uniquely define the sequence of operations on a string in the grammar using precedence matrices.\n\nUsage examples : \nOne of the well-known examples of a simple precedence grammar is arithmetic expressions, in which the addition and multiplication operations obey the rules of mathematical precedence. For example :\n- The addition operation has low precedence compared to multiplication, i.e. multiplication takes precedence over addition, which is expressed by the precedence relation + < *. This means that multiplication is performed before addition if both symbols are in the same expression.\n- An opening parenthesis has high precedence over any symbol following it, and a closing parenthesis terminates a group of operators.\n\nThis approach allows for the development of efficient parsers that build a parse tree of expressions, thereby minimizing the need for additional checking of inference rules."
    },
    {
        "question": "If the union of a regular language and a language L is context-free, then is L context-free?",
        "answer": "\nThe statement is true. If the union of a regular language R and a language L is context-free, then L is necessarily context-free.\n\n1. Closure of context-free languages:\n- Context-free languages ​​are closed under intersection with regular languages.\n- Regular languages ​​are closed under complement.\n\n2. Proof:\n- Since R is a regular language, its complement is also regular.\n- The union L combine R is a context-free language by assumption.\n- Consider the intersection (L combine R) intersect the complement of R.\n- Since L combine R is context-free and the complement of R is regular, their intersection is context-free.\n- But (L combine R) intersect the complement of R = (L intersect the complement of R) combine (R intersect the complement of R) = L intersect the complement of R, since R intersect the complement of R = the empty set.\n- Therefore, L intersect the complement of R is a context-free language.\n- Now note that L = (L intersect the complement of R) union (L intersect R).\n- L intersect R is the intersection of L and a regular language R, which is a context-free language.\n- Thus, the union of two context-free languages ​​(L intersect the complement of R) and (L intersect R) gives us L, which is context-free.\n\n3. Conclusion:\n- Therefore, if L intersect R is context-free, then L is also context-free.\n- Your original answer contains an error in stating that L is not necessarily context-free."
    },
    {
        "question": "If the union of a regular language and a language L is context-free, then is L context-free?",
        "answer": "\nThe statement is true. If the union of a regular language R and a language L is context-free, then L is necessarily context-free.\n\n1. Closedness of context-free languages:\n- Context-free languages ​​are closed under intersection with regular languages.\n- Regular languages ​​are closed under complement.\n\n2. Proof:\n- Since R is a regular language, its complement is also regular.\n- The union of L combines R is a context-free language by assumption.\n- Consider the intersection (L combines R) combines with the complement of R.\n- Since L combines R is context-free and the complement of R is regular, their intersection is context-free.\n- But (L combines R) combines with the complement of R = (L combines with the complement of R) combines (R combines with the complement of R) = L combines with the complement of R, since R combines with the complement of R = the empty set.\n- Therefore, L intersects the complement of R, which is a context-free language.\n- Now note that L = (L intersects the complement of R) the union of (L intersects R).\n- L intersects R is the intersection of L and a regular language R, which is a context-free language.\n- Thus, the union of two context-free languages ​​(L intersects the complement of R) and (L intersects R) gives us L, which is context-free.\n\n3. Conclusion:\n- Therefore, if L intersects R, which is context-free, then L is also context-free.\n- Your original answer contains an error in stating that L is not necessarily context-free."
    },
    {
        "question": "If the union of a regular language and a language L is context-free, then is L context-free?",
        "answer": "\nThis statement is true. If the union of a regular language R and a language L is context-free, then L is necessarily context-free.\n\n1. Closure of context-free languages:\n— Context-free languages ​​are closed under intersection with regular languages.\n— Regular languages ​​have closed complement.\n\n2. Proof:\n— Since R is a regular language, its complement is regular.\n— The union of L unites R is a context-free language by hypothesis.\n- Consider the intersection (L unites R) that cuts the complement of R.\n- Since L unites R, context-free, and the complement of R is regular, their intersection is context-free.\n- But (L unites R) cuts the complement of R = (L cuts the complement of R) unites (R cuts the complement of R) = L cuts the complement of R, since R cuts the complement of R = the empty set.\n- So L intersects the complement of R, a context-free language.\n- Now note that L = (L cuts the complement of R) the union of (L cuts R).\n- L intersects R is the intersection of L and a regular language R, which is a context-free language.\n- Thus, the union of two languages ​​out of context (L intersects the complement of R) and (L intersects R) gives us L, which is out of context.\n\n3. Conclusion:\n- Therefore, if L intersects R out of context, then L is also out of context.\n- Your original answer contains a mistake in stating that L is not necessarily context-free."
    },
    {
        "question": "If the union of a regular language and a language L is context-free, then is L context-free?",
        "answer": "\nThis statement is true. If the union of a regular language R and a language L is context-free, then L is necessarily context-free.\n\n1. Closure of context-free languages:\n— Context-free languages ​​are closed under their intersection with regular languages.\n- Regular languages ​​are closed under their complement.\n\n2. Proof:\n— Since R is a regular language, its complement is regular.\n- The union L unites R is, by hypothesis, a context-free language.\n- Consider the intersection (L unites R) that intersects the complement of R.\n- Since L unites R, is context-free, and the complement of R is regular, their intersection is context-free.\n- But (L unites R) intersects the complement of R = (L unites R) unites (R unites R) = L unites R, since R unites R = the empty set.\n- Therefore, L unites R and is a context-free language.\n- Now note that L = (L intersects the complement of R) the union of (L intersects R).\n- L intersects R is the intersection of L and a regular language R, which is a context-free language.\n- Thus, the union of two context-free languages ​​(L intersects the complement of R) and (L intersects R) gives us L, which is context-free.\n\n3. Conclusion:\n- Therefore, if L intersects R, which is context-free, then L is also context-free.\n- Your original answer contains a mistake in saying that L is not necessarily out of context."
    },
    {
        "question": "What techniques exist for eliminating ε-rules and chain rules from context-free grammars?",
        "answer": "\nTechniques for eliminating ε-rules (rules that produce the empty string) and chain rules (rules of the form A -> B) from context-free grammars involve several steps and algorithms. Here is the correct description of these techniques:\n\nGot it, here is the corrected text with the initial symbol adjustment added:\n\n1. Eliminating ε-rules:\n- Defining ε-rules: These are rules of the form A -> ε, where ε denotes the empty string. Eliminating such rules from a grammar requires a transformation that does not change the language, except when the language contains the empty string.\n- Steps for eliminating ε-rules:\n1. Defining nullable symbols: Define the set of all nonterminals that can produce the empty string. For example, if there is a rule A -> ε, then A is nullable. Also, if there is a rule A -> B1 B2 ... Bk and each Bi is nullable, then A is also nullable.\n2. Constructing new rules: For each rule of the form A -> B1 B2 ... Bk, new rules are added where nullable symbols can be omitted. For example, if there is a rule A -> BCD, and B and D are nullable, then new rules are created A -> CD, A -> BC, and A -> C.\n3. Removing ε-rules: All rules of the form A -> ε are removed, except for cases where the starting symbol can produce ε if the empty string belongs to the language. In this case, a new rule is added S' -> S | ε, where S' is the new starting symbol, to preserve the possibility of producing the empty string.\n\nExample:\nInitial grammar:\nS -> AB | C\nA -> ε\nB -> b\nC -> c\n\nSteps:\n1. Determine that A is nullable.\n2. Build new rules:\n- For the rule S -> AB, create S -> B (without A) and S -> A (without B).\n3. Delete the rule A -> ε.\n\nResulting grammar:\nS -> AB | B | A | C\nB -> b\nC -> c\n\n2. Eliminate chain rules:\n- Define chain rules: Chain rules are rules of the form A -> B, where A and B are nonterminals, and they create chains of substitutions that can lengthen the parsing process.\n- Steps for eliminating chain rules:\n1. Construct chain sets: For each nonterminal A, define the set of all nonterminals B that can be reached by chain rules. This is done by constructing a closure: if A -> C and C -> D, then A is linked to D.\n2. Creating new rules: For each rule B -> α, where B is in the chain set of the non-terminal A, a new rule A -> α is added.\n3. Removing chain rules: All chain rules are removed after adding new derived rules.\n\n3. Checking and adjusting the starting symbol:\n- If the starting symbol S can produce the empty string, a new starting symbol S' and a rule S' -> S | ε must be added to preserve the possibility of producing the empty string."
    },
    {
        "question": "What are the methods for eliminating ε-rules and chain rules from context-free grammars?",
        "answer": "\nThe methods for eliminating ε-rules (rules that produce the empty string) and chain rules (rules of the form A -> B) from context-free grammars involve several steps and algorithms. Here is the correct description of these methods:\n\nGot it, here is the corrected text with the initial symbol adjustment added:\n\n1. Eliminating ε-rules:\n- Defining ε-rules: These are rules of the form A -> ε, where ε denotes the empty string. Eliminating such rules from a grammar requires a transformation that does not change the language, except when the language contains the empty string.\n- Steps for eliminating ε-rules:\n1. Defining nullable symbols: Define the set of all nonterminals that can produce the empty string. For example, if there is a rule A -> ε, then A is nullable. Also, if there is a rule A -> B1 B2 ... Bk and each Bi is nullable, then A is also nullable.\n\n2. Constructing new rules: For each rule of the form A -> B1 B2 ... Bk, add new rules in which nullable symbols can be omitted. For example, if there is a rule A -> BCD, and B and D are nullable, then create new rules A -> CD, A -> BC, and A -> C.\n3. Deleting ε-rules: All rules of the form A -> ε are deleted, except in cases where the initial symbol can produce ε if the empty string belongs to the language. In this case, add a new rule S' -> S | ε, where S' is the new initial symbol, to preserve the possibility of producing the empty string.\n\nExample:\nSource grammar:\nS -> AB | C\nA -> ε\nB -> b\nC -> c\n\nSteps:\n1. Determine that A is nullable.\n2. Build new rules:\n- For the rule S -> AB, create S -> B (without A) and S -> A (without B).\n3. Delete the rule A -> ε.\n\nResulting grammar:\nS -> AB | B | A | C\nB -> b\nC -> c\n\n2. Delete chain rules:\n- Identify chain rules: Chain rules are rules of the form A -> B, where A and B are nonterminals, and they create chains of substitutions that can lengthen the parsing process.\n- Steps to delete chain rules:\n1. Build chain sets: For each nonterminal A, determine the set of all nonterminals B that can be reached using chain rules. This is done by constructing a closure: if A -> C and C -> D, then A is related to D.\n2. Create new rules: For each rule B -> α, where B is in the chain set of the nonterminal A, a new rule A -> α is added.\n3. Remove chain rules: All chain rules are removed after adding new derived rules.\n\n3. Check and adjust the starting symbol:\n- If the starting symbol S can create an empty string, a new starting symbol S' and a rule S' -> S | ε must be added to preserve the possibility of creating an empty string."
    },
    {
        "question": "What are the methods for eliminating ε-rules and chaining rules from context-free grammars?",
        "answer": "\nThe methods for eliminating ε rules (rules that produce the empty string) and union rules (rules of the form A -> B) from context-free grammars involve several steps and algorithms. Here is the correct description of these methods:\n\nOkay, here is the corrected text with some initial symbol changes:\n\n1. Elimination of ε rules:\n- Definition of ε rules: These are rules of the form A -> ε, where ε denotes the empty string. Removing such rules from a grammar requires a transformation that does not change the language, unless the language contains the empty string.\n- Steps for eliminating ε rules:\n1. Definition of nullable symbols: Determine the set of all nonterminals that can produce the empty string. For example, if there is a rule A -> ε, then A is nullable. Also, if there is a rule A -> B1 B2...Bk and each Bi is nullable, then A is also nullable.\n\n2. Constructing new rules: For each rule of the form A -> B1 B2...Bk, add new rules that allow omitting nullable characters. For example, if there is a rule A -> BCD and B and D are nullable, then create new rules A -> CD, A -> BC, and A -> C.\n3. Removing ε rules. All rules of the form A -> ε are removed, except that the starting character can produce ε if the empty string belongs to the language. In this case, the new rule is S' -> S | ε, where S' is a new starting character that allows preserving the possibility of creating an empty string.\n\nExample:\nOriginal grammar:\nC -> AB | C\nA -> e\nB -> b\nC -> c\n\nMeasures:\n1. Determine that A is nullable.\n2. Create new rules:\n- For the rule S -> AB, create S -> B (without A) and S -> A (without B).\n3. Delete the rule A -> ε.\n\nResulting grammar:\nC -> AB | B | A | C\nB -> b\nC -> c\n\n2. Remove chained rules:\n- Identify chaining rules: Chaining rules are rules of the form A -> B, where A and B are nonterminals, and they create chains of substitutions that can lengthen the parsing process.\n- Steps to remove chained rules:\n1. Build chained sets: For each nonterminal A, determine the set of all nonterminals B that can be reached using chaining rules. This is done by constructing a closure: if A -> C and C -> D, then A is connected to D.\n2. Create new rules: for each rule B -> α, where B is in the chain set of nonterminals of A, a new rule A -> α is added.\n3. Remove chain rules. All chain rules are removed after adding new derived rules.\n\n3. Check and adjust the starting character:\n- If the main character S can create an empty string, a new main character S' and a rule S' -> S | ε must be added to preserve the possibility of creating an empty string."
    },
    {
        "question": "What methods exist for eliminating ε-rules and chain rules from context-free grammars?",
        "answer": "\nThe techniques for eliminating ε rules (rules that produce the empty string) and chain rules (rules of the form A -> B) from context-free grammars involve several steps and algorithms. Here is the correct description of these techniques:\n\nI got it, here is the corrected text with the initial symbol adjustment added:\n\n1. Eliminating ε rules:\n- Defining ε rules: These are rules of the form A -> ε, where ε denotes the empty string. Removing such rules from a grammar requires a transformation that does not change the language, except when the language contains the empty string.\n- Steps to eliminate ε rules:\n\n1. Defining nullable symbols. Determine the set of all nonterminals that can produce the empty string. For example, if there is a rule A -> ε, then A is nullable. Also, if there is a rule A -> B1 B2...Bk and each Bi is nullable, then A is also nullable.\n2. Constructing new rules: For each rule of the form A -> B1 B2... Bk, new rules are added that can omit nullable symbols. For example, if there is a rule A -> BCD, and B and D are nullable, then new rules are created: A -> CD, A -> BC, and A -> C.\n3. Removing ε rules: All rules of the form A -> ε are removed, except for the case where the starting symbol can produce ε if the empty string belongs to the language. In this case, a new rule is added: S' -> S | ε, where S' is the new starting symbol, to preserve the possibility of producing an empty string.\n\nExample:\nInitial grammar:\nC -> AB | C\nA -> e\nB -> b\nC -> c\n\nSteps:\n1. Determine that A is nullable.\n2. Create new rules:\n- For the rule S -> AB, create S -> B (without A) and S -> A (without B).\n3. Remove the rule A -> ε.\n\nResulting grammar:\nC -> AB | B | A | C\nB -> b\nC -> c\n\n2. Eliminate channel rules:\n- Define chain rules: Chain rules are rules of the form A -> B, where A and B are nonterminals, and they create chains of substitutions that can lengthen the parsing process.\n- Steps to eliminate channel rules:\n1. Build chain sets: For each nonterminal A, determine the set of all nonterminals B that can be reached by chain rules. This is done by constructing a closure: if A -> C and C -> D, then A is related to D.\n2. Create new rules: For each rule B -> α, where B is in the set of strings of the nonterminal A, a new rule A -> α is added.\n3. Deleting chain rules. All chain rules are deleted after adding new derived rules.\n\n3. Checking and adjusting the start symbol:\n- If the start symbol S can create an empty string, a new start symbol S' and the rule S' -> S | ε must be added to keep the possibility of creating an empty string."
    },
    {
        "question": "What problems arise when transforming context-free grammars into equivalent ones that do not contain ε-rules (epsilon-rules)?",
        "answer": "\nTransforming context-free grammars (CFGs) into equivalent ones that do not contain ε-rules (rules that produce an empty string) may be associated with a number of problems and features. Here are the main ones:\n\n1. Increasing the size of the grammar:\n- After excluding ε-rules, the number of rules in the grammar may increase significantly. This is due to the need to add all possible combinations of symbols that may or may not produce an empty string. Such an expansion complicates the grammar and increases its size, which may complicate further syntactic analysis and processing.\n\n2. Difficulty accounting for all nullable symbols:\n- It is necessary to determine all symbols that can produce ε. This process requires iterative analysis of all the grammar rules and the construction of a set of nullable symbols. Errors in this process can lead to incomplete exclusion of ε-rules and, as a consequence, to a change in the language that the grammar produces.\n\n3. Preserving language equivalence:\n- When transforming, it is important that the new grammar produces the same language as the original, except for the empty string. Incorrectly adding new rules or omitting necessary derivations may result in the grammar producing a different language or not all the strings that the original grammar produced.\n\n4. Taking into account the starting symbol:\n- If the starting symbol of the grammar can produce the empty string, it is necessary to add a new starting symbol and a rule that allows producing either the original starting symbol or the empty string. This is important in order to preserve the ability to produce the empty string in the language, if it is present.\n\n5. Increasing the complexity of syntactic analysis:\n- A grammar without ε-rules may contain a larger number of complex rules, which may complicate the syntactic analysis process, increasing the parser execution time and memory requirements. In some cases, this may lead to the need to optimize the parsing algorithms.\n\n6. Concatenation and union issues:\n- When transforming rules where ε-rules were combined with other symbols, difficulties may arise in constructing new rules that correctly describe all possible derivatives without using ε. This is especially noticeable when processing long chains of nonterminals, where all possible combinations of the presence and absence of nullable symbols must be taken into account.\n\n7. LL grammar features:\n- LL grammars may contain ε-rules. However, for many LL grammars, removing ε-rules is not required, since the parsing performed by the LL(1) parser is already linear and efficiently processes such rules. Removing ε-rules from an LL grammar may be redundant and unnecessarily complicate the grammar, especially if the parser is already configured to work with such rules.\n\nTransforming a context-free grammar into an equivalent one without ε-rules requires careful analysis of all rules and taking into account nullable symbols. This can lead to an increase in the size and complexity of the grammar, which complicates syntactic analysis. However, in the case of LL grammars, ε-rules can be processed directly by the parser, so their removal is not always justified."
    },
    {
        "question": "Give the definition of a superregular set in the theory of formal languages. Define the set of regular languages",
        "answer": "\nLet the alphabet Σ={c1,c2,…,ck} be given.\nThe set R\nwill be called a superregular set over the alphabet Σ if:\n\nR0⊂R, where R0={∅,{ε},{c1},{c2},…,{ck}},\nL1,L2∈R⇒L1∪L2∈R,L1L2∈R,L∗1∈R.\nThen the set of regular languages ​​REG′\nover the alphabet Σ={c1,c2,…,ck}\nis the intersection of all superregular sets over this alphabet."
    },
    {
        "question": "Give the definition of a superregular set in the theory of formal languages. Define the set of regular languages",
        "answer": "\nLet the alphabet Σ={c1,c2,…,ck} be given.\nThe set R\nwill be called a superregular set over the alphabet Σ if:\n\nR0⊂R, where R0={∅,{ε},{c1},{c2},…,{ck}},\nL1,L2∈R⇒L1∪L2∈R,L1L2∈R,L∗1∈R.\nThen the set of regular languages ​​REG′\nover the alphabet Σ={c1,c2,…,ck}\nis the intersection of all superregular sets over this alphabet."
    },
    {
        "question": "Give the definition of a superregular set in the theory of formal languages. Definition of the set of ordinary languages",
        "answer": "\nLet the alphabet Σ={c1,c2,…,ck}.\nDefine R\nwill be called a superregular set in the alphabet Σ if:\n\nR0⊂R, where R0={∅,{ε},{c1},{c2},…,{ck}},\nL1, L2€R⇒L1∪L2€R, L1L2€R, L∗1€R.\nThen the set of regular languages ​​REG′\non the alphabet Σ={c1,c2,…,ck}\nis the intersection of all superregular sets of this alphabet."
    },
    {
        "question": "Give the definition of a superregular set in formal language theory. Define all ordinary languages",
        "answer": "\nLet the alphabet Σ={c1,c2,…,ck}.\nThe set R\nwill be called a superregular set in the alphabet Σ if:\n\nR0⊂R, where R0={∅,{ε},{c1},{c2},…,{ck}},\nL1,L2€R⇒L1∪L2€R,L1L2€R,L∗1€R.\nThen the set of regular languages ​​​​REG′\non the alphabet Σ={c1,c2,…,ck}\nis the intersection of all superregular sets of this alphabet."
    },
    {
        "question": "How to check if a pair of deterministic finite automata are equivalent using breadth-first search?",
        "answer": "\nTwo automata can also be tested for equivalence using breadth-first traversal. We will synchronously traverse two automata, starting from the starting states, in search of a string that distinguishes two states of these automata. That is, it will be accepted by one automaton, but will not belong to the language of the other.\n\nSince equivalent automata accept the same language, when moving along the same symbols in both automata, the word must be accepted by both automata simultaneously. That is, the vertices to which we have moved must be either simultaneously terminal or simultaneously non-terminal, which is what the given algorithm checks.\n\nPseudocode\n// aut[i][c]\n— state number to which there is a transition from state i\nby symbol c\n\nboolean bfsEquivalenceCheck\n(aut1\n: int[][], aut2\n: int[][]):\nQ.push(⟨s1,s2⟩)\n// Q\n— queue of state pairs\nwhile Q≠∅\n\nu,v←Q.pop()\n\nif isTerminal1[u]≠isTerminal2[v]\n\nreturn false\nused[u][v]←\ntrue\nfor c∈Σ\n\nif not used[aut1[u][c]][aut2[v][c]]\n\nQ.push(⟨aut1[u][c],aut2[v][c]⟩)\n\nreturn true\nThe correctness of the algorithm follows from a rigorous proof of the fact that if two states u\nand v\ndiffer by some string, then they differ by a string of length O(n).\n\nThe intuitive understanding of the algorithm is as follows: let the string w\ntake us to states ⟨u,v⟩, and let them both be non-terminal. After that, we make a transition by symbol c\nto states ⟨u′,v′⟩.\n\nThen if isTerminal1[u′]≠isTerminal2[v′], then the string wc\ndistinguishes these two states. This means that the automata are not equivalent."
    },
    {
        "question": "How to check equivalence of a pair of deterministic finite automata using breadth-first search?",
        "answer": "\nTwo automata can also be tested for equivalence using breadth-first traversal. We will synchronously traverse two automata, starting from the initial states, in search of a string that distinguishes the two states of these automata. That is, it will be accepted by one automaton, but will not belong to the language of the other.\n\nSince equivalent automata accept the same language, when moving along the same symbols in both automata, the word must be accepted by both automata simultaneously. That is, the vertices we have moved to must be either simultaneously finite or simultaneously non-finite, which is what this algorithm checks.\n\nPseudocode\n// aut[i][c]\n— state number to which the transition from state i occurs\nby symbol c\n\nboolean bfsEquivalenceCheck\n(aut1\n: int[][], aut2\n: int[][]):\nQ.push(⟨s1,s2⟩)\n// Q\n— queue of state pairs\nwhile Q≠∅\n\nu,v←Q.pop()\n\nif isTerminal1[u]≠isTerminal2[v]\n\nreturn false\nused[u][v]←\ntrue\nfor c∈Σ\n\nif not used[aut1[u][c]][aut2[v][c]]\n\nQ.push(⟨aut1[u][c],aut2[v][c]⟩)\n\nreturn true\nThe correctness of the algorithm follows from a rigorous proof that if two states u\nand v\ndiffer by some string, then they differ by a string of length O(n).\n\nThe intuitive understanding of the algorithm is as follows: let the string w\ntransfer us to states ⟨u,v⟩, and let them both be non-terminal. After that, we make a transition by symbol c\nto states ⟨u′,v′⟩.\n\nThen if isTerminal1[u′]≠isTerminal2[v′], then the string wc\ndistinguishes these two states. This means that the automata are not equivalent."
    },
    {
        "question": "How to check equivalence of a pair of deterministic finite automata using breadth-first search?",
        "answer": "\nThe equivalence of two machines can also be checked by scanning in breadth. We will synchronously traverse the two machines, starting from the initial states, looking for a line that distinguishes the two states of these machines. In other words, it will be accepted by one machine, but will not belong to the language of the other.\n\nSince equivalent machines accept the same language, when moving along the same symbols in both machines, the word must be accepted by both machines simultaneously. In other words, the vertices to which we have moved must be either simultaneously finite or simultaneously non-finite, which is what this algorithm checks.\n\nPseudocode\n//auto[i][c]\n— state number to which the transition from state i occurs\nsymbol c\n\nlogical value bfsEquivalenceCheck\n(auto1\n: int[][], aut2\n:int[][]):\nQ.push(⟨s1,s2⟩)\n// Question\n— ​​queue of state pairs\nwhile Q≠∅\n\nand,in ←Q.pop()\n\nif isTerminal1[u]≠isTerminal2[v]\n\nreturn false\nused[u][v] ←\ntrue\nfor cεΣ\n\nif not used[aut1[u][c]][aut2[v][c]]\n\nQ.push(⟨aut1[u][c],aut2[v][c]⟩)\n\nreturn true\nThe correctness of the algorithm is due to ​a rigorous proof that if two states u\nand v\ndiffer by a string, then they differ by a string of length O(n).\n\nThe intuitive understanding of the algorithm is as follows: let the string w\nlead us to states ⟨u,v⟩, and both of them are non-terminal. After that, we go to the symbol c\nin states ⟨u′,v′⟩.\n\nThus, if isTerminal1[u′]≠isTerminal2[v′], then the string wc\ndistinguishes these two states. This means that the automata are not equivalent."
    },
    {
        "question": "How to check equivalence of a pair of deterministic finite automata using breadth-first search?",
        "answer": "\nWe can also check the equivalence of two automata using breadth-first scanning. We will synchronously traverse the two automata, starting from the starting states, in search of a chain that distinguishes the two states of these automata. In other words, it will be accepted by one automaton, but will not belong to the language of the other.\n\nSince equivalent automata accept the same language, then when moving along the same symbols in both automata, the word must be accepted by both automata simultaneously. That is, the vertices to which we have moved must be either simultaneously terminal or simultaneously non-terminal, which is checked by this algorithm.\n\nPseudocode\n// aut[i][c]\n— state number to which the transition from state i occurs\nsymbol c\n\nlogical value bfsEquivalenceCheck\n(aut1\n: int[][], aut2\n:int[][]):\nQ.push(⟨s1,s2⟩)\n// Question\n— ​​queue of state pairs\nwhile Q≠∅\n\nand,in ←Q.pop()\n\nif isTerminal1[u]≠isTerminal2[v]\n\nreturns false\nw/u[u][v] ←\ntrue\nfor cεΣ\n\nif not used[aut1[u][c]][aut2[v][c]]\n\nQ.push(⟨aut1[u][c],aut2[v][c]⟩)\n\nreturns truth\nThe correction of the algorithm is the result of a rigorous proof of the fact that if two states u\nand b\ndiffer by some string, then they differ by a string of length O(n).\n\nThe intuitive understanding of the algorithm is as follows: let the string w\nlead us to states ⟨u,v⟩, and both of them are non-terminal. After that, we make a transition through the symbol c\nto states ⟨u′,v′⟩.\n\nThen if isTerminal1[u′]≠isTerminal2[v′], then the string wc\ndistinguishes these two states. This means that the automata are not equivalent."
    },
    {
        "question": "What properties do CF languages ​​have with respect to the operations of union, concatenation, Kleene closure, and direct homomorphism?",
        "answer": "\nThe union\nL1∪L2\nis a CF-language.\nProof:\nLet us construct a CF-grammar for the language L1∪L2. To do this, consider the corresponding CF-grammars for the languages ​​L1\nand L2. Let the starting symbols in them be named S\nand T\nrespectively. Then we denote the starting symbol for L1∪L2\nby S′\nand add the rule S′→S∣T.\n\nLet us show that S′⇒∗w⟺S⇒∗w∨T⇒∗w.\n\n⇒\n\nSince S⇒∗w\nis the rule S′→S\n, then, by the definition of ⇒∗\nwe obtain that S′⇒∗w. Similarly for T.\n⇐\n\nLet S′⇒∗w. Since S′→S∣T\nare the only rules in which the nonterminal S′\nis present on the right-hand side, this means that either S′⇒S⇒∗w or S′⇒T⇒∗w.\n\nConcatenation\nL1L2\nis a CFG.\nProof\nWe need to use the idea of ​​the previous proof and construct a CFG for L1L2 with the rule S′→ST, where S\nand T\nare the starting symbols of the languages ​​L1\nand L2\nrespectively.\n\nL∗=⋃i=0∞Li\nis a CFG.\n\nHomomorphism\nCFGs are closed under direct homomorphism.\n\nInverse homomorphism\nCF languages ​​are closed under inverse homomorphism.\n\nIntersection\nIf L1=aibicj,L2=aibjcj\n, then L1∩L2\nis not a CF language.\n\nReversal\nL^R={w^R∣w∈L}\nis context-free."
    },
    {
        "question": "What properties do CF languages ​​have with respect to the operations of union, concatenation, Kleene closure, and direct homomorphism?",
        "answer": "\nThe union\nL1∪L2\nis a CF-language.\nProof:\nLet us construct a CF-grammar for the language L1∪L2. To do this, consider the corresponding CF-grammars for the languages ​​L1\nand L2. Let the initial symbols in them be called S\nand T\nrespectively. Then denote the initial symbol for L1∪L2\nby S′\nand add the rule S′→S∣T.\n\nLet us show that S′⇒∗w⟺S⇒∗w∨T⇒∗w.\n\n⇒\n\nSince S⇒∗w\nis a rule of S′→S\n, then by the definition of ⇒∗\nwe obtain that S′⇒∗w. Similarly for T.\n⇐\n\nLet S′⇒∗w. Since S′→S∣T\nare the only rules in which the nonterminal S′\nappears on the right-hand side, this means that either S′⇒S⇒∗w or S′⇒T⇒∗w.\n\nThe concatenation\nL1L2\nis a CFG.\nProof\nWe need to use the idea of ​​the previous proof and construct a CFG for L1L2 with the rule S′→ST, where S\nand T\nare the initial symbols of the languages ​​L1\nand L2\nrespectively.\n\nL∗=⋃i=0∞Li\nis a CFG.\n\nThe homomorphism\nCFGs are closed under the direct homomorphism.\n\nInverse homomorphism\nCF languages ​​are closed under inverse homomorphism.\n\nIntersection\nIf L1=aibicj,L2=aibjcj\n, then L1∩L2\nis not a CF language.\n\nInversion\nL^R={w^R∣w∈L}\nis context-free."
    },
    {
        "question": "What properties do CF languages ​​have with respect to union, concatenation, Kleene closure, and direct homomorphism operations?",
        "answer": "\nAssociation\nL1∪L2\nis a CF language.\nProof:\nLet us construct a CF grammar for the language L1∪L2. To do this, consider the corresponding CF grammars for the languages ​​L1.\nand L2. Let their initial symbols be called S\nand T\nrespectively. Next, let us denote the starting symbol for L1∪L2\nby S'\nand add the rule S′→S∣T.\n\nLet us show that S′⇒∗w⟺S⇒∗w∨T⇒∗w.\n\n⇒\n\nSince S⇒∗w\nis a rule of S′→S\n, then by the definition of ⇒∗\nwe find that S′⇒∗w. Similarly for T.\n⇐\n\nLet S′⇒∗w. Since S′→S∣T\nare the only rules in which the nonterminal S'\nappears on the right-hand side, this means that either S′⇒S⇒∗w or S′⇒T⇒∗w.\n\nThe chain\nЛ1Л2\nis a CFG.\nProof\nWe must take the idea of ​​the previous proof and construct a CFG for L1L2 with the rule S′→ST, where S\nand T\nare the initial symbols of the languages ​​L1\nand L2\nrespectively.\n\nL∗=⋃i=0∞Li\nis a CFG.\n\nHomomorphism\nCFGs are closed under the forward homomorphism.\n\nInverse homomorphism\nThe languages ​​CF are closed under the inverse homomorphism.\n\nIntersection\nIf L1=aibicj,L2=aibjcj\n, then L1∩L2\nis not a CF language.\n\nCall\nL^R={w^R∣wاL}\nout of context."
    },
    {
        "question": "What properties do CF languages ​​have with respect to the operations of union, concatenation, Kleene closure, and direct homomorphism?",
        "answer": "\nConjunction\nL1∪L2\nis a CF language.\nProof:\nLet us construct a CF grammar for the language L1∪L2. To do this, consider the corresponding CF grammars for the languages ​​L1.\nand L2. Let the starting symbols of the latter be called S\nand T\nrespectively. Then we denote the starting symbol for L1∪L2\nby S'\nand add the rule S′→S∣T.\n\nWe will show that S′⇒∗w⟺S⇒∗w∨T⇒∗w.\n\n⇒\n\nSince S⇒∗w\nis a rule S′→S\n, then by the definition of ⇒∗\nwe obtain that S′⇒∗w. Similarly for T.\n⇐\n\nLet S′⇒∗w. Since S′→S∣T\nare the only rules in which the nonterminal S'\nappears on the right-hand side, this means that either S′⇒S⇒∗w or S′⇒T⇒∗w.\n\nThe concatenation\nЛ1Л2\nis a CFG.\nProof\nWe need to use the idea of ​​the previous proof and construct a CFG for L1L2 by the rule S′→ST, where S\nand T\nare the starting symbols of the languages ​​L1\nand L2\nrespectively.\n\nL∗=⋃i=0∞Li\nis a CFG.\n\nHomomorphism\nCFGs are closed under the forward homomorphism.\n\nInverse homomorphism\nThe languages ​​CF are closed under the inverse homomorphism.\n\nIntersection\nIf L1=aibicj,L2=aibjcj\nthen L1∩L2\nis not a CF language.\n\nInversion\nL^R={w^R∣wاL}\nis found without context."
    },
    {
        "question": "prove that the language of alphabetic prefix grammar is regular",
        "answer": "\nDefinition of APG\nGiven an SRS S with two kinds of rewriting rules:\nai → b1 . . . bn ai → ε\nAllow the rules to apply only to the first letters\nof a word. Let a pair ⟨S, w0⟩ be given, where w0 is a word in the alphabet\nΣ. This pair defines an alphabetic prefix\ngrammar.\nClaim\nThe language L⟨S, w0⟩ is regular.\n2 / 24\n▲\nAlphabetic Prefix Grammars\nClaim\nThe language L⟨S, w0⟩ is regular.\nSay that a ↠ ε (a collapses) if either a → ε ∈ S or\n∃b1, . . . , bn(∀bi(bi ↠ ε) & a → b1 . . . bn ∈ S).\nGiven APG ⟨S, s1 . . . sn⟩, we generate a right-linear grammar G. To each\nsymbol of the alphabet ai we assign Ai, a nonterminal of G.\n1 Let a → b1 . . . bn and ∃bi(¬(bi ↠ ε) & ∀j(j < i ⇒ bj ↠ ε)).\nThen we add to G the rules A → B1b2 . . . bn, A → B2b3 . . . bn,. . . ,\nA → Bibi+1 . . . bn, A → a.\n2 If there is no such bi, add to G all rules of the form\nA → B1b2 . . . bn, . . . , A → Bn−1bn, A → Bn, A → a.\n3 Introduce a starting nonterminal S and add for it an expansion into\nthe original word s1 . . . sm according to the rules above.\n4 If all si collapse, then add to G the rule S → ε.\n2 / 24\n▲\nAlphabetic prefix grammars\nWe say that a ↠ ε (a collapses) if either a → ε ∈ S or\n∃b1, . . . , bn(∀bi(bi ↠ ε) & a → b1 . . . bn ∈ S).\nGiven APG ⟨S, s1 . . . sn⟩, we generate a right-linear grammar G. To each\nsymbol of the alphabet ai we assign Ai, a nonterminal of G.\n1 Let a → b1 . . . bn and ∃bi(¬(bi ↠ ε) & ∀j(j < i ⇒ bj ↠ ε)).\nThen we add to G the rules A → B1b2 . . . bn, A → B2b3 . . . bn,. . . ,\nA → Bibi+1 . . . bn, A → a.\n2 If there is no such bi, add to G all rules of the form\nA → B1b2 . . . bn, . . . , A → Bn−1bn, A → Bn, A → a.\n3 Introduce the starting nonterminal S and add an expansion for it into\nthe original word s1 . . . sm according to the rules above.\n4 If all si collapse, then add to G the rule S → ε.\nIt remains to make an expansion of the rules of the form A → Bn, or to go from G to\nan NFA with ε-transitions.\n2 / 24\n▲"
    },
    {
        "question": "prove that the language of alphabetic prefix grammar is regular",
        "answer": "\nDefinition of APG\nGiven an SRS S with two kinds of rewriting rules:\nai → b1 . . . bn ai → ε\nLet the rules apply only to the first letters\nof a word. Given a pair ⟨S, w0⟩, where w0 is a word in the alphabet\nΣ. This pair defines an alphabetic prefix\ngrammar.\nClaim\nThe language L⟨S, w0⟩ is regular.\n2 / 24\n▲\nAlphabetic Prefix Grammars\nClaim\nThe language L⟨S, w0⟩ is regular.\nSay that a ↠ ε (a collapses) if either a → ε ∈ S or\n∃b1, . . . , bn(∀bi(bi ↠ ε) & a → b1 . . . bn ∈ S).\nGiven an APG ⟨S, s1 . . . sn⟩, we generate a right-linear grammar G. To each\nsymbol of the alphabet ai, we assign Ai, a nonterminal of G.\n1 Let a → b1 . . . bn and ∃bi(¬(bi ↠ ε) & ∀j(j < i ⇒ bj ↠ ε)).\nWe then append to G the rules A → B1b2 . . . bn, A → B2b3 . . . bn,. . . ,\nA → Bibi+1 . . . bn, A → a.\n2 If there is no such bi, then add to G all rules of the form\nA → B1b2 . . . bn, . . . , A → Bn−1bn, A → Bn, A → a.\n3 Introduce an initial nonterminal S and add an extension for it to\nthe original word s1 . . . sm according to the rules above.\n4 If all si collapse, then add to G the rule S → ε.\n2 / 24\n▲\nAlphabetic prefix grammars\nWe say that a ↠ ε (a collapses) if either a → ε ∈ S or\n∃b1, . . . , bn(∀bi(bi ↠ ε) & a → b1 . . . bn ∈ S).\nGiven an APG ⟨S, s1 . . . sn⟩, we generate a right-linear grammar G. To each alphabet symbol ai, we assign Ai, a nonterminal of G.\n1 Let a → b1 . . . bn and ∃bi(¬(bi ↠ ε) & ∀j(j < i ⇒ bj ↠ ε)).\nThen we append to G the rules A → B1b2 . . . bn, A → B2b3 . . . bn,. . . ,\nA → Bibi+1 . . . bn, A → a.\n2 If there is no such bi, add to G all rules of the form\nA → B1b2 . . . bn, . . . , A → Bn−1bn, A → Bn, A → a.\n3 Introduce the initial nonterminal S and add an extension for it to the\noriginal word s1 . . . sm according to the rules above.\n4 If all si are destroyed, then add to G the rule S → ε.\nIt remains to make an extension of the rules of the form A → Bn, or go from G to\nan NFA with ε-transitions.\n2 / 24\n▲"
    },
    {
        "question": "prove that a grammatical language with an alphabetic prefix is ​​regular",
        "answer": "\nDefinition of a PNG\nGiven an SRS S with two types of rewriting rules:\nai → b1. . . bn ai → ε\nAllow the rules to apply only to the first letters\nof a word. Consider a pair ⟨S, w0⟩, where w0 is a word of the alphabet\nΣ. This pair defines an alphabetic prefix\ngrammar.\nClaim\nThe language L⟨S, w0⟩ is regular.\n2 / 24\n▲\nAlphabetic Prefix Grammars\nClaim\nThe language L⟨S, w0⟩ is regular.\nSay that a ↠ ε (a collapses) if either a → ε ∈ S or\n∃b1, . . . , bn(∀bi(bi ↠ ε) & a → b1 . . bn ∈ S).\nFor a PNG ⟨S, s1. . . sn⟩, we generate a direct linear grammar G. Each\nalphabetic symbol ai we assign to Ai, a nonterminal of G.\n1 Let a → b1. . . bn and ∃bi(¬(bi ↠ ε) & ∀j(j < i ⇒ bj ↠ ε)).\nThen we add rules A → B1b2 to G. . . lol, A → B2b3. . . good,. . . ,\nA → Bibi+1. . . mkr, A → a.\n2 If such bi does not exist, then add to G all rules of the form\nA → B1b2. . . well, . . . , A → Bn−1bn, A → Bn, A → a.\n3 Introduce an initial nonterminal S and add to it an extension to\nthe original word s1. . . see the rules above.\n4 If all if collapse, then add to G the rule S → ε.\n2 / 24\n▲\nGrammars of Alphabetic Prefixes\nWe say that a ↠ ε (a collapses) if either a → ε ∈ S or\n∃b1, . . . , bn(∀bi(bi ↠ ε) & a → b1 . . bn ∈ S).\nGiven an APG ⟨S, s1. . . sn⟩, we generate a forward linear grammar G. To each alphabet symbol ai, we assign Ai, a nonterminal of G.\n1 Let a → b1. . . bn and ∃bi(¬(bi ↠ ε) & ∀j(j < i ⇒ bj ↠ ε)).\nWe then add rules A → B1b2 to G. . . lol, A → B2b3. . . good,. . . ,\nA → Bibi+1. . . mkr, A → a.\n2 If there is no such bi, add to G all rules of the form\nA → B1b2. . . good,. . . , A → Bn−1bn, A → Bn, A → a.\n3 Enter the initial nonterminal S and add to it an extension to\nthe original word s1. . . see the rules above.\n4 If all ifs are destroyed, then add to G the rule S → ε.\nIt remains to extend the rules of the form A → Bn, i.e. go from G to\nan NFA with ε-transitions.\n2 / 24\n▲"
    },
    {
        "question": "prove that the language of alphabetic prefix grammar is regular",
        "answer": "\nDefinition of PNG\nConsider an SRS S with two types of rewriting rules:\nai → b1. . . bn ai → ε\nLet us allow the rules to apply only to the first\nletter of a word. Consider a pair ⟨S, w0⟩, where w0 is a word of the alphabet\nΣ. This pair defines a prefix grammar\nalphabetic.\nClaim\nThe language L⟨S, w0⟩ is regular.\n2 / 24\n▲\nAlphabetic Prefix Grammars\nClaim\nThe language L⟨S, w0⟩ is regular.\nWe say that a ↠ ε (a collapses) if a → ε ∈ S or\n∃b1, . . . , bn(∀bi(bi ↠ ε) & a → b1...bn ∈ S).\nGiven a NGL ⟨S, s1. . . sn⟩, we generate a direct linear grammar G. At each\nsymbol of the alphabet ai, we assign Ai, a nonterminal of G.\n1 Let a → b1. . . bn and ∃bi(¬(bi ↠ ε) & ∀j(j < i ⇒ bj ↠ ε)).\nThen add to G the rules A → B1b2. . . bn, A → B2b3. . . Okay,. . . ,\nA → Bibi+1. . . bn, A → a.\n2 If there is no such bi, add to G all rules of the form\nA → B1b2. . . good, . . . , A → Bn−1bn, A → Bn, A → a.\n3 Introduce an initial nonterminal S and add an extension for it to\nthe original word s1. . . see the rules above.\n4. If all ifs collapse, then add to G the rule S → ε.\n2 / 24\n▲\nAlphabetic grammars with prefixes\nWe say that a ↠ ε (a collapses) if a → ε ∈ S or\n∃b1, . . . , bn(∀bi(bi ↠ ε) & a → b1...bn ∈ S).\nGiven a PNG ⟨S, s1. . . sn⟩, we generate a direct linear grammar G. At each\nsymbol of the alphabet ai we assign Ai, a non-terminal of G.\n1 Let a → b1. . . bn and ∃bi(¬(bi ↠ ε) & ∀j(j < i ⇒ bj ↠ ε)).\nThen add to G the rules A → B1b2. . . bn, A → B2b3. . . Good,. . . ,\nA → Bibi+1. . . bn, A → a.\n2 If there is no such bi, add to G all rules of the form\nA → B1b2. . . good,. . . , A → Bn−1bn, A → Bn, A → a.\n3 Enter the initial nonterminal S and add an extension for it to the\noriginal word s1. . . see the rules above.\n4. If all ifs fail, then add the rule S → ε to G.\nIt remains to develop rules of the form A → Bn, or to move from G to\nANF with ε-transitions.\n2 / 24\n▲"
    },
    {
        "question": "write Arden's lemma",
        "answer": "\nLet X = (pX) | q, where X is an unknown RE, and p, q are\nknown, with ε /∈ L (p). Then X = (p)\n∗q.\nThat is, p\n∗q is the smallest (but not the only) fixed\npoint of the expression px | q with respect to ⩽, and the only one if\nε /∈ L (p)."
    },
    {
        "question": "write Arden's lemma",
        "answer": "\nLet X = (pX) | q, where X is an unknown RE, and p, q are known, with ε /∈ L (p). Then X = (p)\n∗q.\nThat is, p\n∗q is the smallest (but not the only) fixed\npoint of the expression px | q with respect to ⩽, and the only one if\nε /∈ L (p)."
    },
    {
        "question": "write Arden's lemma",
        "answer": "\nLet X = (pX) | q, where X is an unknown RE, and p, q are known and ε /ε L (p). Then X = (p)\n∗q.\nThat is, p\n∗q is the smallest (but not the only) fixed\npoint of the expression px | q with respect to ⩽ and is unique if\nε /ε L (p)."
    },
    {
        "question": "write Arden's lemma",
        "answer": "\nLet X = (pX) | q, where X is an unknown RE, and p, q are\nknown, with ε /ε L (p). Then X = (p)\n∗q.\nThat is, p\n∗q is the smallest (but not the only) point\nfixed expression px | q with respect to ⩽, and the only one if\nε /ε L (p)."
    },
    {
        "question": "Formulate a theorem on the closure of regular languages",
        "answer": "\nLet L1,L2\nbe regular languages ​​over a single alphabet Σ. Then the following languages ​​are also regular:\nLanguages ​​obtained by applying the following set-theoretic operations:\nL1∪L2,\nL1¯¯¯¯¯¯,\nL1∩L2,\nL1∖L2;\nL∗1;\nL1L2;\nL1←;"
    },
    {
        "question": "Formulate a theorem on the closure of regular languages.",
        "answer": "\nLet L1,L2\nbe regular languages ​​over a single alphabet Σ. Then the following languages ​​are also regular:\nLanguages ​​obtained by applying the following set-theoretic operations:\nL1∪L2,\nL1¯¯¯¯¯¯¯,\nL1∩L2,\nL1∖L2;\nL∗1;\nL1L2;\nL1←;"
    },
    {
        "question": "Formulate a closure theorem for regular languages.",
        "answer": "\nLet L1,L2\nbe regular languages ​​on the same alphabet Σ. Then the following languages ​​are also regular:\nThe languages ​​are obtained by applying the following operations of set theory:\nЛ1∪Л2,\nЛ1¯¯¯¯¯¯¯¯,\nЛ1∩Л2,\nЛ1∖Л2;\nL∗1;\nЛ1Л2;\nЛ1 ←;"
    },
    {
        "question": "Formulate a theorem on the closure of regular languages.",
        "answer": "\nLet L1, L2\nbe regular languages ​​on one alphabet Σ. Then the following languages ​​are also regular:\nThe languages ​​are obtained by applying the following operations on sets:\nЛ1∪Л2,\nЛ1¯¯¯¯¯¯¯,\nЛ1∩Л2,\nЛ1∖Л2 ;\nL∗1 ;\nЛ1Л2;\nL1 ← ;"
    },
    {
        "question": "Formulate theorems on the construction of LL(k) grammars without empty rules",
        "answer": "\nTheorem 1\nFor every LL(k)-\ngrammar, there exists and can be effectively constructed an LL(k + 1)-\ngrammar without empty rules defining the same language.\n\nTheorem 2\nFor every LL(k)-grammar without empty rules, one can construct an LL(k)-grammar in N.V. Greibach defining the same language."
    },
    {
        "question": "Formulate theorems on the construction of LL(k) grammars without empty rules.",
        "answer": "\nTheorem 1\nFor each LL(k)-\ngrammar, there exists and can be effectively constructed an LL(k + 1)-\ngrammar without empty rules that defines the same language.\n\nTheorem 2\nFor each LL(k)-\ngrammar without empty rules, it is possible to construct an LL(k)-\ngrammar in N. V. Greibach that defines the same language."
    },
    {
        "question": "Formulate theorems on the construction of LL(k)-grammars without empty rules.",
        "answer": "\nTheorem 1\nFor every LL(k)-\ngrammar, there exists and can be effectively constructed an LL(k + 1)-\ngrammar without empty rules defining the same language.\n\nTheorem 2\nFor any LL(k)-grammar without empty rules, we can construct an LL(k)-grammar:\ngrammar in N. V. Greibach defining the same language."
    },
    {
        "question": "Formulate theorems on the construction of LL(k)-grammars without empty rules.",
        "answer": "\nTheorem 1\nFor any LL(k)-grammar, there exists and can be effectively constructed an LL(k + 1)-grammar without empty rules defining the same language.\n\nTheorem 2\nFor any LL(k)-grammar without empty rules, one can construct an LL(k)-grammar of N.V. Greibach defining the same language."
    },
    {
        "question": "Formulate the Rosenkranz-Stearns lemma for ordinary grammar.",
        "answer": "\nLemma 1 (Rosenkranz and Stearns [1970]). For every ordinary grammar G =\n(Σ, N, R, S), there is another grammar G0 = (Σ, N ∪ N0, R0, S0), where N0 = { A0| A ∈\nN }, satisfying the following conditions.\n1. Every nonterminal symbol A ∈ N defines in G0 the same language as in G.\n2. Every nonterminal symbol A0 ∈ N in G0\ndefines the same language as in G, with\nthe empty string excluded: LG0(A0) = LG(A0) \\ {ε}. In particular, L(G0) = L(G) \\ {ε}.\n3. No rule from R0 begins with a non-terminal symbol from N.\n4. If G is LL(k), then G0 is also LL(k)."
    },
    {
        "question": "Formulate the Rosenkranz-Stearns lemma for ordinary grammar.",
        "answer": "\nLemma 1 (Rosenkranz and Stearns [1970]). For every ordinary grammar G =\n(Σ, N, R, S), there is another grammar G0 = (Σ, N ∪ N0, R0, S0), where N0 = { A0| A ∈\nN }, satisfying the following conditions.\n1. Every nonterminal symbol A ∈ N in G0\ndefines the same language as in G.\n2. Every nonterminal symbol A0 ∈ N in G0\ndefines the same language as in G, with\nthe empty string excluded: LG0(A0) = LG(A0) \\ {ε}. In particular, L(G0) = L(G) \\ {ε}.\n3. No rule in R0 begins with a nonterminal symbol in N.\n4. If G is LL(k), then G0 is also LL(k)."
    },
    {
        "question": "Formulate the Rosenkranz-Stearns lemma for ordinary grammar.",
        "answer": "\nLemma 1 (Rosenkranz and Stearns [1970]). For any ordinary grammar G =\n(Σ, N, R, S), there is another grammar G0 = (Σ, N ∪ N0, R0, S0), where N0 = { A0| A €\nN }, satisfying the following conditions.\n1. Any nonterminal symbol A ∈ N defines in G0 the same language as in G.\n2. Any nonterminal symbol A0 ∈ N in G0\ndefines the same language as in G, with\nexcluded by the empty string: LG0(A0) = LG(A0) \\ {ε}. In particular, L(G0) = L(G) \\ {ε}.\n3. No rule R0 begins with a non-terminal symbol N.\n4. If G is LL(k), then G0 is also LL(k)."
    },
    {
        "question": "Formulate the Rosenkranz-Stearns lemma for ordinary grammar.",
        "answer": "\nLemma 1 (Rosenkranz and Stearns [1970]). For any ordinary grammar G =\n(Σ, N, R, S), there is another grammar G0 = (Σ, N ∪ N0, R0, S0), where N0 = { A0| A €\nN }, satisfying the following conditions.\n1. Any nonterminal symbol A ∈ N defines the same language in G0 as in G.\n2. Any nonterminal symbol A0 ∈ N in G0\ndefines the same language as in G, except for\nthe empty string: LG0(A0) = LG(A0) \\ {ε}. In particular, L(G0) = L(G) \\ {ε}.\n3. No rule R0 begins with a non-terminal symbol N.\n4. If G is LL(k), then G0 is also LL(k)."
    },
    {
        "question": "Formulate the Rosenkranz-Stearns lemma for LL(k)-grammar",
        "answer": "\nLemma 2 (Rosenkranz and Stearns [1970]). Let G = (Σ, N, R, S) be an LL(k)-grammar\ncontaining no rules of the form A → Bγ, where ε ∈ LG(B). Let the nullable nonterminal\nsymbols be denoted by N0 = { A | A ∈ N, ε ∈ LG(A) }, and let N1 = { A | A ∈ N, ε /∈\nLG(A) } be all the rest. Then there exists an LL(k + 1)-grammar G0 = (Σ, N0, R0, S0), where N0 is a finite set of non-terminal symbols of the form [Xθ], where X ∈ Σ ∪ N1 and\nθ ∈ N∗0, and LG0([Xθ]) = LG(Xθ) for every [Xθ] ∈ N0\n[Xθ] ∈ N0.\nMoreover, the set N0 does not contain any element of the form [XθAθ0Aθ00], which\nwould contain repeated occurrences of the same non-terminal symbol."
    },
    {
        "question": "Formulate the Rosenkranz-Stearns lemma for LL(k)-grammar.",
        "answer": "\nLemma 2 (Rosenkranz and Stearns [1970]). Let G = (Σ, N, R, S) be an LL(k)-grammar\ncontaining no rules of the form A → Bγ, where ε ∈ LG(B). Let the nullable nonterminal\nsymbols be denoted by N0 = {A | A ∈ N, ε ∈ LG(A)}, and N1 = {A | A ∈ N, ε /∈\nLG(A)} the rest. Then there exists an LL(k + 1)-grammar G0 = (Σ, N0, R0, S0), where N0 is a finite set of non-terminal symbols of the form [Xθ], where X ∈ Σ ∪ N1 and\nθ ∈ N∗0, and LG0([Xθ]) = LG(Xθ) for any [Xθ] ∈ N0\n[Xθ] ∈ N0.\nMoreover, the set N0 does not contain any element of the form [XθAθ0Aθ00] that\nwould contain repeated occurrences of the same non-terminal symbol."
    },
    {
        "question": "Formulate the Rosenkranz-Stearns lemma for LL(k)-grammar.",
        "answer": "\nLemma 2 (Rosenkranz and Stearns [1970]). Let G = (Σ, N, R, S) be an LL(k)-grammar\ncontaining no rules of the form A → Bγ, where ε ∈ LG(B). Leave the nonterminal value null\nsymbols are denoted by N0 = {A | A ∈ N, ε ∈ LG(A)} and N1 = {A | A ∈ N, ε /ε\nЛГ(А)} are all the rest. Then there exists an LL(k + 1)-grammar G0 = (Σ, N0, R0, S0), where N0 is a finite set of non-terminal symbols of the form [Xθ], where X ∈ Σ ∪ N1 and\nθ ∈ N∗0 and LG0([Xθ]) = LG(Xθ) for all [Xθ] ∈ N0\n[Xθ] ∈ N0.\nIn this case, the set N0 does not contain any element of the form [XθAθ0Aθ00], which\nwill contain repeated occurrences of the same non-terminal symbol."
    },
    {
        "question": "Formulate the Rosenkranz-Stearns lemma for LL(k)-grammar.",
        "answer": "\nLemma 2 (Rosenkranz and Stearns [1970]). Let G = (Σ, N, R, S) be an LL(k)-grammar\ncontaining no rule of the form A → Bγ, where ε ∈ LG(B). Let nonterminal symbols be NULL.\ndenoted by N0 = { A | A ∈ N, ε ∈ LG(A) }, and let N1 = { A | A ∈ N, ε /ε\nLG(A) } be everything else. Then there exists an LL(k + 1)-grammar G0 = (Σ, N0, R0, S0), where N0 is a finite set of non-terminal symbols of the form [Xθ], where X ∈ Σ ∪ N1 and\nθ ∈ N∗0 and LG0([Xθ]) = LG(Xθ) for all [Xθ] ∈ N0\n[Xθ] ∈ N0.\nIn this case, the set N0 does not contain any element of the form [XθAθ0Aθ00], which\nwould contain repeated occurrences of the same non-terminal symbol."
    },
    {
        "question": "Write a theorem on the disjoint union of LL(k)-languages",
        "answer": "\nTheorem (On the disjoint union of LL(k)-languages: Rosenkranz and Stearns [1970]).\nLet L1, . . , Ln ⊆ Σ\n∗ be pairwise disjoint LL(k)-languages, and let their union\nL1 ∪ . . . ∪ Ln be regular. Then all languages ​​L1, . . . , Ln must be regular."
    },
    {
        "question": "Write a theorem on the disjoint union of LL(k)-languages.",
        "answer": "\nTheorem (On the disjoint union of LL(k)-languages: Rosenkranz and Stearns [1970]).\nLet L1, . . , Ln ⊆ Σ\n∗ be pairwise disjoint LL(k)-languages, and let their union\nL1 ∪ . . . ∪ Ln be regular. Then all languages ​​L1, . . . , Ln must be regular."
    },
    {
        "question": "Write a theorem on the disjoint union of LL(k) languages.",
        "answer": "\nTheorem (On disjoint union of LL(k) languages): Rosenkranz and Stearns [1970]).\nLet L1, . . , Ln ⊆Σ\n∗ be pairwise disjoint LL(k) languages ​​and their union\nЛ1 ∪ . . . ∪ Ln be regular. Then all languages ​​L1, . . . , Ln must be regular."
    },
    {
        "question": "Write a theorem on the disjoint union of LL(k) languages.",
        "answer": "\nTheorem (On the disjoint union of LL(k) languages): Rosenkranz and Stearns [1970]).\nLet L1, . . , Ln ⊆ Σ\n∗ be two by two disjoint LL(k) languages, and let their union be\nЛ1 ∪ . . . ∪ Regular Ln. Thus all L1, . . . , Ln must be regular."
    },
    {
        "question": "Tell me about the properties of LL grammars",
        "answer": "\nIt is easy to construct a grammar that is LL(k) but not LL(k − 1): for example, S →\na\nk\n| a\nk−1 is such a grammar. But in this example, there is an obvious LL(1) grammar for\nthe same language: S → a\nk−1A, A → a | ε. Does the class of languages ​​grow with k? Yes.\nLemma 1. Let G = (Σ, N, R, S) be an LL(k) grammar without empty rules, and let evaluating the LL(k) parser on some string w = xyz after reading x yield\nthe configuration (yz, Y γ), and then after reading y yield the configuration (z, γ). Then for every string z\n0 that matches z in the first k − 1 symbols, the computation on xyz0 comes to the\nconfiguration (z\n0, γ).\nSketch of proof. In parsing y as Y , the parser sees at least one last symbol of y for each rule choice. Therefore, the differences between z and z\n0 for this\nchoice will be beyond the horizon, and hence the parser will read y in the same way in both\ncases.\nTheorem 1 (Kurki-Suonio [1969]; Rosenkranz and Steenrs [1970]). For every k > 1, there exists a language Lk that is determined by an LL(k)-grammar, but not by any LL(k)-grammar without empty rules — and hence not by any LL(k − 1)-grammar.\nProof. The language Lk is defined as Lk = S\nn>1\na^n{b^kd, b, c}^n, and it is described by\nthe following grammar.\nS → aCA\nC → aCA | ε\nA → bB | c\nB → b\nk−1\nd | ε\nTo verify that this grammar is LL(k), we must check that the parser can\nchoose a rule for B. Indeed, the rule B → ε is chosen for all Firstk(LG(A)\n+) strings it scans, all of which consist entirely of b and c: the string b\nk−1d\nis not one of these strings.\nThe negative result is obtained by the method of the substituted string in front. If Lk\nis defined by an LL(k)-grammar without empty rules, then, by the theorem from the previous lecture, there is\nan LL(k)-grammar G = (Σ, N, R, S) in Greibach normal form defining the language L(G) = Lk.\nLet αn be the stack contents of the corresponding LL(k)-parser at the input a\nn+k−1\n. . .,\nafter reading the symbols u = a\nn\n.\nClaim. αm 6= αn for all m 6= n.\nClaim. There exists a number n > 0 such that |αn| > 2k − 1"
    },
    {
        "question": "Tell me about the properties of LL grammars",
        "answer": "\nIt is easy to construct a grammar that is LL(k) but not LL(k − 1): for example, S →\na\nk\n| a\nk−1 is such a grammar. But in this example, there is an obvious LL(1) grammar for\nthe same language: S → a\nk−1A, A → a | ε. Does the class of languages ​​grow with k? Yes.\nLemma 1. Let G = (Σ, N, R, S) be an LL(k) grammar without dummy rules, and let evaluating an LL(k) parser on some string w = xyz after reading x yield\nthe configuration (yz, Y γ), and then after reading y yield the configuration (z, γ). Then for every string z\n0 that matches z in the first k − 1 symbols, evaluating xyz0 comes to the\nconfiguration (z\n0, γ).\nSketch of proof. When parsing y as Y, the parser sees at least one last symbol of y for each choice of the rule. Therefore, the difference between z and z\n0 for that choice will be over the horizon, and hence the parser will read y the same in both\ncases.\nTheorem 1 (Kurki-Suonio [1969]; Rosenkranz and Stiners [1970]). For every k > 1, there is a language Lk that is determined by an LL(k)-grammar, but not by every LL(k)-grammar without empty rules — and hence not by every LL(k − 1)-grammar.\nProof. The language Lk is defined as Lk = S\nn>1\na^n{b^kd, b, c}^n, and it is described by\nthe following grammar.\nS → aCA\nC → aCA | ε\nA → bB | c\nB → b\nk−1\nd | ε\nTo verify that this grammar is LL(k), we must check that the parser can\nchoose a rule for B. Indeed, the rule B → ε is chosen for all Firstk(LG(A)\n+) strings it scans, all of which consist entirely of b and c: string b\nk−1d\nis not one of these strings.\nThe negative result is obtained by the prepended string method. If Lk\nis defined by an LL(k)-grammar without empty rules, then by the theorem from the previous lecture there exists\nan LL(k)-grammar G = (Σ, N, R, S) in Greibach normal form defining the language L(G) = Lk.\nLet αn be the stack contents of the corresponding LL(k)-parser at the input a\nn+k−1\n. . .,\nafter reading the symbols u = a\nn\n.\nClaim. αm 6= αn for all m 6= n.\nClaim. There exists a number n > 0 such that |αn| > 2k − 1"
    },
    {
        "question": "Tell me about the properties of LL grammars.",
        "answer": "\nIt is easy to construct a grammar that is LL(k) but not LL(k − 1): for example, S →\nA\nk\n| A\nk−1 is such a grammar. But in this example, there is an obvious LL(1) grammar for\nthe same language: S → a\nk−1A, A → a | e. Does the class of languages ​​grow with k? Yes.\nLemma 1. Let G = (Σ, N, R, S) be an LL(k) grammar without empty rules, and let an LL(k) parser evaluate a string w = xyz after reading x and yield\nthe configuration (yz, Y γ), then after reading y it yields the configuration (z, γ). Thus, for every string z\n0 that matches z in the first k − 1 symbols, computing xyz0 reduces to\nconfiguration (z\n0,y).\nSketch of proof. When parsing y as Y, the parser sees at least one last symbol of y for each choice of the rule. Thus, the differences between z and z\nzeros for that choice will be over the horizon, and so the parser will read them the same in both cases.\ncase.\nTheorem 1 (Kurki-Suonio [1969]; Rosenkranz and Stiners [1970]). For all k > 1, there exists a language Lk that is defined by an LL(k) grammar, but not by an LL(k) grammar without empty rules, and hence not by an LL(k − 1) grammar.\nProof. The language Lk is defined as Lk = S\nn>1\na^n{b^kd, b, c}^n, and this describes\nthe following grammar.\nS → aCA\nC → aCA | ε\nA → bB | c\nB → b\nk-1\nd | ε\nTo verify that this grammar is LL(k), we must check that the parser can\nchoose a rule for B. Indeed, the rule B → ε is chosen for all Firstk(LG(A)\n+) scanned strings that consist entirely of b and c: string b\nk−1d\nis not one of these channels.\nThe negative result is obtained by inserting the line in front. If Luc\nis defined by an LL(k)-grammar without empty rules, then by the theorem of the previous course there exists\nan LL(k)-grammar G = (Σ, N, R, S) in Greibach normal form defining a language L(G) = Lk.\nLet αn be the stack contents of the LL(k) parser corresponding to the input a\nn+k−1\n. . .,\nafter reading the symbols u=a\nн\n.\nClaim. αm 6= αn for all m 6= n.\nClaim. There exists a number n > 0 such that |αn| > 2к - 1"
    },
    {
        "question": "Tell me about the properties of LL grammars.",
        "answer": "\nIt is easy to construct a grammar that is LL(k) but not LL(k − 1): for example, S →\nhas\nk\n| has\nk−1 is such a grammar. But in this example, there is an obvious LL(1) grammar for\nthe same language: S → a\nk−1A, A → a | e. Does the class of languages ​​grow with k? Yes.\nLemma 1. Let G = (Σ, N, R, S) be an LL(k) grammar without empty rules, and let the LL(k) parser evaluation of a string w = xyz after reading x yield\nthe configuration (yz, Y γ), then after reading y yield the configuration (z, γ). Then for every string z\n0 that matches z in the first k − 1 symbols, evaluating xyz0 yields\nconfiguration (z\n0, γ).\nSketch of proof. By parsing y as Y, the parser sees at least one last symbol of y for each rule choice. Hence, the difference between z and z\n0 for this\nchoice will be over the horizon, and so the parser will read the same in both cases.\ncase.\nTheorem 1 (Kurki-Suonio [1969]; Rosenkranz and Steenrs [1970]). For all k > 1, there exists a language Lk that is defined by an LL(k) grammar, but not by an LL(k) grammar without empty rules — and hence not by an LL(k − 1) grammar.\nProof. The language Lk is defined as Lk = S\nn>1\na^n{b^kd, b, c}^n, and it is described by the formula\nthe following grammar.\nS → aCA\nC → aCA | ε\nA → bB | c\nB → b\nk-1\nd | ε\nTo verify that this grammar is LL(k), we must verify that the parser can\nchoose a rule for B. Indeed, the rule B → ε is chosen for all Firstk(LG(A)\n+) strings it parses, each of which consists entirely of b and c: the string b\nk−1d\nis not one of these channels.\nThe negative result is obtained by the forward substitution string method. If Lk is defined by an LL(k)-grammar without empty rules, then by the theorem of the previous lesson there exists an LL(k) grammar G = (Σ, N, R, S) in Greibach normal form defining a language L(G) = Lk.\nLet αn be the stack contents of the LL(k) parser corresponding to the input a\nn+k−1\n. . .,\nafter reading the symbols u = a\nн\n.\nClaim. αm 6= αn for all m 6= n.\nClaim. There exists a number n > 0 such that |αn| > 2к - 1"
    },
    {
        "question": "What are local automata?",
        "answer": "\nDefinition:\nAn automaton A\nis called local (English local automaton, Glushkov automaton), if for any c\nfrom Σ\nthe set {δ(s,c)∣s∈S}\ncontains at most one element.\n\nDefinition:\nA local automaton A\nis called a standard local automation (English standard local automation), if it has no transition to the initial state.\n\nThus, the automaton is local if for each c\nfrom Σ\nthere are no transitions marked with c, or if all of them lead to the same state.\n\nLet us show that the Myhill graph can be transformed into a standard local automaton in such a way that the language it recognizes does not change.\n\nTheorem:\nA language is recognized by the Myhill graph if and only if it is recognized by a standard local automaton whose initial state is not terminal."
    },
    {
        "question": "What are local automata?",
        "answer": "\nDefinition:\nA machine A\nis called local (Glushkov automaton) if for any c\nfrom Σ\nthe set {δ(s,c)∣s∈S}\ncontains at most one element.\n\nDefinition:\nA local machine A\nis called a standard local automation (standard local automation) if it has no transition to the initial state.\n\nThus, the machine is local if for each c\nfrom Σ\nthere are no transitions marked with c, or if they all lead to the same state.\n\nWe will show that the Myhill graph can be transformed into a standard local automaton in such a way that the language it recognizes does not change.\n\nTheorem:\nA language is recognized by the Myhill graph if and only if it is recognized by a standard local automaton whose initial state is not terminal."
    },
    {
        "question": "What are local machines?",
        "answer": "\nDefinition:\nA machine A\nis called local (English local automaton, Glushkov automaton), if for any c\nΣ\nthe set {δ(s,c)∣sεS}\ncontains no more than one element.\n\nDefinition:\nA local PLC A\nis called a standard local automation if it does not include a transition to the initial state.\n\nThus, the machine is local if for each c\nΣ\nthere are no transitions marked with the letter c, or they all lead to the same state.\n\nWe will show that the Myhill graph can be transformed into a standard local automaton so that the language it recognizes does not change.\n\nTheorem:\nA language is recognized by the Myhill graph if and only if it is recognized by a standard local automaton whose initial state is not terminal."
    },
    {
        "question": "What are local automata?",
        "answer": "\nDefinition:\nAutomaton A\nis called local (in English local automaton, Glushkov automaton), if for all c\nΣ\nthe set {δ(s,c)∣sεS}\ncontains at most one element.\n\nDefinition:\nLocal automaton A\nis called standard local automation (in English Standard Local Automation), if it does not represent any transition to the initial state.\n\nThus, the automaton is local if for all c\nΣ\nthere are no transitions marked with the letter c, or they all lead to the same state.\n\nWe will show that the Myhill graph can be transformed into a standard local automaton so that the language it recognizes does not change.\n\nTheorem:\nA language is recognized by the Myhill graph if and only if it is recognized by a standard local automaton whose initial state is not terminal."
    },
    {
        "question": "Corollary of Turchin's theorem",
        "answer": "\nLet G be an alphabetic prefix grammar with N rules with non-empty right-hand side, and the maximum length of the right-hand side of a rule is equal to M. Then any sequence of words generated by it\na_1...a_n\n...\nε\nof length at least N^M*(n+1) contains a pair of the form τ_1=Φ*Θ_0, τ_2=Φ*Ψ*Θ_0 such that |Φ|<=M and on the interval [τ_1, τ_2] there are no words of length less than |Θ_0|+1."
    },
    {
        "question": "Corollary of Turchin's theorem",
        "answer": "\nLet G be an alphabetic prefix grammar with N rules with a non-empty right-hand side, and the maximum length of the right-hand side of a rule is M. Then any sequence of words generated by it\na_1...a_n\n...\nε\nof length at least N^M*(n+1) contains a pair of the form τ_1=Φ*Θ_0, τ_2=Φ*Ψ*Θ_0 such that |Φ|<=M and on the interval [τ_1, τ_2] there are no words of length less than |Θ_0|+1."
    },
    {
        "question": "Corollary of Turchin's theorem",
        "answer": "\nLet G be an alphabetic prefix grammar with N rules and a non-empty right-hand side, and the maximum length of the right-hand side of a rule is M. Then any sequence of words generated by it,\na_1...a_n\n...\nε\nof length at least N^M*(n+1), contains a pair of the form τ_1=Φ*Θ_0, τ_2=Φ*Ψ*Θ_0 such that |Φ|<=M and on the interval [τ_1, τ_2 ] there are no words of length less than |Θ_0|+1."
    },
    {
        "question": "Corollary of Turchin's theorem",
        "answer": "\nLet G be an alphabetic grammar with a prefix of N rules with a non-empty right-hand side and the maximum length of the right-hand side of a rule of which is equal to M. Then any sequence of words generated by it,\na_1...a_n\n...\nε\nof length at least N^M*(n+1) contains a pair of the form τ_1=Φ*Θ_0, τ_2=Φ*Ψ*Θ_0 such that |Φ|<=M and on the interval [τ_1, τ_2] there are no words of length less than |Θ_0|+1."
    },
    {
        "question": "Synchronization criterion",
        "answer": "\nDFA A synchronizing <=> ∀q,q'∃w, q_x(q-w->q_x & q' -w->q_x)."
    },
    {
        "question": "Synchronization criterion",
        "answer": "\nDFA A synchronizing <=> ∀q,q'∃w, q_x(q-w->q_x & q' -w->q_x)."
    },
    {
        "question": "Synchronization criterion",
        "answer": "\nSynchronization DFA A <=> ∀q,q'∃w, q_x(q-w->q_x & q' -w->q_x)."
    },
    {
        "question": "Synchronization criterion",
        "answer": "\nDFA Synchronization <=> ∀q,q'∃w, q_x(q-w->q_x & q' -w->q_x)."
    },
    {
        "question": "Prove that the set of automaton grammars forms a Boolean algebra",
        "answer": "\nThe class of regular sets forms a Boolean algebra. Is the class of regular sets closed under reverse?\nΩ=<∨,∧,¬> - signature of Boolean algebra\nProperties of Boolean algebra:\n● A∧B=B∧A - commutativity under multiplication\n● A∧(B∧C)=(A∧B)∧C - associativity under multiplication\n● A∧(B∨C)=(A∧B) ∨(A∧C) - distributivity\n● ¬¬A=A\n● A∨¬A=1\n● A∧¬A=0\n● A∧1=A\n● A∧0=A\n● A∨0=A\n● A∨1=1\nProof:\nThe properties of addition and multiplication operations follow from the Lemma on regular expressions\nThe complement of a regular set is regular ⇐ if the set is regular, then it is\nrecognized by the automaton. If in the automaton M we change the set of final states to Q\\F, then\nwe obtain an automaton that recognizes the complement of a given language."
    },
    {
        "question": "Prove that the set of automaton grammars forms a Boolean algebra.",
        "answer": "\nThe class of regular sets forms a Boolean algebra. Is the class of regular sets closed under its inverse?\nΩ=<∨,∧,¬> - signature of Boolean algebra\nProperties of Boolean algebra:\n● A∧B=B∧A - commutativity under multiplication\n● A∧(B∧C)=(A∧B)∧C - associativity under multiplication\n● A∧(B∨C)=(A∧B) ∨(A∧C) - distributivity\n● ¬¬A=A\n● A∨¬A=1\n● A∧¬A=0\n● A∧1=A\n● A∧0=A\n● A∨0=A\n● A∨1=1\nProof:\nThe properties of addition and multiplication follow from the Lemma on regular expressions\nThe complement of a regular set is regular ⇐ if the set is regular, then it is\nrecognized by the automaton. If in the automaton M we change the set of final states to Q\\F, then\nwe will get an automaton that recognizes the complement of a given language."
    },
    {
        "question": "Show that the set of grammars of automata forms a Boolean algebra.",
        "answer": "\nThe class of regular sets forms a Boolean algebra. Is the class of regular sets closed under its inverse?\nΩ=<∨,∧,¬> - signature of Boolean algebra\nProperties of Boolean algebra:\n● A∧B=B∧A – commutativity under multiplication\n● A∧(B∧C)=(A∧B)∧C – associativity under multiplication\n● A∧(B∨C)=(A∧B) ∨(A∧C) - distributivity\n● ¬¬A=A\n● A∨¬A=1\n● A∧¬A=0\n● A∧1=A\n● A∧0=A\n● A∨0=A\n● A∨1=1\nProof:\nThe properties of addition and multiplication operations follow from the lemma on regular expressions.\nThe complement of a regular set is regular ⇐ if the set is regular, then it is\nrecognized automatically. If we change the set of final states in the automaton M to Q\\F, then\nwe will get an automaton that recognizes the complement of the given language."
    },
    {
        "question": "Show that the set of grammars of automata forms a Boolean algebra.",
        "answer": "\nThe class of regular sets forms a Boolean algebra. Is the class of regular sets closed by inversion?\nΩ=<∨,∧,¬> - signature of Boolean algebra\nProperties of Boolean algebra:\n● A∧B=B∧A - commutativity under multiplication\n● A∧(B∧C)=(A∧B)∧C - associativity under multiplication\n● A∧(B∨C)=(A∧B) ∨(A∧C) - distributivity\n● ¬¬A=A\n● A∨¬A=1\n● A∧¬A=0\n● A∧1=A\n● A∧0=A\n● A∨0=A\n● A∨1=1\nProof:\nThe properties of addition and multiplication operations follow from the lemma on regular expressions\nThe complement of a regular set is regular ⇐ if the set is regular, then it is\nrecognized by the automaton. If in the automaton M we change the set of final states to Q\\F, then\nwe obtain an automaton that recognizes the complement of the given language."
    },
    {
        "question": "Formulate the Myhill-Nerode theorem",
        "answer": "\nLanguage L is a VPL => the set of balanced words (i.e. those in which each symbol from Σ_c has a corresponding symbol from Σ_r) is partitioned into a finite number of Myhill-Nerode equivalence classes with respect to L."
    },
    {
        "question": "Formulate the Myhill-Neroda theorem.",
        "answer": "\nLanguage L is a VPL => the set of balanced words (i.e. those in which each symbol from Σ_c has a corresponding symbol from Σ_r) is partitioned into a finite number of Myhill-Nerode equivalence classes with respect to L."
    },
    {
        "question": "Formulate the Myhill-Neroda theorem.",
        "answer": "\nThe language L is a VPL => the set of balanced words (i.e. those in which each symbol Σ_c has a corresponding symbol Σ_r) is partitioned into a finite number of Myhill-Neroda equivalence classes compared to L."
    },
    {
        "question": "Formulate the Myhill-Neroda theorem.",
        "answer": "\nLanguage L is a VPL => the set of balanced words (i.e. those in which each symbol Σ_c has a corresponding symbol Σ_r) is partitioned into a finite number of Myhill-Neroda equivalence classes compared to L."
    },
    {
        "question": "What is conjunctive grammar?",
        "answer": "\nA conjunctive grammar G is a grammar whose rules are of the form A_i -> Φ_1 & ... & Φ_2 where A_i is a nonterminal; Φ_j are strings in a mixed alphabet of terminals and nonterminals."
    },
    {
        "question": "What is conjunctive grammar?",
        "answer": "\nA conjunctive grammar G is a grammar whose rules are of the form A_i -> Φ_1 & ... & Φ_2, where A_i is a nonterminal; Φ_j are strings in a mixed alphabet of terminals and nonterminals."
    },
    {
        "question": "What is connective grammar?",
        "answer": "\nA conjunctive grammar G is a grammar whose rules are of the form A_i -> Φ_1 & ... & Φ_2, where A_i is a nonterminal; Φ_j are strings in a mixed alphabet of terminals and nonterminals."
    },
    {
        "question": "What is connective grammar?",
        "answer": "\nA conjunctive grammar G is a grammar whose rules are of the form A_i -> Φ_1 & ... & Φ_2, where A_i is a nonterminal; Φ_j are strings in a mixed alphabet of terminals and nonterminals."
    },
    {
        "question": "What is a wood machine?",
        "answer": "\nA tree automaton is defined by an input alphabet (signature of constructors) Σ⊂{⟨f, n⟩} and a final state, as well as transition rules: ⟨f, n⟩ ∈ Σ => (q_1,..., q_n, f)->q_s."
    },
    {
        "question": "What is a woodworking machine?",
        "answer": "\nThe tree of the automaton is defined by the input alphabet (signature of constructors) Σ⊂{⟨f, n⟩} and the final state, as well as the transition rules: ⟨f, n⟩ ∈ Σ => (q_1,..., q_n, f)->q_s."
    },
    {
        "question": "What is a woodworking machine?",
        "answer": "\nThe tree of automata is defined by the input alphabet (the signature of the constructor) Σ⊂{⟨f, n⟩} and the final state, as well as the transition rules: ⟨f, n⟩ ∈ Σ => (q_1,.. ., q_n, f)->q_s."
    },
    {
        "question": "What is a woodworking machine?",
        "answer": "\nA tree-like automaton is defined by an input alphabet (signature of constructors) Σ⊂{⟨f, n⟩} and a final state, as well as transition rules: ⟨f, n⟩ ∈ Σ => (q_1,... , q_n, f)->q_s."
    },
    {
        "question": "Consequences of Parikh's theorem",
        "answer": "\n1)The sets of regular and CF-languages ​​over a single-letter alphabet coincide.\n2)The commutative image of a CF-language is a regular language."
    },
    {
        "question": "Consequences of Parikh's theorem",
        "answer": "\n1) The sets of regular and CF-languages ​​over a single-letter alphabet coincide.\n2) The commutative image of a CF-language is a regular language."
    },
    {
        "question": "Consequences of Parikh's theorem",
        "answer": "\n1) The sets of regular and CF languages ​​on a single-letter alphabet coincide.\n2) The commutative image of the CF language is a regular language."
    },
    {
        "question": "Consequences of Parikh's theorem",
        "answer": "\n1) The sets of ordinary and CF languages ​​on a single-letter alphabet coincide.\n2) The commutative image of a CF language is a regular language."
    },
    {
        "question": "The intersection of KS grammar and regular language",
        "answer": "\nGiven a finite-state grammar G and a finite-state machine A, we can construct a finite-state grammar G' such that L(G')=L(G)∩L(A)."
    },
    {
        "question": "The intersection of KS grammar and ordinary language",
        "answer": "\nGiven a finite grammar G and a finite automaton A, we can construct a finite grammar G' such that L(G')=L(G)∩L(A)."
    },
    {
        "question": "The intersection of CS grammar and ordinary language.",
        "answer": "\nGiven a finite grammar G and a finite automaton A, we can construct a finite grammar G' such that L(G')=L(G)∩L(A)."
    },
    {
        "question": "The intersection of CS grammar and ordinary language.",
        "answer": "\nGiven a finite grammar G and a finite automaton A, we can construct a finite grammar G' such that L(G')=L(G)∩L(A)."
    },
    {
        "question": "What is a stacking machine?",
        "answer": "\nStack automaton A is a tuple ⟨Π, Σ, Q, δ, q_0, Z_0⟩, where:\nΠ is the stack alphabet;\nΣ is the language alphabet;\nQ is the set of states;\nδ is the transition rules of the form ⟨q_i, t, P_i⟩ -> ⟨q_j, α⟩, where t ∈ Σ ∪ {ε}, α ∈ Π*\nq_0 is the starting state;\nZ_0 is the bottom of the stack."
    },
    {
        "question": "What is a stacker?",
        "answer": "\nA stack automaton A is a tuple ⟨Π, Σ, Q, δ, q_0, Z_0⟩, where:\nΠ is the stack alphabet;\nΣ is the language alphabet;\nQ is the set of states;\nδ are the transition rules of the form ⟨q_i, t, P_i⟩ -> ⟨q_j, α⟩, where t ∈ Σ ∪ {ε}, α ∈ Π*\nq_0 is the initial state;\nZ_0 is the bottom of the stack."
    },
    {
        "question": "What is a stacker?",
        "answer": "\nA stack automaton A is a tuple ⟨Π, Σ, Q, δ, q_0, Z_0⟩, where:\nП is the stack alphabet;\nΣ is the language alphabet;\nQ is the set of states;\nδ are the transition rules of the form ⟨q_i, t, P_i⟩ -> ⟨q_j, α⟩, where t ∈ Σ ∪ {ε}, α ∈ Π*\nq_0 is the initial state;\nZ_0 is the bottom of the stack."
    },
    {
        "question": "What is a stacker?",
        "answer": "\nA stack automaton A is a tuple ⟨Π, Σ, Q, δ, q_0, Z_0⟩, where:\nΠ is the stack alphabet;\nΣ is the language alphabet;\nQ is the set of states;\nδ are the transition rules of the form ⟨q_i, t, P_i⟩ -> ⟨q_j, α⟩, where t ∈ Σ ∪ {ε}, α ∈ Π*\nq_0 is the initial state;\nZ_0 is the bottom of the stack."
    },
    {
        "question": "Prove Early's algorithm",
        "answer": "____⟹\n\nWe will prove by induction on the execution of the algorithm.\nInduction base:\n[S′→⋅S,0]∈D0.\nInduction step:\nLet the assumption be true for all lists of situations with numbers less than j. Let us figure out as a result of applying which rule the situation [A→α⋅β,i] got into Dj\n1. We include by the scan rule. This happened if α=α′a, a=wj−1 and [A→α′⋅aβ,i]∈Dj−1.\nBy the induction hypothesis S′⇒∗w0…wi−1Aδ and α′⇒∗wi…wj−2, then by virtue of a=wj−1 we obtain α=α′a⇒∗wi…wj−2wj−1=wi…wj−1. Thus, the conditions: S′⇒∗w0…wi−1Aδ and α⇒∗wi…wj−1 are satisfied.\n2. We include by the predict rule. By construction: α=ε and i=j, which automatically implies the second point of the assertion.\nMoreover, ∃i′≤i and the situation [A′→α′⋅Aδ′,i′]∈Di, from which, by the induction hypothesis, it follows that S′⇒∗w0…wi′−1A′δ′′ and α′⇒∗wi′…wi−1. We get that S′⇒∗w0…wi′−1A′δ′′, which means S⇒∗w0…wi′−1α′Aδ′δ′′, therefore S′⇒∗w0…wi′−1wi′…wi−1Aδ′δ′′, as a result S′⇒∗w0…wi−1Aδ, which is what we needed.\n3. We include according to the complete rule. By construction: α=α′A′ and ∃i′,δ:[A→α′⋅A′β,i]∈Di′∧[A′→η⋅,i′]∈Dj. Therefore, α=α′A ⇒∗wi…wi′−1wi′…wj=wi…wj−1, which gives us the second point of the assertion, and since the first point follows from the induction hypothesis, everything is fine.\n\n⟸\n\nWe will prove the opposite by induction on the total length of the output w0…wi−1Aδ from S′ and wi…wj−1 from α. After that, we apply induction on the length of the output wi…wj−1 from α. Let us consider three cases of the last symbol α:\n1. α=α′a, then a=wj−1 and α′⇒∗wi…wj−2. By the induction hypothesis: [A→α′⋅aβ,i]∈Dj−1, and from here by the scan rule we obtain [A→α′a⋅β,i]∈Dj.\n2. α=α′B, then ∃i′:α′⇒∗wi…wi′−1∧B′⇒∗wi′…wj−1. Then we have [A→α′a⋅β,i]∈Dj. You can also write S′⇒∗w0…wi−1Aδ as S′⇒∗w0…wi−1wi…wi′−1Bβδ, as well as B→η∧η→wi′…wj−1.\nApplying induction on the second parameter we obtain [B→η⋅,i′]∈Dj, from which, using the complete rule, we obtain [A→α′B⋅β,i]∈Dj.\n\n3. α=ε, then i=j. Then either i=0∧A=S∧δ=ε, which proves the base of induction, or the conclusion can be written as S′⇒∗w0…wi′−1wi′…wi−1Aδ′δ′′=w0…wi−1Aδ for some rule (A′→wi′…wi−1Aδ′)∈P. Hence, by the induction hypothesis, [A′→⋅wi′…wi−1Aδ′,i′]∈Di′ , which after several applications of the scan rule leads to [A′→wi′…wi−1⋅Aδ′,i′]∈Di, after which, by the predict rule, we obtain [A→⋅β,i]∈Dj, as required."
    },
    {
        "question": "Prove Early's algorithm",
        "answer": "____⟹\n\nWe will prove by induction on the execution of the algorithm.\nInduction base:\n[S′→⋅S,0]∈D0.\nInduction step:\nLet the assumption be true for all lists of situations with numbers less than j. Let us figure out as a result of applying which rule the situation [A→α⋅β,i] got into Dj\n1. We include it by the scanning rule. This happened if α=α′a, a=wj−1 and [A→α′⋅aβ,i]∈Dj−1.\nBy the induction hypothesis S′⇒∗w0…wi−1Aδ and α′⇒∗wi…wj−2, then by virtue of a=wj−1 we obtain α=α′a⇒∗wi…wj−2wj−1=wi…wj−1. Thus, the conditions: S′⇒∗w0…wi−1Aδ and α⇒∗wi…wj−1 are satisfied.\n2. We include by the rule to predict. By construction: α=ε and i=j, which automatically implies the second point of the statement.\nIn this case, ∃i′≤i and the situation [A′→α′⋅Aδ′,i′]∈Di, from which, by the induction hypothesis, it follows that S′⇒∗w0…wi′−1A′δ′′ and α′⇒∗wi′…wi−1. We obtain that S′⇒∗w0…wi′−1A′δ′′, and therefore S⇒∗w0…wi′−1α′Aδ′δ′′, therefore S′⇒∗w0…wi′−1wi′…wi−1Aδ′δ′′, as a result S′⇒∗w0…wi−1Aδ, as required.\n3. We include according to the full rule. By construction: α=α′A′ and ∃i′,δ:[A→α′⋅A′β,i]∈Di′∧[A′→η⋅,i′]∈Dj. Therefore, α=α′A ⇒∗wi…wi′−1wi′…wj=wi…wj−1, which gives us the second point of the assertion, and since the first point follows from the induction hypothesis, everything is in order.\n\n⟸\n\nWe prove the converse by induction on the total length of the derivation w0…wi−1Aδ from S′ and wi…wj−1 from α. After that, we apply induction on the length of the derivation wi…wj−1 from α. Let us consider three cases of the last symbol α:\n1. α=α′a, then a=wj−1 and α′⇒∗wi…wj−2. By the induction hypothesis: [A→α′⋅aβ,i]∈Dj−1, and hence by the scanning rule we obtain [A→α′a⋅β,i]∈Dj.\n2. α=α′B, then ∃i′:α′⇒∗wi…wi′−1∧B′⇒∗wi′…wj−1. Then we have [A→α′a⋅β,i]∈Dj. You can also write S′⇒∗w0…wi−1Aδ as S′⇒∗w0…wi−1wi…wi′−1Bβδ, and also B→η∧η→wi′…wj−1.\nUsing induction on the second parameter, we get [B→η⋅,i′]∈Dj, whence, using the full rule, we get [A→α′B⋅β,i]∈Dj.\n\n3. α=ε, then i=j. Then either i=0∧A=S∧δ=ε, which proves the base of induction, or the conclusion can be written as S′⇒∗w0…wi′−1wi′…wi−1Aδ′δ′′=w0…wi−1Aδ for some rule (A′→wi′…wi−1Aδ′)∈P. Therefore, by the induction hypothesis, [A′→⋅wi′…wi−1Aδ′,i′]∈Di′ , which after several applications of the scanning rule leads to [A′→wi′…wi−1⋅Aδ′,i′]∈Di, after which, by the prediction rule, we obtain [A→⋅β,i]∈Dj, as required."
    },
    {
        "question": "Prove Early's algorithm",
        "answer": "____⟹\n\nWe will prove this by induction on the execution of the algorithm.\nInduction base:\n[S′→⋅S,0]еD0.\nInduction stage:\nSuppose that the assumption is true for all lists of situations with numbers less than j. Let's see what rule led to the situation [A→α⋅β,i] leading to Dj\n1. Activate according to the analysis rule. This happened if α=α′a, a=wj−1 and [A→α′⋅aβ,i]εDj−1.\nBy the induction hypothesis S′⇒∗w0…wi−1Aδ and α′⇒∗wi…wj−2, then thanks to a=wj−1 we get α=α′a⇒∗wi…wj−2wj− 1=wi …wj −1. Thus, the conditions: S′⇒∗w0…wi−1Aδ and α⇒∗wi…wj−1 are satisfied.\n2. Include the prediction rule. By construction: α=ε and i=j, which automatically leads to the second point of the statement.\nIn this case ∃i′妻i and the situation is [A′→α′⋅Aδ′,i′]εDi, from which by the induction hypothesis it follows that S′⇒∗w0…wi′ −1A′ δ′′ and α′ ⇒∗wi′…wi−1. We obtain that S′⇒∗w0…wi′−1A′δ′′, and, consequently, S⇒∗w0…wi′−1α′Aδ′δ′′, therefore, S′⇒∗w0…wi′−1wi′…wi −1Aδ′δ′′, and consequently, S′⇒∗w0…wi−1Aδ, as required.\n3. Light by the complete rule. By construction: α=α′A′ and ∃i′,δ:[A→α′⋅A′β,i]εDi′∧[A′→η⋅,i′]εDj. Thus, α=α′A ⇒∗wi…wi′−1wi′…wj=wi…wj−1, which gives us the second point of the assertion, and since the first point follows from the induction hypothesis, everything is in order. order.\n\n⟸\n\nWe will prove the converse by induction on the full length of the output w0…wi−1Aδ of the function S′ and wi…wj−1 of the function α. ​​After that, we apply induction on the length of the output wi…wj−1 of the function α. Let us consider three cases of the last symbol α:\n1. α=α′a, then a=wj−1 and α′⇒∗wi…wj−2. By the induction hypothesis: [A→α′⋅aβ,i]εDj−1, and from here, by the scanning rule, we obtain [A→α′a⋅β,i]εDj.\n2. α=α′B, then ∃i′:α′⇒∗wi…wi′−1∧B′⇒∗wi′…wj−1. Then we have [A→α′a⋅β,i]εDj. You can also write S′⇒∗w0…wi−1Aδ as S′⇒∗w0…wi−1wi…wi′−1Bβδ, and also B→η∧η→wi′…wj−1.\nUsing induction on the second parameter, we get [B→η⋅,i′]εDj, whence by the full rule we get [A→α′B⋅β,i]εDj.\n\n3. α=ε, then i=j. Then either i=0∧A=S∧δ=ε, which proves the basis of induction, or the conclusion can be written S′⇒∗w0…wi′−1wi′…wi−1Aδ′δ′′ =w0… wi −1Aδ for the rule (A→wi′…wi−1Aδ′)€P. Therefore, by the induction hypothesis, [A′→⋅wi′…wi−1Aδ′,i′]εDi′ , which after several applications of the scanning rule leads to [A′→wi′…wi −1⋅Aδ′ ,i′ ]εDi, after which, according to the prediction rule, we obtain [A→⋅β,i]εDj, as necessary."
    },
    {
        "question": "Demonstrate Early's algorithm",
        "answer": "____⟹\n\nWe will prove by induction on the execution of the algorithm.\nBasis of induction:\n[S′→⋅S,0]еD0.\nInduction stage:\nLet the hypothesis be true for all lists of situations with numbers less than j. We will determine, using the rule, the situation [A→α⋅β,i], included in Dj\n1. We include it according to the unfolding rule. This happened if α=α′a, a=wj−1 and [A→α′⋅aβ,i]εDj−1.\nBy the induction hypothesis S′⇒∗w0…wi−1Aδ and α′⇒∗wi…wj−2, then by virtue of a=wj−1 we obtain α=α′a⇒∗wi…wj−2wj− 1= wi…wj-1. Thus, the conditions: S′⇒∗w0…wi−1Aδ and α⇒∗wi…wj−1 are satisfied.\n2. We include by the prediction rule. By construction: α=ε and i=j, from which the second point of the statement automatically follows.\nMoreover, ∃i′妻i and the situation [A′→α′⋅Aδ′,i′]εDi, whence by the induction hypothesis it follows that S′⇒∗w0…wi′− 1A′δ′ ′ and α′⇒∗wi′…wi−1. We obtain that S′⇒∗w0…wi′−1A′δ′′, which means S⇒∗w0…wi′−1α′Aδ′δ′′, therefore S′⇒∗w0…wi′−1wi′… wi −1Aδ′δ′′, as a result S′⇒∗w0…wi−1Aδ, which is what we need.\n3. Turn on according to the full rule. By construction: α=α′A′ and ∃i′,δ:[A→α′⋅A′β,i]εDi′∧[A′→η⋅,i′]εDj. Therefore, α=α′A ⇒∗wi…wi′−1wi′…wj=wi…wj−1, which gives us the second point of the assertion, and since the first point follows from the induction hypothesis, everything is in order.\n\n⟸\n\nWe prove the converse by induction on the full output length w0…wi−1Aδ of the circuit S′ and wi…wj−1 of the circuit α. After that, we apply induction on the output length wi…wj−1 of the function α. Let us consider three cases of the last symbol α:\n1. α=α′a, then a=wj−1 and α′⇒∗wi…wj−2. By the recurrence hypothesis: [A→α′⋅aβ,i]εDj−1, and from there by the scanning rule we obtain [A→α′a⋅β,i]εDj.\n2. α=α′B, then ∃i′:α′⇒∗wi…wi′−1∧B′⇒∗wi′…wj−1. Then we have [A→α′a⋅β,i]εDj. We can also write S′⇒∗w0…wi−1Aδ as S′⇒∗w0…wi−1wi…wi′−1Bβδ, and also B→η∧η→wi′…wj−1.\nApplying induction on the second parameter, we get [B→η⋅,i′]εDj, whence by the full rule we get [A→α′B⋅β,i]εDj.\n\n3. α=ε, then i=j. Then either i=0∧A=S∧δ=ε, which proves the basis of induction, or the conclusion can be written as S′⇒∗w0…wi′−1wi′…wi−1Aδ′δ′ ′=w0… wi−1Aδ for the rule (A′→wi′…wi−1Aδ′)εP. Thus, by the induction hypothesis [A′→⋅wi′…wi−1Aδ′,i′]εDi′ , which after several applications of the scanning rule leads to [A′→wi′…wi− 1⋅Aδ′ ,i′]εDi, after which, by the prediction rule, we obtain [A→⋅β,i]εDj, as required."
    },
    {
        "question": "LL-fitting",
        "answer": "\n●Left recursion elimination\n●Left context extraction:\nIf A->Φγ_1, A->Φγ_2 are given, then equivalent rules A -> ΦA', A'-> γ_1|γ_2 can be constructed."
    },
    {
        "question": "LL-fitting",
        "answer": "\n●Left recursion elimination\n●Left context extraction:\nIf A->Φγ_1, A->Φγ_2 are given, then equivalent rules A -> ΦA', A'-> γ_1|γ_2 can be constructed."
    },
    {
        "question": "LL fitting",
        "answer": "\n●Elimination of left recursion.\n●Extraction of left context:\nIf A->Fu_1, A->Fu_2 are given, then equivalent rules A->FA', A'->γ_1|γ_2 can be constructed."
    },
    {
        "question": "LL fitting",
        "answer": "\n●Elimination of left recursion.\n●Extraction of left context:\nIf A->Fu_1, A->Fu_2 are given, then equivalent rules A->FA', A'->γ_1|γ_2 can be constructed."
    },
    {
        "question": "What is a recursive nonterminal?",
        "answer": "\nA recursive nonterminal is a nonterminal symbol in a grammar that can be output to a string containing itself. That is, in the process of unfolding the rules of the grammar, the nonterminal can lead to a chain of outputs where it appears again.\nFor example, if a nonterminal A has a rule A → αAβ or after some steps is output to such a rule, then A is recursive."
    },
    {
        "question": "What is a recursive nonterminal?",
        "answer": "\nA recursive nonterminal is a nonterminal symbol in a grammar that can be output to a string containing itself. That is, in the process of unfolding the rules of the grammar, the nonterminal can lead to a chain of outputs where it appears again.\nFor example, if a nonterminal A has a rule A → αAβ or after some steps is output to such a rule, then A is recursive."
    },
    {
        "question": "What is a recursive nonterminal?",
        "answer": "\nA recursive nonterminal is a nonterminal symbol in a grammar that can be written to a string containing itself. In other words, in the process of discovering the grammar rules, a nonterminal can lead to a chain of inferences in which it appears again.\nFor example, if a nonterminal A has a rule A → αAβ or after several steps is deduced to such a rule, then A is recursive."
    },
    {
        "question": "What is a recursive nonterminal?",
        "answer": "\nA recursive nonterminal is a nonterminal symbol in a grammar that can be output to a string containing itself. That is, during the expansion of the grammar rules, the nonterminal can lead to a chain of outputs where it appears again.\nFor example, if a nonterminal A has a rule A → αAβ or after several steps such a rule is output, then A is recursive."
    },
    {
        "question": "What is a left-recursive nonterminal?",
        "answer": "\nA left-recursive nonterminal is a nonterminal that has a rule where it is the first symbol on the right-hand side of its own rule, i.e. A → Aα, where α is some string of symbols.\nLeft-recursion can cause problems in top-down parsing, as it can cause infinite recursion in recursive parsers."
    },
    {
        "question": "What is a left-recursive nonterminal?",
        "answer": "\nA left-recursive nonterminal is a nonterminal that has a rule where it is the first symbol on the right-hand side of its rule, i.e. A → Aα, where α is some string of symbols.\nLeft-recursion can cause problems in top-down parsing, since it can cause infinite recursion in recursive parsers."
    },
    {
        "question": "What is a left-recursive nonterminal?",
        "answer": "\nA left-recursive nonterminal is a nonterminal for which there is a rule in which it is the first symbol on the right-hand side of the rule, i.e. A → Aα, where α is a string.\nLeft recursion can cause problems in top-down parsing, since it can cause infinite recursion in recursive parsers."
    },
    {
        "question": "What is a left-recursive nonterminal?",
        "answer": "\nA left-recursive nonterminal is a nonterminal for which there is a rule in which it is the first symbol on the right-hand side of its rule, i.e. A → Aα, where α is a string of symbols.\nLeft recursion can cause problems in top-down parsing, since it can cause infinite recursion in recursive parsers."
    },
    {
        "question": "What is a right recursive nonterminal?",
        "answer": "\nA right-recursive nonterminal is a nonterminal that has a rule where it appears as the last symbol on the right-hand side of its own rule, i.e. A → αA, where α is some string of symbols.\nRight-recursion is usually unproblematic in top-down parsing and may be preferable for some types of parsers, although it may impact efficiency at high recursion depths."
    },
    {
        "question": "What is a right recursive nonterminal?",
        "answer": "\nA right-recursive nonterminal is a nonterminal that has a rule in which it appears as the last symbol on the right-hand side of its rule, i.e. A → αA, where α is some string of symbols.\nRight recursion is generally unproblematic in top-down parsing and may be preferable for some types of parsers, although it may affect performance at high recursion depths."
    },
    {
        "question": "What is a right recursive nonterminal?",
        "answer": "\nA right-recursive nonterminal is a nonterminal for which there is a rule in which it appears as the last symbol on the right-hand side of the rule, i.e. A → αA, where α is a string.\nRight-recursion generally does not cause problems with top-down parsing and may be preferable for certain types of parsers, although it may affect performance at high recursion depths."
    },
    {
        "question": "What is a right recursive nonterminal?",
        "answer": "\nA right-recursive nonterminal is a nonterminal for which there is a rule that it appears as the last symbol on the right-hand side of its own rule, i.e. A → αA, where α is a string of symbols.\nProper recursion is generally unproblematic in top-down parsing and may be preferable for certain types of parsers, although it may affect efficiency at high recursion depths."
    },
    {
        "question": "Consider the following context-free grammar G:\n1. S → aSb\n2. S → bSa\n3. S → SS\n4. S → ε\nProve that the language L(G) generated by G is nonempty and describe this language.",
        "answer": "\nThe grammar G is non-empty because it contains a rule S → ε that allows us to output the empty string ε. This means that εin L(G), and therefore the language L(G) is non-empty.\n\nLet us describe the language L(G) generated by the grammar:\n\n1. All transition rules preserve the same number of a and b in a word.\n\n2. At the same time, any word with the same number of a and b is in the language. We prove this by induction on the length of the word. If the first and last symbols are different, we can discard them, obtaining a string with the same number of a and b of shorter length and applying the induction hypothesis. If they are the same, then, by virtue of discrete continuity, there is a gap between the symbols inside such that there are strings with the same number of a and b symbols on the right and left, for each of which we can apply the induction hypothesis.\n\nDescription of the language L(G):\n\nThe language L(G) consists of all strings containing the same number of a and b symbols.\n\nThat is, L(G) = {w ∈ {a, b}^* ||w|_a = |w|_b}."
    },
    {
        "question": "Consider the following context-free grammar G:\n1. S → aSb\n2. S → bSa\n3. S → SS\n4. S → ε\nProve that the language L(G) generated by G is nonempty, and describe this language.",
        "answer": "\nThe grammar G is non-empty because it contains a rule S → ε that allows us to output the empty string ε. This means that ε is in L(G), and therefore the language L(G) is non-empty.\n\nLet us describe the language L(G) generated by the grammar:\n\n1. All transition rules preserve the same number of symbols a and b in a word.\n\n2. At the same time, any word with the same number of symbols a and b exists in the language. We prove this by induction on the length of the word. If the first and last symbols are different, we can discard them, obtaining a string with the same number of symbols a and b of shorter length and apply the induction hypothesis. If they are the same, then by virtue of discrete continuity between the symbols inside there is a gap such that there are strings on the right and left with the same number of symbols a and b, for each of which we can apply the induction hypothesis.\n\nDescription of the language L(G):\n\nThe language L(G) consists of all strings containing the same number of symbols a and b.\n\nThat is, L(G) = {w ∈ {a, b}^* ||w|_a = |w|_b}."
    },
    {
        "question": "Consider the following context-free grammar G:\n1. C → acB\n2. C → bCa\n3. C → CC\n4. S → ε\nShow that the language L(G) generated by G is nonempty, and describe this language.",
        "answer": "\nThe grammar G is non-empty because it contains a rule S → ε that allows us to output the empty string ε. This means that ε belongs to L(G), and hence the language L(G) is not empty.\n\nLet us describe the language L(G) generated by the grammar:\n\n1. All transition rules preserve the same number of symbols a and b in a word.\n\n2. At the same time, the language contains any word with the same number of symbols a and b. We prove this by induction on the length of the word. If the first and last symbols are different, we can ignore them, obtaining a string with the same number of symbols a and b of shorter length and apply the induction hypothesis. If they are the same, then due to the discrete continuity between the symbols, a gap arises inside such that there are strings with the same number of symbols a and b on the right and left, for each of which we can apply the induction hypothesis.\n\nDescription of the language L(G):\n\nThe language L(G) consists of all strings containing the same number of symbols a and b.\n\nIn other words, L(G) = {w ∈ {a, b}^* ||w|_a = |w|_b}."
    },
    {
        "question": "Consider the following context-free grammar G:\n1. S → aSb\n2. C → bCa\n3. C → CC\n4. S → ε\nProve that the language L(G) generated by G is nonempty, and describe this language.",
        "answer": "\nThe grammar G is not empty because it contains a rule S → ε that allows us to output the empty string ε. This means that εin L(G), and therefore the language L(G) is not empty.\n\nLet us describe the language L(G) generated by the grammar:\n\n1. All transition rules preserve the same number of letters \"a\" and \"b\" in a word.\n\n2. At the same time, the language contains any word with the same number of a and b. Let us prove this by induction on the length of the word. If the first and last symbols are different, we can exclude them, get a string with the same number of letters a and b of shorter length, and apply the induction hypothesis. If they are identical, then by virtue of the discrete continuity between the symbols inside there is a gap such that on the right and left there are strings with the same number of symbols a and b, for each of which we can apply the induction hypothesis.\n\nDescription of the language L(G):\n\nThe language L(G) consists of all strings containing the same number of symbols a and b.\n\nThat is, L(G) = {w ∈ {a, b}^* ||w|_a = |w|_b}."
    },
    {
        "question": "Consider the following context-free grammar G:\n\n1. S → aSb\n\n2. S → bSa\n\n3. S → SS\n\n4. S → ε\nConstruct a non-deterministic pushdown automaton (NSPA) equivalent to G.",
        "answer": "\nTo build a NMPA that recognizes the language L(G), we can use the following approach:\n\nInput alphabet: {a, b}\n\nStack alphabet: {A, B, Z} (symbols for tracking the number of a and b, and the start symbol of the stack)\n\nState set: {q_0, q_f } (start and end states)\n\nTransition functions:\n\n1. Processing symbol a:\n\nWhen reading symbol a, the NMPA pushes symbol A onto the stack unless there is a symbol B on the stack that can be removed.\n\nΔ(q_0, a, Z) = {(q_0, AZ)}\n\nΔ(q_0, a, A) = {(q_0, AA)}\n\nOtherwise, B is removed.\n\nΔ(q_0, a, B) = {(q_0, ε)} \n\n2. Processing symbol b:\n\nWhen reading symbol b, NMPA removes symbol A from the top of the stack, if it is there.\n\nΔ(q_0, b, A) = { (q_0, ε) } \n\nOtherwise, add B to the stack: \n\nΔ(q_0, b, Z) = {(q_0, BZ)} \n\nΔ(q_0, b, B) = {(q_0, BB)} \n\n3. Handling an empty symbol (to accept an empty string): \n\nΔ(q_0, ε, Z) = { (q_f, Z) } \n\nDescription of NMPA operation: \n\n- NMPA starts in state q_0 with a stack containing the initial symbol Z. \n\n- When reading each symbol a, the automaton pushes A onto the stack, incrementing the counter of the number of a, or decrementing the counter of the number of b, if possible. \n\n- When reading each symbol b, the automaton removes A from the top of the stack, decreasing the counter if possible, or increasing the counter of the number of b \n\n- If after processing the entire input string, the stack returns to the initial symbol Z (i.e. the stack is empty with respect to the symbols of A ), the automaton accepts the string.\n\n- NMPA accepts the string if the number of symbols of a is equal to the number of symbols of b.\n\nCorrespondence to grammar G:\n\n- The rule S → aSb corresponds to increasing the stack by A when reading a and then decreasing it when reading b.\n\n- The rule S → bSa corresponds to increasing the stack by B when reading b and then decreasing it when reading a.\n\n- The rule S → SS corresponds to the non-determinism of NMPA, which allows it to split the string into substrings processed independently.\n\n- The rule S → ε corresponds to the ability of NMPA to accept an empty string without changing the stack."
    },
    {
        "question": "Consider the following context-free grammar G:\n\n1. S → aSb\n\n2. S → bSa\n\n3. S → SS\n\n4. S → ε\nConstruct a non-deterministic pushdown automaton (NSPA) equivalent to G.",
        "answer": "\nTo build an NMPA that recognizes a language L(G), we can use the following approach:\n\nInput alphabet: {a, b}\n\nStack alphabet: {A, B, Z} (symbols to keep track of the number of a's and b's, and the start symbol of the stack)\n\nState set: {q_0, q_f } (start and end states)\n\nTransition functions:\n\n1. Processing symbol a:\n\nWhen reading symbol a, NMPA pushes symbol A onto the stack unless there is a symbol B on the stack that can be popped.\n\nΔ(q_0, a, Z) = {(q_0, AZ)}\n\nΔ(q_0, a, A) = {(q_0, AA)}\n\nOtherwise, B is popped.\n\nΔ(q_0, a, B) = {(q_0, ε)} \n\n2. Processing symbol b:\n\nWhen reading symbol b, NMPA removes symbol A from the top of the stack, if it is there.\n\nΔ(q_0, b, A) = { (q_0, ε) } \n\nOtherwise, add B to the stack: \n\nΔ(q_0, b, Z) = {(q_0, BZ)} \n\nΔ(q_0, b, B) = {(q_0, BB)} \n\n3. Handling an empty character (to accept an empty string): \n\nΔ(q_0, ε, Z) = { (q_f, Z) } \n\nDescription of NMPA operation: \n\n- NMPA starts in state q_0 with a stack containing the initial character Z. \n\n- As each character a is read, the automaton pushes A onto the stack, incrementing the counter of a or decrementing the counter of b, if possible. \n\n- When reading each character b, the automaton removes A from the top of the stack, decreasing the counter if possible, or increasing the counter of b \n\n- If, after processing the entire input string, the stack is back to the initial character Z (i.e. the stack is empty relative to the characters of A ), the automaton accepts the string.\n\n- NMPA accepts the string if the number of characters of a is equal to the number of characters of b.\n\nCorrespondence to grammar G:\n\n- The rule S → aSb corresponds to increasing the stack by A when reading a and then decreasing it when reading b.\n\n- The rule S → bSa corresponds to increasing the stack by B when reading b and then decreasing it when reading a.\n\n- The rule S → SS corresponds to the nondeterminism of NMPA, which allows it to split the string into substrings that are processed independently.\n\n- The rule S → ε corresponds to the ability of NMPA to accept an empty string without changing the stack."
    },
    {
        "question": "Consider the following context-free grammar G:\n\n1. C → acB\n\n2. C → bCa\n\n3. C → CC\n\n4. S → ε\nConstruct a non-deterministic storage automaton (NSPA) equivalent to G.",
        "answer": "\nTo build an NMPA that recognizes a language L(G), we can use the following approach:\n\nInput alphabet: {a, b}\n\nStack alphabet: {A, B, Z} (symbols to keep track of the number of letters a and b, and the starting symbol of the stack)\n\nState set: {q_0, q_f } (start and end states)\n\nTransition functions:\n\n1. Treat character A:\n\nWhen reading character a, NMPA pushes character A onto the stack unless there is a character B on the stack that can be removed.\n\nΔ(q_0, a, Z) = {(q_0, AZ)}\n\nΔ(q_0, a, A) = {(q_0, AA)}\n\nOtherwise, B is removed.\n\nΔ(q_0, a, B) = {(q_0, ε)} \n\n2. Processing symbol b:\n\nWhen reading symbol b, NMPA removes symbol A from the top of the stack, if it is there.\n\nΔ(q_0, b, A) = { (q_0, ε) } \n\nOtherwise, we push B onto the stack: \n\nΔ(q_0, b, Z) = {(q_0, BZ)} \n\nΔ(q_0, b, B) = {(q_0, BB)} \n\n3. Handling empty characters (accepting an empty string): \n\nΔ(q_0, ε, Z) = { (q_f, Z) } \n\nNMPA job description: \n\n- NMPA starts in state q_0 with a stack containing the initial character Z. \n\n- Each time a character a is read, the machine pushes A onto the stack, incrementing the counter by a or decrementing the counter by b if possible. \n\n- Each time a character b is read, the machine removes A from the top of the stack, decrementing the counter if possible, or incrementing the counter by b. \n\n- If, after processing the entire input string, the stack wraps around to the initial character Z (i.e. the stack is empty relative to the characters of A), the machine accepts the string. \n\n- NMPA accepts the string if the number of characters of a is equal to the number of characters of b. \n\nCorrespondence to grammar G: \n\n- Rule S → aSb corresponds to growing A's stack when reading a, and then decreasing it when reading b. \n\n- Rule S → bSa corresponds to growing B's stack when reading b, and then decreasing it when reading a. \n\n- Rule S → SS corresponds to NMPA's non-determinism, allowing the string to be split into independently processed substrings. \n\n- Rule S → ε corresponds to NMPA's ability to accept an empty string without changing the stack."
    },
    {
        "question": "Consider the following context-free grammar G:\n\n1. S → aSb\n\n2. C → bCa\n\n3. C → CC\n\n4. S → ε\nConstruct a non-deterministic stack automaton (NSPA) equivalent to G.",
        "answer": "\nTo build an NMPA that recognizes a language L(G), we can use the following approach:\n\nInput alphabet: {a, b}\n\nStack alphabet: {A, B, Z} (symbols to keep track of the number of a's and b's, and the start symbol of the stack)\n\nState set: {q_0, q_f } (start and end states)\n\nTransition functions:\n\n1. Processing symbol a:\n\nWhen reading symbol a, NMPA pushes symbol A onto the stack unless there is a symbol B on the stack that can be popped.\n\nΔ(q_0, a, Z) = {(q_0, AZ)}\n\nΔ(q_0, a, A) = {(q_0, AA)}\n\nOtherwise, B is popped.\n\nΔ(q_0, a, B) = {(q_0, ε)} \n\n2. Processing symbol b:\n\nWhen reading symbol b, NMPA removes symbol A from the top of the stack, if it is there.\n\nΔ(q_0, b, A) = { (q_0, ε) } \n\nOtherwise, push B onto the stack: \n\nΔ(q_0, b, Z) = {(q_0, BZ)} \n\nΔ(q_0, b, B) = {(q_0, BB)} \n\n3. Handling an empty character (accepting an empty string): \n\nΔ(q_0, ε, Z) = { (q_f, Z) } \n\nDescription of how NMPA works: \n\n- NMPA starts in state q_0 with a stack containing the initial character Z. \n\n- As each character a is read, the automaton pushes A onto the stack, incrementing the counter by the amount of a or decrementing the counter by the amount of b, if possible. \n\n- When reading each character b, the automaton removes A from the top of the stack, decreasing the counter if possible, or increasing the counter by b. \n\n- If, after processing the entire input string, the stack returns to the original character Z (i.e. the stack is empty compared to the characters of A), the automaton accepts the string. \n\n- NMPA accepts the string if the number of characters in a is equal to the number of characters in b. \n\nCorrespondence to grammar G: \n\n- Rule S → aSb corresponds to increasing A's stack when reading a and decreasing it when reading b. \n\n- Rule S → bSa corresponds to increasing B's stack when reading b, and then decreasing it when reading a. \n\n- Rule S → SS corresponds to NMPA's non-determinism, which allows it to split a string into independently processed substrings. \n\n- Rule S → ε corresponds to NMPA's ability to accept an empty string without changing the stack."
    },
    {
        "question": "Describe an algorithm for eliminating left recursion",
        "answer": "\n\nSteps of the left recursion elimination algorithm:\n\n1. Identification of left recursion:\n- Immediate left recursion: When a rule has the form A → Aα | β, where A is a non-terminal, α is a sequence of symbols (non-empty), and β is an alternative that does not start with A.\n- Indirect left recursion: When a sequence of rules leads to a situation where A → Bα, and B can output A as the first symbol (i.e. B ⇒+ A).\n\n2. Elimination of immediate left recursion for each non-terminal:\nFor each non-terminal A that has rules of the form:\n\nA → Aα₁ | Aα₂ |... | Aαₙ | β₁ | β₂ |... | βₘ\n\nwhere β₁, β₂,..., βₘ are alternatives that do not start with A.\n\n- Create a new non-terminal A'.\n- Rewrite the rules as follows:\n\nA → β₁A' | β₂A' |... | βₘA'\nA' → α₁A' | α₂A' |... | αₙA' | ε\n\nHere ε denotes the empty string.\n\n3. Elimination of indirect left recursion:\n- Arrange the non-terminals in some order A₁, A₂,..., An.\n- For each non-terminal Ai from i = 1 to n:\n- For each rule of the form Ai → Ajγ, where j < i:\n- Replace Aj with its right-hand sides.\n- After the replacement, we eliminate the immediate left recursion from the rules for Ai using the method from step 2."
    },
    {
        "question": "Describe the algorithm for eliminating left recursion",
        "answer": "\n\nSteps of the left recursion elimination algorithm:\n\n1. Left recursion detection:\n- Immediate left recursion: when a rule has the form A → Aα | β, where A is a non-terminal, α is a sequence of symbols (non-empty), and β is an alternative that does not start with A.\n- Indirect left recursion: when a sequence of rules leads to a situation where A → Bα, and B can output A as the first symbol (i.e. B ⇒+ A).\n\n2. Immediate left recursion elimination for each non-terminal:\nFor each non-terminal A that has rules of the form:\n\nA → Aα₁ | Aα₂ |... | Aαₙ | β₁ | β₂ |... | βₘ\n\nwhere β₁, β₂,..., βₘ are alternatives that do not start with A.\n\n- Create a new non-terminal A'.\n- Rewrite the rules as follows:\n\nA → β₁A' | β₂A' |... | βₘA'\nA' → α₁A' | α₂A' |... | αₙA' | ε\n\nHere ε denotes the empty string.\n\n3. Eliminating indirect left recursion:\n- Arrange the non-terminals in some order A₁, A₂,..., An.\n- For each non-terminal Ai from i = 1 to n:\n- For each rule of the form Ai → Ajγ, where j < i:\n- Replace Aj with its right-hand sides.\n- After the replacement, we eliminate immediate left recursion from the rules for Ai using the method from step 2."
    },
    {
        "question": "Describe the algorithm for eliminating left recursion.",
        "answer": "\n\nSteps of the algorithm for eliminating left recursion:\n\n1. Define left recursion:\n- Direct left recursion: when a rule has the form A → Aα | β, where A is a non-terminal, α is a (non-empty) sequence of symbols, and β is an alternative that does not start with A.\n- Indirect left recursion: when a sequence of rules leads to a situation where A → Bα and B can display A as the first symbol (i.e. B ⇒+ A).\n\n2. Eliminate immediate left recursion for each non-terminal:\nFor each non-terminal A that has rules of the form:\n\nA → Aα₁ | Aα₂ |... | Aαₙ | β₁ | β₂ |... | βₘ\n\nwhere β₁, β₂,..., βₘ are alternatives that do not start with the letter A.\n\n- Create a new non-terminal A'.\n- Rewrite the rules as follows:\n\nA → β₁A' | β₂A' |... | βₘA'\nA' → α₁A' | α₂A' |... | αₙA' | ε\n\nHere ε denotes the empty string.\n\n3. Eliminating left indirect recursion:\n- Arrange the non-terminals in the specified order A₁, A₂,..., An.\n- For each nonterminal Ai from i = 1 to n:\n- For each rule of the form Ai → Ajγ, where j < i:\n- Replace Aj with its right-hand sides.\n- After the replacement, eliminate immediate left recursion from the rules for Ai using the method from step 2."
    },
    {
        "question": "Describe the algorithm for eliminating left recursion.",
        "answer": "\n\nSteps of the left recursion elimination algorithm:\n\n1. Left recursion detection:\n- Immediate left recursion: when a rule has the form A → Aα | β, where A is a non-terminal, α is a sequence of (non-empty) symbols, and β is an alternative not starting with A.\n- Indirect left recursion: when a sequence of rules leads to a situation where A → Bα, and B can produce A as the first symbol (i.e. B ⇒+ A).\n\n2. Eliminating immediate left recursion for each non-terminal:\nFor each non-terminal A that has rules of the form:\n\nA → Aα₁ | Aα₂ |... | Aαₙ | β₁ | β₂ |... | βₘ\n\nwhere β₁, β₂,..., βₘ are alternatives that do not start with the letter A.\n\n- Create a new non-terminal A'.\n- Rewrite the rules as follows:\n\nA → β₁A' | β₂A' |... | βₘA'\nA' → α₁A' | α₂A' |... | αₙA' | ε\n\nHere ε denotes the empty string.\n\n3. Eliminating indirect left recursion:\n- Classify non-terminals in the given order A₁, A₂,..., An.\n- For each non-terminal Ai from i = 1 to n:\n- For each rule of the form Ai → Ajγ, where j < i:\n- Replace Aj with its right terms.\n— After the replacement, we eliminate the immediate left recursion of the rules for Ai using the method from step 2."
    },
    {
        "question": "Differences between context-free grammar and automaton grammar",
        "answer": "\nKey differences between context-free (CF) grammars and automata grammars\n\nContext-free grammars (CF grammars):\n\nContext-free grammars are a more powerful tool for describing languages ​​than regular (automata) grammars. They are used to define context-free languages ​​and occupy the second level in the Chomsky hierarchy (type 2).\n- The rules are of the form: A → α, where A is a non-terminal symbol and α is an arbitrary string of terminal and non-terminal symbols.\n\nAutomata grammars (Regular grammars):\n\nAutomata grammars, also known as regular grammars, are the simplest class of grammars and correspond to regular languages. They are at the lowest level of the Chomsky hierarchy (type 3).\n- The rules are of the form:\n- Right-handed grammars: A → aB or A → a\n- Left-handed grammars: A → Ba or A → a\n- Where A and B are nonterminals, a is a terminal symbol.\n\nMain differences:\n\n1. Expressive power:\n- CFGs are capable of describing more complex languages ​​with recursion and nesting.\n- Regular grammars are limited to describing simple sequences without recursive structures.\n\n2. Types of production rules:\n- In CFGs, the left side of a rule is always a single nonterminal, and the right side can be any string of terminals and nonterminals.\n- In regular grammars, the right side of a rule is limited to a single terminal, possibly followed by a nonterminal (for right-handed grammars), or a nonterminal preceding a terminal (for left-handed grammars).\n\n3. Corresponding automata:\n- CF languages ​​are recognized by pushdown automata that have a stack to account for nested structures.\n- Regular languages ​​are recognized by finite automata without additional memory.\n\n4. Closure with respect to operations:\n- Regular languages ​​are closed with respect to union, concatenation, Kleene closure, intersection, and complementation.\n- CF languages ​​are closed with respect to union, concatenation, and Kleene closure, but not with respect to intersection and complementation."
    },
    {
        "question": "Differences between context-free grammar and automaton grammar",
        "answer": "\nKey differences between context-free (CF) grammars and automata grammars\n\nContext-free grammars (CF grammars):\n\nContext-free grammars are a more powerful tool for describing languages ​​than regular (automata) grammars. They are used to define context-free languages ​​and occupy the second level in the Chomsky hierarchy (type 2).\n- The rules are of the form: A → α, where A is a non-terminal symbol and α is an arbitrary string of terminal and non-terminal symbols.\n\nAutomata grammars (regular grammars):\n\nAutomata grammars, also known as regular grammars, are the simplest class of grammars and correspond to regular languages. They are at the lowest level of the Chomsky hierarchy (type 3).\n- The rules are of the form:\n- Right-handed grammars: A → aB or A → a\n- Left-handed grammars: A → Ba or A → a\n- Where A and B are nonterminals, a is a terminal symbol.\n\nMain differences:\n\n1. Expressive power:\n- CFGs are capable of describing more complex languages ​​with recursion and nesting.\n- Regular grammars are limited to describing simple sequences without recursive structures.\n\n2. Types of production rules:\n- In CFGs, the left-hand side of a rule is always a single nonterminal, while the right-hand side can be any string of terminals and nonterminals.\n- In regular grammars, the right-hand side of a rule is limited to a single terminal, which may be followed by a nonterminal (for right-handed grammars) or a nonterminal preceding the terminal (for left-handed grammars).\n\n3. Corresponding automata:\n- CF languages ​​are recognized by pushdown automata that have a stack to account for nested structures.\n- Regular languages ​​are recognized by finite automata without additional memory.\n\n4. Closure under operations:\n- Regular languages ​​are closed under union, concatenation, Kleene closure, intersection, and complementation.\n- CF languages ​​are closed under union, concatenation, and Kleene closure, but not under intersection and complementation."
    },
    {
        "question": "Differences between context-free grammar and automaton grammar",
        "answer": "\nMain differences between context-free (CF) grammars and automata grammars\n\nContext-free grammars (CS grammars):\n\nContext-free grammars are a more powerful tool for describing languages ​​than classical (automatic) grammars. They are used to define languages ​​without context and occupy the second level in the Chomsky hierarchy (type 2).\n- The rules are of the form: A → α, where A is a non-terminal symbol, and α is an arbitrary string of terminal and non-terminal symbols.\n\nAutomatic grammars (regular grammars):\n\nAutomata grammars, also called regular grammars, are the simplest class of grammars and correspond to regular languages. They are at the lowest level of the Chomsky hierarchy (type 3).\n- The rules are:\n- Right-handed grammars: A → aB or A → a.\n- Left-handed grammars: A → Ba or A → a\n- If A and B are not terminals, a is a terminal symbol.\n\nMain differences:\n\n1. Expressive power:\n— CFGs are capable of describing more complex languages ​​with recursion and nesting.\n- Regular grammars are limited to describing simple sequences without recursive structures.\n\n2. Types of product rules:\n— In CFGs, the left-hand side of a rule is always a single nonterminal, and the right-hand side can be any chain of terminals and nonterminals.\n- In regular grammars, the right-hand side of a rule is limited to a single terminal, which may be followed by a nonterminal (for right-handed grammars) or a nonterminal preceding a terminal (for left-handed grammars). ). ).\n\n3. Conforming machines:\n— CF languages ​​are recognized by storage machines that have a stack to accommodate nested structures.\n— Ordinary languages ​​are recognized by finite automata without additional memory.\n\n4. Closure of operations:\n— Ordinary languages ​​are closed under union, concatenation, Kleene closure, intersection, and complementation.\n— CF languages ​​are closed under union, concatenation, and Kleene closure, but not under intersection and complementation."
    },
    {
        "question": "Differences between context-free grammar and automaton grammar",
        "answer": "\nKey differences between context-free (CF) grammars and automata grammars\n\nContext-free grammars (CF grammars):\n\nContext-free grammars are a more powerful tool for describing languages ​​than regular grammars (automata). They are used to define context-free languages ​​and occupy the second level of the Chomsky hierarchy (type 2).\n- The rules are of the form: A → α, where A is a non-terminal symbol and α is an arbitrary string of terminal and non-terminal symbols.\n\nAutomata grammars (regular grammars):\n\nAutomata grammars, also called regular grammars, are the simplest class of grammars and correspond to regular languages. They are located at the lowest level of the Chomsky hierarchy (type 3).\n- The rules are of the form:\n- Right-handed grammars: A → aB or A → a.\n- Left-handed grammars: A → Ba or A → a.\n- If A and B are not terminals, a is a terminal symbol.\n\nMain differences:\n\n1. Expressive power:\n- CFGs are capable of describing more complex languages ​​with recursion and nesting.\n- Regular grammars are limited to describing simple sequences without recursive structures.\n\n2. Kinds of production rules:\n- In CFGs, the left-hand side of a rule is always a single nonterminal, and the right-hand side can be any chain of terminals and nonterminals.\n- In regular grammars, the right-hand side of a rule is limited to a single terminal, which may be followed by a nonterminal (for right-handed grammars) or a nonterminal preceding the terminal (for left-handed grammars).\n\n3. Corresponding automata:\n— CF languages ​​are recognized by stack automata, which have a stack to account for nested structures.\n— Regular languages ​​are recognized by finite automata without additional memory.\n\n4. Closure with respect to operations:\n— Regular languages ​​are closed with respect to union, concatenation, Kleene closure, intersection, and complementation.\n— CF languages ​​are closed with respect to union, concatenation, and Kleene closure, but not with respect to intersection and complementation."
    },
    {
        "question": "What is the law of idempotency?",
        "answer": "\nAn operation is called idempotent if the result of applying the operation to two identical values ​​as operands is equal to that value. The usual arithmetic operations are not idempotent. In general, x + x ≠ x and x × x ≠ x (although there are some values ​​of x for which this equality holds, such as 0 + 0 = 0). However, union and intersection are idempotent operations, so the following law holds for regular expressions.\nL + L = L. This law, the law of idempotency of the union operation, states that the union of two identical expressions can be replaced by a single such expression."
    },
    {
        "question": "What is the law of idempotency?",
        "answer": "\nAn operation is called idempotent if the result of applying the operation to two identical values ​​as operands is equal to that value. The usual arithmetic operations are not idempotent. In general, x + x ≠ x and x × x ≠ x (although there are some values ​​of x for which this equality holds, such as 0 + 0 = 0). However, union and intersection are idempotent operations, so the following law holds for regular expressions.\nL + L = L. This law, the law of idempotency of the union operation, states that the union of two identical expressions can be replaced by a single such expression."
    },
    {
        "question": "What is the law of idempotency?",
        "answer": "\nAn operation is called idempotent if the result of applying the operation to two identical values ​​as operands is equal to that value. The usual arithmetic operations are not idempotent. In general, x + x ≠ x and x × x ≠ x (although there are some values ​​of x for which this equality is true, such as 0 + 0 = 0). However, union and intersection are idempotent operations, so the following law applies to regular expressions.\nL + L = L. This law, the law of idempotency of the union operation, states that the union of two identical expressions can be replaced by only one of those expressions."
    },
    {
        "question": "What is the law of idempotency?",
        "answer": "\nAn operation is said to be idempotent if the result of applying the operation to two identical values ​​as operands is equal to that value. The usual arithmetic operations are not idempotent. In general, x + x ≠ x and x × x ≠ x (although there are some values ​​of x for which this equality is true, such as 0 + 0 = 0). However, union and intersection are idempotent operations, so the following law applies to regular expressions.\nL + L = L. This law, the law of idempotency of the union operation, states that the union of two identical expressions can be replaced by a single expression of the same type."
    },
    {
        "question": "What is the normal form of a DMP machine?",
        "answer": "\nA normal form DMF is a deterministic pushdown automaton M, represented by a finite set of states Q, an input alphabet on a tape Σ, a stack alphabet Γ, and a set of transitions Δ, which satisfies the following conditions:\n1. If (p,a,S) → (q,α) ∈ Δ, then |α|⩽2, where α ∈ Г* is a sequence of stack symbols, S ∈ Г.\n2. If (p,ε,S) → (q,α) ∈ Δ, then α = ε.\n3. Δ does not contain useless transitions (the transition (p,a,S) → (q,α) is considered useless if L(q,α)= ∅, that is, nothing can be deduced from the configuration (q,α)."
    },
    {
        "question": "What is the typical shape of a DMP machine?",
        "answer": "\nA normal form DMF is a deterministic pushdown automaton M represented by a finite set of states Q, an input alphabet on a tape Σ, a stack alphabet Γ, and a set of transitions Δ that satisfies the following conditions:\n1. If (p,a,S) → (q,α) ∈ Δ, then |α|⩽2, where α ∈ Г* is a sequence of stack symbols, S ∈ Г.\n2. If (p,ε,S) → (q,α) ∈ Δ, then α = ε.\n3. Δ does not contain useless transitions (a transition (p,a,S) → (q,α) is considered useless if L(q,α) = ∅, i.e. nothing can be deduced from the configuration (q,α)."
    },
    {
        "question": "What is the normal form of a DMP machine?",
        "answer": "\nA normal form DMF is a deterministic memory automaton M represented by a set of finite states Q, an input strip alphabet Σ, a stack alphabet Γ, and a set of transitions Δ that satisfies the following conditions:\n1. If (p,a,S) → (q,α) ∈ ∆, then |α|⩽2, where α ∈ Г* is a sequence of stack symbols, S ∈ Г.\n2. If (p,ε,S) → (q,α) ∈ ∆, then α = ε.\n3. ∆ does not contain unnecessary transitions (the transition (p,a,S) → (q,α) is considered unnecessary if L(q,α) = ∅, i.e. nothing can be deduced from the configuration (q, α)."
    },
    {
        "question": "What is the normal form of a DMP machine?",
        "answer": "\nA normal form DMF is a deterministic stack automaton M, represented by a finite set of states Q, an input alphabet on a tape Σ, a stack alphabet Γ, and a set of transitions Δ, which satisfies the following conditions:\n1. If (p,a,S) → (q,α) ∈ ∆, then |α|⩽2, where α ∈ Г* is a sequence of stack symbols, S ∈ Г.\n2. If (p,ε,S) → (q,α) ∈ ∆, then α = ε.\n3. ∆ does not contain unnecessary transitions (the transition (p,a,S) → (q,α) is considered unnecessary if L(q,α)= ∅, i.e. nothing can be deduced from the configuration (q,α)."
    },
    {
        "question": "What is a non-deterministic pushdown automaton?",
        "answer": "\nA pushdown automaton is a set A = ⟨Σ,Γ,Q,s ∈ Q,T ⊂ Q,z0 ∈ Γ,δ: Q×Σ∪{ε}×Γ → 2^(Q×Γ*)⟩, where\n1. Σ is the input alphabet on the tape,\n2. Γ is the stack alphabet,\n3. Q is the set of states of the automaton,\n4. s is the starting state of the automaton,\n5. T is the set of accepting states of the automaton,\n6. z0 is the marker of the bottom of the stack,\n7. δ is the transition function.\nSymbols of the input alphabet are sequentially read from the tape (ci is the currently read symbol). Symbol x is removed from the top of the stack. Instead, a string α is placed in such a way that the first character of the string is on the top of the stack.\nUsually, an automaton with a stack is understood to be a non-deterministic automaton."
    },
    {
        "question": "What is a nondeterministic pushdown automaton?",
        "answer": "\nA stack automaton is a set A = ⟨Σ,Γ,Q,s ∈ Q,T ⊂ Q,z0 ∈ Γ,δ: Q×Σ∪{ε}×Γ → 2^(Q×Γ*)⟩, where\n1. Σ is the input alphabet on the tape,\n2. Γ is the stack alphabet,\n3. Q is the set of states of the automaton,\n4. s is the initial state of the automaton,\n5. T is the set of accepting states of the automaton,\n6. z0 is the marker of the bottom of the stack,\n7. δ is the transition function.\nThe symbols of the input alphabet are sequentially read from the tape (ci is the currently read symbol). The symbol x is removed from the top of the stack. Instead, the string α is placed so that the first symbol of the string is on the top of the stack.\nUsually, a stack automaton is understood as a nondeterministic automaton."
    },
    {
        "question": "What is a non-deterministic vending machine?",
        "answer": "\nA machine with a magazine is a set A = ⟨Σ,Γ,Q,s ∈ Q,T ⊂ Q,z0 ∈ Γ,δ : Q×Σ∪{ε}×Γ → 2^(Q×Γ*)⟩ , Or\n1. Σ – input alphabet on tape,\n2. Γ – stack alphabet,\n3. Q – set of machine states,\n4. s – initial state of machine,\n5. T – set of accept states of automaton,\n6. z0 – bottom marker of stack,\n7. δ – transition function.\nSymbols of input alphabet are read from tape sequentially (this is the symbol being read at the moment). Symbol x is removed from the top of stack. Instead, string α is allocated such that the first symbol of string is on the top of stack.\nUsually, a machine with a stack means a non-deterministic machine."
    },
    {
        "question": "What is a non-deterministic battery-powered machine?",
        "answer": "\nA pushbutton automaton is a set A = ⟨Σ,Γ,Q,s ∈ Q,T ⊂ Q,z0 ∈ Γ,δ : Q×Σ∪{ε}×Γ → 2^(Q×Γ*)⟩ , or\n1. Σ is the input alphabet on the tape,\n2. Γ is the stack alphabet,\n3. Q is the set of states of the automaton,\n4. s is the initial state of the automaton,\n5. T is the set of acceptance states of the automaton,\n6. z0 is the bottom marker of the stack,\n7. δ is the transition function.\nThe symbols of the input alphabet are read sequentially from the tape (this is the symbol being read at the moment). Symbol x is removed from the top of the stack. Instead, a string α is placed so that the first symbol of the string is on the top of the stack.\nIn general, an automaton with a stack is considered a non-deterministic automaton."
    },
    {
        "question": "DMP-automata and ambiguity",
        "answer": "\nTheorem 1: \"If L=N(S) for some DMP-automaton S, then L has an unambiguous CF-grammar.\"\n\nProof:\nWe claim that the construction of the theorem generates an unambiguous CF-grammar Γ when the MP-automaton to which it is applied is deterministic. First, recall the theorem that says that for the grammar Γ to be unambiguous, it suffices to show that it has unique left derivations.\nSuppose S admits w on an empty magazine. Then it does so with a single sequence of transitions, since it is deterministic and cannot operate after the magazine is empty. Knowing this sequence of transitions, we can uniquely determine the choice of each production in the left derivation of w in Γ.\nThe rule of the automaton S on the basis of which the production is applied is always unique. But a rule, say δ(q,a,X)={(r,Y1Y2…Yk)}, can generate many productions of Γ, with different states in positions reflecting the states of S after deleting each of Y1, Y2, …, Yk. \nHowever, since S is deterministic, only one of these sequences of transitions is realized, so only one of these productions actually leads to the generation of w. \n\nTheorem 2: \"If L=L(S) for some DMP automaton S, then L has an unambiguous CFG.\" \n\nProof: \nLet $ be a \"trailing marker\" missing from strings of L, and let L'=L$. Thus, strings of L' are strings of L with the symbol $ appended to them. Then L' has the prefix property, and L'=N(S') for some DMP automaton S'. By the theorem, there exists an unambiguous grammar Γ' that generates the language N(S'), i.e. L'.\nNow, given the grammar Γ', we construct Γ for which L(Γ)=L. To do this, we only need to get rid of the marker $ in the strings. We will regard $ as a variable of the grammar Γ and introduce the production $→ϵ; the remaining productions of Γ and Γ' are the same. Since L(Γ')=L', we obtain that L(Γ)=L.\nWe claim that Γ is unambiguous. Indeed, the left generations in Γ coincide with the left generations in Γ', with the exception of the last step in Γ — changing $ to ϵ. Thus, if a terminal string w had two left generations in Γ, then w$ would have two generations in Γ'.\n\nSince Γ' is unambiguous, Γ is also unambiguous."
    },
    {
        "question": "DMP-automata and ambiguity",
        "answer": "\nTheorem 1: \"If L=N(S) for some DMP automaton S, then L has an unambiguous CF grammar.\"\n\nProof:\nWe claim that the construction of the theorem generates an unambiguous CF grammar Γ when the MP automaton to which it is applied is deterministic. First, recall the theorem that says that for a grammar Γ to be unambiguous, it suffices to show that it has unique left outputs.\nSuppose that S accepts w on an empty store. Then it does so with a single sequence of transitions, since it is deterministic and cannot operate after the store is empty. Knowing this sequence of transitions, we can uniquely determine the choice of each production in the left output of w in Γ.\nThe rule of the automaton S on the basis of which the production is applied is always unique. But a rule, say δ(q,a,X)={(r,Y1Y2…Yk)}, can generate many productions Γ with different states at the positions reflecting the states of S after deleting each of Y1, Y2, …, Yk. \nHowever, since S is deterministic, only one of these transition sequences is realized, so only one of these productions actually results in the generation of w. \n\nTheorem 2: \"If L=L(S) for some DMP S, then L has a unique CFG.\" \n\nProof: \nLet $ be a \"terminating marker\" missing from the strings of L, and let L'=L$. Thus, the strings of L' are the strings of L with the symbol $ appended to them. Then L' has the prefix property, and L'=N(S') for some DMP S'. By the theorem, there is an unambiguous grammar Γ' that generates the language N(S'), i.e. L'.\nNow, given a grammar Γ', we construct Γ for which L(Γ)=L. To do this, we only need to get rid of the marker $ in the strings. We will consider $ as a variable of Γ and introduce a production $→ϵ; the remaining productions of Γ and Γ' are the same. Since L(Γ')=L', we obtain L(Γ)=L.\nWe claim that Γ is unambiguous. Indeed, the left generations of Γ coincide with the left generations of Γ', except for the last step in Γ — replacing $ with ϵ. Thus, if a terminal string w had two left generations in Γ, then w$ would have two generations in Γ'.\n\nSince Γ' is unambiguous, Γ is also unambiguous."
    },
    {
        "question": "DMP Machines and Ambiguity",
        "answer": "\nTheorem 1: \"If L=N(S) for a DMP automaton S, then L has a unique CF grammar.\"\n\nProof:\nWe claim that the construction of the theorem generates a unique CF Γ grammar if the automaton MP to which it is applied is deterministic. First, recall the theorem that says that for a grammar Γ to be unique, it suffices to show that it has unique left outputs.\nSuppose that S admits w in an empty storage. It then does so with a single sequence of transitions, since it is deterministic and cannot operate when the storage is empty. Knowing this sequence of transitions, we can uniquely determine the choice of each production in the left output w in Γ.\nThe rule of the automaton S on the basis of which the production is applied is always unique. But a rule, say δ(q,a,X)={(r,Y1Y2…Yk)}, can generate many products Γ with different states at the positions reflecting the states of S after removing each of Y1, Y2 ,…, Aha. \nHowever, since S is deterministic, only one of these transition sequences is executed, so only one of these products actually results in the generation of w. \n\nTheorem 2: \"If L=L(S) for a DMP S, then L has a unique CGF.\" \n\nProof: \nLet $ be the missing \"final marker\" in the strings of L and let L'=L$. So the strings of L' are the strings of L with the symbol $ added. Then L' has the prefix property and L'=N(S') for some DMP S'. According to the theorem, there is a unique grammar Γ' that generates the language N(S'), i.e. L'.\nNow, given the grammar Γ', we construct Γ for which L(Γ)=L. To do this, simply remove the $ marker in the lines. We will regard $ as a variable of the grammar Γ and introduce the production $→ϵ; the remaining products of Γ and Γ' are the same. Since L(Γ')=L', we obtain L(Γ)=L.\nWe claim that Γ is unambiguous. Indeed, the left generations of Γ coincide with the left generations of Γ', except for the last step of Γ: replacing $ with e. Thus, if the terminal chain w had two generations left in Γ, then w$ would have two generations left in Γ'.\n\nSince Γ' is unambiguous, so is Γ."
    },
    {
        "question": "DMP machines and ambiguity",
        "answer": "\nTheorem 1: \"If L=N(S) for a DMP automaton S, then L has an unambiguous CF-grammar.\"\n\nProof:\nWe confirm that the construction of the theorem generates an unambiguous CF-grammar Γ when the automaton MP to which it is applied is deterministic. First, recall the theorem that says that for a grammar Γ to be unambiguous, it suffices to show that it has unique left derivations.\nSuppose that S admits w in empty storage. Therefore, it does so with only one transition sequence, because it is deterministic and cannot operate when the storage is empty. Knowing this transition sequence, we can uniquely determine the choice of each production in the left derivation of w in Γ.\nThe rule of the automaton S on the basis of which the production is applied is always unique. But a rule, say δ(q,a,X)={(r,Y1Y2…Yk)}, can generate a set of products Γ with different states at the positions reflecting the states of S after removing each of Y1, Y2, …, Yk. \nHowever, since S is deterministic, only one of these transition sequences is executed, so only one of these products actually results in the generation of w. \n\nTheorem 2: \"If L=L(S) for a DMP S, then L has a unique CFG. \" \n\nProof : \nLet $ be the missing \"end marker\" in the strings of L and let L'=L$. Thus, the strings of L' are the strings of L with the symbol $ appended to them. Then L' has the prefix property and L'=N(S') for a DMP S'. According to the theorem, there is an unambiguous grammar Γ' that generates the language N(S'), i.e. L'.\nNow, given the grammar Γ', we construct Γ for which L(Γ)=L. To do this, simply delete the $ marker in the lines. We will regard $ as a variable of the grammar Γ and introduce the production $→ϵ; the remaining products Γ and Γ' are the same. Since L(Γ')=L', we obtain L(Γ)=L.\nWe confirm that Γ is unambiguous. Indeed, the left generations of Γ coincide with the left generations of Γ', except for the last step in Γ, which is the replacement of $ by ϵ. Thus, if a terminal chain w had two left generations in Γ, then w$ would have two generations in Γ'.\n\nSince Γ' is unambiguous, Γ is also unambiguous."
    },
    {
        "question": "Prove the theorem of non-coincidence of the class of languages ​​recognized by DMP automata and arbitrary MP automata",
        "answer": "\nTheorem: \"The classes of languages ​​defined by MP-automata and DMP-automata with tolerance by accepting state do not coincide.\"\n\nProof:\nConsider the language L={0^n 1^n}∪{0^n 1^2n}. Obviously, the language L is context-free.\nSince the sets of languages ​​recognized by a DMP-automaton with tolerance by accepting state and a DMP-automaton with tolerance by empty stack coincide, we will consider a DMP-automaton with tolerance by accepting state in the proof.\nLet there exist a DMP-automaton with tolerance by accepting state M that recognizes the language L. Due to the determinism of the automaton, (s,z0,0^n 1^n)⊢∗(q1,γ1,1^n)⊢∗(q2,γ2,ε), where q1,q2∈T.\nLet us also consider the language L′′={0^n 1^n 2^n}.\n\nLet us construct a non-deterministic MP-automaton based on M that recognizes the language L′={0^n 1^n 2^n}∪{0^n 1^n}∪{0^n 1^2n}.\n1. First, let us construct an automaton M′ based on the automaton M, replacing all occurrences of the symbol 1 with the symbol 2.\n2. Next, let us combine the automata M and M′ into an automaton M′′, connecting the accepting states with ε-transitions (as shown in the picture).\nThe automaton M′′ is a non-deterministic MP-automaton and does not accept the context-free language L′.\nThe resulting contradiction proves that there is no DMP-automaton with an admission by an admitting state that recognizes the language L. But from the fact that L is context-free, it follows that there is a non-deterministic MP-automaton that recognizes it."
    },
    {
        "question": "Prove a theorem on the non-coincidence of the class of languages ​​recognized by DMP automata and arbitrary MP automata.",
        "answer": "\nTheorem: \"The classes of languages ​​defined by MP-automata and DMP-automata with state-tolerance do not coincide.\"\n\nProof:\nConsider the language L={0^n 1^n}∪{0^n 1^2n}. Obviously, the language L is context-free.\nSince the sets of languages ​​recognized by a DMP-automaton with state-tolerance and a DMP-automaton with empty stack tolerance coincide, in the proof we will consider a DMP-automaton with state-tolerance.\nLet there be a DMP-automaton with tolerance for accepting a state M, recognizing a language L. Due to the determinism of the automaton, (s,z0,0^n 1^n)⊢∗(q1,γ1,1^n)⊢∗(q2,γ2,ε), where q1,q2∈T.\nLet us also consider the language L′′={0^n 1^n 2^n}.\n\nLet us construct a non-deterministic MP-automaton based on M, recognizing the language L′={0^n 1^n 2^n}∪{0^n 1^n}∪{0^n 1^2n}.\n1. First, we construct an automaton M′ based on the automaton M, replacing all occurrences of the symbol 1 with the symbol 2.\n2. Next, we combine the automata M and M′ into an automaton M′′, connecting the accepting states with ε-transitions (as shown in the figure).\nThe automaton M′′ is a non-deterministic MP-automaton and does not accept the context-free language L′.\nThe resulting contradiction proves that there is no DMP-automaton with a tolerance on the accepting state that recognizes the language L. But since L is context-free, it follows that there is a non-deterministic MP-automaton that recognizes it."
    },
    {
        "question": "Prove a theorem on the gap between the class of languages ​​recognized by DMP automata and arbitrary MP automata.",
        "answer": "\nTheorem: \"The language classes defined by MP automata and DMP automata with state tolerance do not coincide.\"\n\nProof:\nConsider the language L={0^n 1^n}∪{0^n 1^2n}. Obviously, the language L is taken out of context.\nSince the sets of languages ​​recognized by the state-tolerant DMP automaton and the empty-stack-tolerant DMP automaton coincide, in the proof we will consider the state-tolerant DMP automaton.\nConsider the state-tolerant DMP automaton M that recognizes the language L. Since the automaton is deterministic, (s,z0,0^n 1^n)⊢∗(q1,γ1,1^n)⊢∗(q2,γ2 ,ε), where q1,q2εT.\nConsider also the language L′′={0^n 1^n 2^n}.\n\nLet us construct a non-deterministic MP automaton based on M that recognizes the language L′={0^n 1^n 2^n}∪{0^n 1^n}∪{0^n 1^2n}.\n1. First, let us construct an automaton M′ based on the automaton M, replacing all occurrences of the symbol 1 with the symbol 2.\n2. Next, we combine the automata M and M′ into an automaton M′′, connecting the tolerance states with ε-transitions (as shown in the figure).\nThe automaton M′′ is a non-deterministic MP automaton and does not accept the context-free language L′.\nThe resulting contradiction proves that there is no state-stable DMP that recognizes the language L. But since L is context-free, it follows that there is a non-deterministic MP that recognizes it."
    },
    {
        "question": "Prove the theorem of non-coincidence of the class of languages ​​recognized by DMP automata and arbitrary MP automata.",
        "answer": "\nTheorem: \"The language classes defined by the MP automata and accepting state-tolerant automata DMP do not coincide.\"\n\nProof:\nConsider the language L={0^n 1^n}∪{0^n 1^2n}. Obviously, the language L is a-contextual.\nSince the sets of languages ​​recognized by the accepting state-tolerant automaton DMP and the empty glass-tolerant automaton DMP coincide, in the proof we will consider the accepting state-tolerant automaton DMP.\nConsider the state-tolerant automaton DMP that accepts M and recognizes the language L. Since the automaton is deterministic, (s,z0,0^n 1^n)⊢∗(q1,γ1,1^n)⊢∗(q2 ,γ2,ε), where q1,q2εT.\nConsider also the language L′′={0^n 1^n 2^n}.\n\nLet us construct a non-deterministic MP automaton based on M that recognizes the language L′={0^n 1^n 2^n}∪{0^n 1^n}∪{0^n 1^2n}.\n1. First, let us construct an automaton M′ based on the automaton M, replacing all occurrences of the symbol 1 with the symbol 2.\n2. Next, let us combine the automata M and M′ into an automaton M′′, connecting the acceptor states with ε-transitions (as shown in the figure).\nThe automaton M′′ is a non-deterministic MP automaton and does not accept the context-free language L′.\nThe resulting contradiction proves that there is no DMP automaton with an admission by an admitting state that recognizes the language L. But since L is context-free, it follows that there is a non-deterministic MP automaton that recognizes it. ."
    },
    {
        "question": "Homomorphisms and context-free languages",
        "answer": "\nTheorem 1. For any homomorphism h:Σ1* → Σ2* and a context-free language L ⊆ Σ1*, the language h(L) is context-free.\n\nTheorem 2. For any homomorphism h:Σ1* → Σ2* and a context-free language L ⊆ Σ2*, the language h^(−1)(L) is context-free.\n\nTheorem 3. Consider an alphabet Σ0 = {a1, a2, b1, b2, c} and a language L0 ⊆ Σ0* generated by a context-free grammar G0:S → Cb1b2b1RC, C → c, C → cDC, D → A, D → AD, A → a1, A → a2, A → b1, A → b2, R → ε, R → T a1T b1, R → T a2T b2, T → R, T → RCC. An arbitrary language L ⊆ Σ* is context-free if and only if there exists a homomorphism h:Σ* → Σ0* such that L = h:(−1)(L0) or L = h:(−1)(L0 ∪ {ε}).\n\nChomsky–Schützenberger theorem. An arbitrary language L ⊆ Σ* is context-free if and only if there exist a natural number n, an automaton language L1 over the alphabet Σn^(D) = {a1, b1, a2, b2,...,an, bn}, and a homomorphism h:(Σn^(D))* → Σ* such that L = h(Ln^(D) ∩ L1), where Ln^(D) is the Dyck language over 2n letters.\n\nDefinition. A Dik language over 2n letters is a context-free language over the alphabet {a1, b1, a2, b2,...,an, bn} generated by the grammar S → ε, S → a1Sb1S, ..., S → anSbnS."
    },
    {
        "question": "Homomorphisms and context-free languages",
        "answer": "\nTheorem 1. For any homomorphism h:Σ1* → Σ2* and a context-free language L ⊆ Σ1*, the language h(L) is context-free.\n\nTheorem 2. For any homomorphism h:Σ1* → Σ2* and a context-free language L ⊆ Σ2*, the language h^(−1)(L) is context-free.\n\nTheorem 3. Consider an alphabet Σ0 = {a1, a2, b1, b2, c} and a language L0 ⊆ Σ0* generated by a context-free grammar G0:S → Cb1b2b1RC, C → c, C → cDC, D → A, D → AD, A → a1, A → a2, A → b1, A → b2, R → ε, R → T a1T b1, R → T a2T b2, T → R, T → RCC. An arbitrary language L ⊆ Σ* is context-free if and only if there exists a homomorphism h:Σ* → Σ0* such that L = h:(−1)(L0) or L = h:(−1)(L0 ∪ {ε}).\n\nChomsky–Schützenberger theorem. An arbitrary language L ⊆ Σ* is context-free if and only if there exist a natural number n, an automaton language L1 over the alphabet Σn^(D) = {a1, b1, a2, b2,...,an, bn}, and a homomorphism h:(Σn^(D))* → Σ* such that L = h(Ln^(D) ∩ L1), where Ln^(D) is the Dyck language over 2n letters.\n\nDefinition. The Dik language over 2n letters is a context-free language over the alphabet {a1, b1, a2, b2,...,an, bn} generated by the grammar S → ε, S → a1Sb1S, ..., S → anSbnS."
    },
    {
        "question": "Homomorphisms and context-free languages",
        "answer": "\nTheorem 1. For any homomorphism h:Σ1* → Σ2* and a context-free language L ⊆ Σ1*, the language h(L) is context-free.\n\nTheorem 2. For any homomorphism h:Σ1* → Σ2* and a context-free language L ⊆ Σ2*, the language h^(−1)(L) is context-free.\n\nTheorem 3. Consider the alphabet Σ0 = {a1, a2, b1, b2, c} and the language L0 ⊆ Σ0* generated by the context-free grammar G0:S → Cb1b2b1RC, C → c, C → cDC, D → A, D → AD, A → a1, A → a2, A → b1, A → b2, R → ε, R → T a1T b1, R → T a2T b2, T → R, T → RCC. An arbitrary language L ⊆ Σ* is context-free if and only if there exists a homomorphism h:Σ* → Σ0* such that L = h:(−1)(L0) or L = h:(−1)(L0 ∪ {ε}).\n\nChomsky–Schutzenberger theorem. An arbitrary language L ⊆ Σ* is context-free if and only if there exists a natural number n, an automaton language L1 over the alphabet Σn^(D) = {a1, b1, a2, b2,...,an , bn}, and a homomorphism h:(Σn^(D))* → Σ* such that L = h(Ln^(D) ∩ L1), where Ln^(D) is the Dyck language consisting of 2n letters.\n\nDefinition. The language Dik on 2n letters is a context-free language on the alphabet {a1, b1, a2, b2,...,an, bn} generated by the grammar S → ε, S → a1Sb1S, ..., S → anСбнС."
    },
    {
        "question": "Homomorphisms and context-free languages",
        "answer": "\nTheorem 1. For any homomorphism h:Σ1* → Σ2* and a context-free language L ⊆ Σ1*, the language h(L) is context-free.\n\nTheorem 2. For any homomorphism h:Σ1* → Σ2* and a context-free language L ⊆ Σ2*, the language h^(−1)(L) is context-free.\n\nTheorem 3. Consider the alphabet Σ0 = {a1, a2, b1, b2, c} and the language L0 ⊆ Σ0* generated by the context-free grammar G0:S → Cb1b2b1RC, C → c, C → cDC, D → A, D → AD, A → a1, A → a2, A → b1, A → b2, R → ε, R → T a1T b1, R → T a2T b2, T → R, T → RCC. An arbitrary language L ⊆ Σ* is a-contextual if and only if there is a homomorphism h:Σ* → Σ0* such that L = h:(−1)(L0) or L = h:(−1)( L0 ∪ {ε}).\n\nChomsky–Schützenberger theorem. An arbitrary language L ⊆ Σ* is a-contextual if and only if there is a natural number n, an automaton language L1 over the alphabet Σn^(D) = {a1, b1, a2, b2,...,an, bn} and a homomorphism h:(Σn^(D))* → Σ* such that L = h(Ln^(D) ∩ L1), where Ln^(D) is the Dyck language on 2n letters.\n\nDefinition. The language Dik on 2n letters is a context-free language on the alphabet {a1, b1, a2, b2,...,an, bn} generated by the grammar S → ε, S → a1Sb1S, ..., S → anСбнС."
    },
    {
        "question": "What is strong and weak bisimulation?",
        "answer": "\n\nStrong bisimulation\nLet there be two finite automata or transition systems, A and B, with state sets S_A and S_B, respectively. Strong bisimulation is a relation R⊆S_A×S_B that holds if for any pair of states (s_A, s_B)∈R:\n1. If in A there is a transition from state s_A​ by label α to state s_A'​, then in B there must also be a transition from state s_B​ by label α to state s_B'​, and (s_A', s_B')∈R.\n2. Similarly, if in B there is a transition by α from state s_B​, then in A there must also be a transition by α from state s_A, and these states must again be in relation R.\n\nWeak bisimulation\n\nLet there again be two finite automata or systems with transitions, A and B, with state sets S_A​ and S_B​. Weak bisimulation is a relation R⊆S_A×S_B, where:\n1. If in A there is a sequence of transitions from state s_A, including zero or more τ-transitions, by label α to state s_A'​, then there must also be a sequence of transitions in B, including zero or more τ-transitions, by label α to state s_B', and (s_A', s_B')∈R.\n2. Similarly, if in B there is a sequence of transitions from state s_B​ along label α to state s_B'​, then in A there must be a corresponding sequence of transitions."
    },
    {
        "question": "What is strong and weak bisimulation?",
        "answer": "\n\nStrong bisimulation\nLet there be two finite automata or transition systems, A and B, with state sets S_A and S_B, respectively. Strong bisimulation is a relation R⊆S_A×S_B that holds if for any pair of states (s_A, s_B)∈R:\n1. If in A there is a transition from state s_A​ by label α to state s_A'​, then in B there must also be a transition from state s_B​ by label α to state s_B'​, and (s_A', s_B')∈R.\n2. Similarly, if B has a transition on α from state s_B, then A must also have a transition on α from state s_A, and these states must again be in relation R.\n\nWeak bisimulation\n\nLet there again be two finite automata or transition systems, A and B, with state sets S_A and S_B. Weak bisimulation is a relation R⊆S_A×S_B, where:\n1. If A has a sequence of transitions from state s_A, including zero or more τ-transitions, by label α to state s_A', then B must also have a sequence of transitions, including zero or more τ-transitions, by label α to state s_B', and (s_A', s_B')∈R.\n2. Similarly, if in B there is a sequence of transitions from state s_B​ by label α to state s_B'​, then in A there must be a corresponding sequence of transitions."
    },
    {
        "question": "What is strong and weak bisimulation?",
        "answer": "\n\nStrong bisimulation\nConsider two finite automata or transition systems A and B with sets of states S_A and S_B, respectively. Strong bisimulation is a relation R⊆S_A×S_B that holds if for any pair of states (s_A, s_B)εR:\n1. If in A there is a transition from state s_A​ by label α to state s_A'​, then in B there must also be a transition from state s_B​ by label α to state s_B'​, and (s_A', s_B ')€R.\n2. Similarly, if B has a transition on α from state s_B, then A must also have a transition on α from state s_A, and these states must again be associated with R.\n\nWeak Bisimulation\n\nSuppose again that there are two finite state machines or transition systems, A and B, with state sets S_A and S_B. Weak bisimulation is a relation R⊆S_A×S_B, where:\n1. If A has a transition sequence from state s_A, involving zero or more transitions τ, on label α to state s_A', then B must also have a transition sequence involving zero or more transitions τ, on label α. to denote s_B' and (s_A', s_B')εR.\n2. Similarly, if B has a sequence of transitions from state s_B to label α to state s_B', then A must have a corresponding sequence of transitions."
    },
    {
        "question": "What is strong and weak bisimulation?",
        "answer": "\n\nStrong bisimulation\nConsider two finite automata or transition systems A and B with sets of states S_A and S_B, respectively. Strong bisimulation is a relation R⊆S_A×S_B that is true if for any pair of states (s_A, s_B)∈R:\n1. If in A there is a transition from state s_A​ by label α to state s_A'​, then in B there must also be a transition from state s_B​ by label α to state s_B'​, and (s_A' , s_B')€R.\n2. Similarly, if there is a transition via α from state s_B​ in B, then there must also be a transition via α from state s_A​ in A, and these states must again be in relation to A.\n\nWeak bisimulation\n\nConsider again two finite automata or transition systems A and B with state sets S_A​ and S_B​. Weak bisimulation is a relation R⊆S_A×S_B, where:\n1. If there is a sequence of transitions in A from state s_A, including zero or more τ-transitions, from label α to state s_A'​, then there must also be a sequence of transitions in B, including zero or more τ-transitions from label α to state s_B' and (s_A', s_B')εR.\n2. Similarly, if in B there is a sequence of transitions from state s_B​ to state s_B'​ by label α, then in A there must also exist a corresponding sequence of transitions."
    },
    {
        "question": "What is behavioral equivalence? How is it different from bisimulation?",
        "answer": "\n\nBehavioural equivalence is a concept used in finite state machine and transition system theory to describe how two states (or systems) behave identically from an observer's perspective.\n\nTwo states are said to be behaviourally equivalent if, given the same inputs, they transition to equivalent states and produce the same outputs.\nBehavioural equivalence is a more general concept than bisimulation. In the case of behavioural equivalence, only the external behaviour is important, not the structure of the transitions in the system."
    },
    {
        "question": "What is behavioral equivalence? How is it different from bisimulation?",
        "answer": "\n\nBehavioural equivalence is a concept used in finite state machine and transition system theory to describe how two states (or systems) behave identically from the perspective of an observer.\n\nTwo states are said to be behaviourally equivalent if, given the same inputs, they transition to equivalent states and produce the same outputs.\nBehavioural equivalence is a more general concept than bisimulation. In the case of behavioural equivalence, only the external behaviour is important, not the structure of the transitions in the system."
    },
    {
        "question": "What is behavioral equivalence? How is it different from bisimulation?",
        "answer": "\n\nBehavioural equivalence is a concept used in finite state machine and transition system theory to describe how two states (or systems) behave identically from the perspective of an observer.\n\nTwo states are said to be behaviorally equivalent if, given the same inputs, they transition to equivalent states and produce the same outputs.\nBehavioural equivalence is a more general concept than bisimulation. In the case of behavioral equivalence, only the external behavior is important, not the structure of the transitions in the system."
    },
    {
        "question": "What is behavioral equivalence? How is it different from bisimulation?",
        "answer": "\n\nBehavioural equivalence is a concept used in finite state machine and transition system theory to describe how two states (or systems) behave identically from the perspective of an observer.\n\nTwo states are said to be behaviorally equivalent if, given the same inputs, they transition to equivalent states and produce the same outputs.\nBehavioural equivalence is a more general concept than bisimulation. In the case of behavioral equivalence, only the external behavior is important, not the structure of the transitions in the system."
    },
    {
        "question": "What is inverse bisimulation?",
        "answer": "\n\nInverse bisimulation is a variation of the bisimulation relation that is used to compare the behavior of states in systems with transitions, such as finite state machines.\n\nIn classical bisimulation, we compare steps from one state to another (transitions), but in inverse bisimulation we look at previous steps, i.e. how states \"return\" to previous states via inverse transitions."
    },
    {
        "question": "What is inverse bisimulation?",
        "answer": "\n\nInverse bisimulation is a variation of the bisimulation relation that is used to compare the behavior of states in systems with transitions, such as finite state machines.\n\nIn classical bisimulation, we compare steps from one state to another (transitions), but in inverse bisimulation, we look at previous steps, i.e. how states \"return\" to previous states via inverse transitions."
    },
    {
        "question": "What is inverse bisimulation?",
        "answer": "\n\nInverse bisimulation is a type of bisimulation relation used to compare the behavior of states in transition systems such as finite state machines.\n\nIn classical bisimulation we compare steps from one state to another (transitions), but in inverse bisimulation we look at previous steps, i.e. how states \"return\" to previous states via inverse transitions."
    },
    {
        "question": "What is inverse bisimulation?",
        "answer": "\n\nInverse bisimulation is a variant of the bisimulation relation that is used to compare the behavior of states in systems with transitions, such as finite state machines.\n\nIn classical bisimulation, we compare the steps from one state to another (transitions), but in inverse bisimulation, we look at previous steps, i.e. how states \"return\" to previous states via inverse transitions."
    },
    {
        "question": "Write a proof of Brzozowski's theorem?",
        "answer": "\n\nProposition 1.\nAn automaton is deterministic if and only if the left languages ​​of its states are pairwise disjoint.\nProposition 2.\nIf A recognizes a language L, then r(A) recognizes r(L).\nProposition 3.\nIf the left language of a state q in A is L_g(q), then its left language in r(A) is L_d(q). Similarly for the right language q.\nProposition 4.\nThe right language of a state q′d(A) is equivalent to the union of the right languages ​​of states q of the automaton A that belong to the set q′.\nProposition 5.\nA deterministic automaton is minimal if and only if the right languages ​​of its states are distinct and all states are reachable.\n\nBy construction, the automaton drdr(A) is deterministic. By Proposition 2, it recognizes the language L.\nLet us show that all right languages ​​drdr(A) are distinct. By Proposition 1, the left languages ​​dr(A) are pairwise disjoint. By Proposition 3, the right languages ​​rdr(A) are left languages ​​dr(A). Thus, they are pairwise disjoint. By Proposition 4, the right language drdr(A) is the union of the right languages ​​rdr(A). Since the right languages ​​rdr(A) are pairwise disjoint, all right languages ​​drdr(A) are distinct.\nSince all right languages ​​drdr(A) are distinct, by Proposition 5 the automaton drdr(A) is minimal."
    },
    {
        "question": "Write a proof of Brzozowski's theorem?",
        "answer": "\n\nProposition 1.\nAn automaton is deterministic if and only if the left languages ​​of its states are pairwise disjoint.\nProposition 2.\nIf A recognizes a language L, then r(A) recognizes r(L).\nProposition 3.\nIf the left language of a state q in A is L_g(q), then its left language in r(A) is L_d(q). Similarly for the right language q.\nProposition 4.\nThe right language of a state q′d(A) is equivalent to the union of the right languages ​​of states q of the automaton A that belong to the set q′.\nProposition 5.\nA deterministic automaton is minimal if and only if the right languages ​​of its states are distinct and all states are reachable.\n\nBy construction, the automaton drdr(A) is deterministic. By Proposition 2, it recognizes the language L.\nLet us show that all right languages ​​drdr(A) are distinct. By Proposition 1, the left languages ​​dr(A) are pairwise disjoint. By Proposition 3, the right languages ​​rdr(A) are left languages ​​rdr(A). Thus, they are pairwise disjoint. By Proposition 4, the right language drdr(A) is the union of the right languages ​​rdr(A). Since the right languages ​​rdr(A) are pairwise disjoint, all right languages ​​rdr(A) are distinct.\nSince all right languages ​​rdr(A) are distinct, by Proposition 5, the automaton drdr(A) is minimal."
    },
    {
        "question": "Write a proof of Brzozowski's theorem?",
        "answer": "\n\nProposition 1.\nAn automaton is deterministic if and only if the left languages ​​of its states do not intersect in pairs.\nProposition 2.\nIf A recognizes a language L, then r(A) recognizes r(L).\nProposition 3.\nIf the left language of a state q in A is L_g(q), then its left language in a state r(A) is L_d(q). The same applies to a good language q.\nProposition 4.\nA good state language q'd(A) is equivalent to the union of good state languages ​​q of an automaton A belonging to the set q'.\nProposition 5.\nA deterministic automaton is minimal if and only if the good languages ​​of its states are distinct and all states are reachable.\n\nBy construction, the automaton drdr(A) is deterministic. By Proposition 2, it recognizes the language L.\nLet us show that all good languages ​​drdr(A) are distinct. By Proposition 1, the left languages ​​dr(A) are pairwise disjoint. By Proposition 3, the right languages ​​rdr(A) are the left languages ​​rdr(A). Thus, they are pairwise disjoint. By Proposition 4, a good language rdr(A) is the union of good languages ​​rdr(A). Since good languages ​​rdr(A) are pairwise disjoint, all good languages ​​rdr(A) are distinct.\nSince all good languages ​​rdr(A) are distinct, by Proposition 5, the automaton drdr(A) is minimal."
    },
    {
        "question": "Write a proof of Brzozowski's theorem?",
        "answer": "\n\nProposition 1.\nAn automaton is deterministic if and only if the left languages ​​of its states do not intersect two by two.\n\nProposition 2.\nIf A recognizes a language L, then r(A) recognizes r(L).\nProposition 3.\nIf the left language of a state q in A is L_g(q), then its left language in r(A) is L_d(q). Similarly for the right language q.\n\nProposition 4.\nThe right language of a state q'd(A) is equivalent to the union of the right languages ​​of states q of the automaton A that belong to the set q'.\n\nProposition 5.\nA deterministic automaton is minimal if and only if the right languages ​​of its states are distinct and all states are reachable.\n\nBy construction, the automaton drdr(A) is deterministic. By Proposition 2, we recognize the language L.\nLet us show that all right languages ​​drdr(A) are distinct. By Proposition 1, the left languages ​​dr(A) are pairwise disjoint. By Proposition 3, the right languages ​​rdr(A) are left languages ​​dr(A). Thus, they are pairwise disjoint. By Proposition 4, the right language drdr(A) is the union of the right languages ​​rdr(A). Since the right languages ​​rdr(A) are pairwise disjoint, all right languages ​​rdr(A) are distinct.\nSince all right languages ​​drdr(A) are distinct, by Proposition 5, the automaton drdr(A) is minimal."
    },
    {
        "question": "Describe the Aho-Corasick algorithm.",
        "answer": "\n\nAho-Corasick Algorithm\nDescription:\nThe Aho-Corasick algorithm is used to search for multiple strings in a text simultaneously.\n\nIt builds an automaton that can efficiently find all occurrences of given substrings (patterns) in a text string in a time proportional to the length of the text and the total number of characters in the patterns.\nThis algorithm combines elements of an automaton and a prefix tree (Trie).\nThe main steps are:\n1. Building a prefix tree (Trie): First, a prefix tree is built for all patterns. Each node in the tree represents a character, and the path from the root to the node corresponds to the prefix of one of the patterns.\n2. Adding transitions: At each level of the prefix tree, transitions are added for characters that do not match existing paths, using \"failure states\" to indicate where to go in case of failure.\n3. Construction of failure states: Each state (node) is assigned a failure state, indicating which node to jump to if the corresponding symbol is not found. This allows the text to be processed more efficiently, avoiding repeated checks. 4. Search: The text is processed in one pass. When reading each symbol, possible transitions in the automaton are checked, and when the end of the pattern is found, the corresponding occurrence is fixed."
    },
    {
        "question": "Describe the Aho-Corasick algorithm.",
        "answer": "\n\nAho-Corasick Algorithm\nDescription:\nThe Aho-Corasick algorithm is used to search for multiple strings in a text simultaneously.\n\nIt creates an automaton that can efficiently find all occurrences of given substrings (patterns) in a text string in time proportional to the length of the text and the total number of characters in the patterns.\nThis algorithm combines elements of an automaton and a prefix tree (Trie).\nThe main steps are:\n1. Creating a prefix tree (Trie): First, a prefix tree is built for all patterns. Each node in the tree represents a character, and the path from the root to the node corresponds to the prefix of one of the patterns.\n2. Adding transitions: At each level of the prefix tree, transitions are added for characters that do not match existing paths, using \"failure states\" to indicate where to go in case of failure.\n3. Constructing failure states: Each state (node) is assigned a failure state indicating which node to go to if the corresponding character is not found. This allows the text to be processed more efficiently, avoiding repeated checks. 4. Search: The text is processed in one pass. When reading each symbol, possible transitions in the machine are checked, and when the end of the pattern is found, the corresponding entry is recorded."
    },
    {
        "question": "Describe the Aho-Corasick algorithm.",
        "answer": "\n\nAho-Corasick Algorithm\nDescription:\nThe Aho-Corasick algorithm is used to search multiple lines of text simultaneously.\n\nIt creates an automaton that can efficiently find all occurrences of given substrings (patterns) in a text string in time proportional to the length of the text and the total number of characters in the patterns.\nThis algorithm combines elements of an automaton and a prefix tree (Trie).\nThe main steps are:\n1. Creating a prefix tree (Trie). First, a prefix tree is built for all patterns. Each node in the tree represents a character, and the path from the root to the node corresponds to a prefix of one of the patterns.\n2. Adding transitions. At each level of the prefix tree, transitions are added for characters that do not match existing paths, using \"failure states\" to indicate where to go in case of failure.\n3. Build Failure States: Each state (node) is assigned a failure state indicating which node to jump to if the matching character is not found. This allows the text to be processed more efficiently by avoiding repeated checks. 4. Search: The text is processed in one pass. As each character is read, possible transitions in the machine are checked, and when the end of the pattern is found, the corresponding occurrence is written."
    },
    {
        "question": "Describe the Aho-Corasick algorithm.",
        "answer": "\n\nAho-Corasick Algorithm\nDescription:\nThe Aho-Corasick algorithm is used to search for multiple strings in a text string simultaneously.\n\nIt builds an automaton that can efficiently find all occurrences of given substrings (patterns) in a text string in time proportional to the length of the text and the total number of characters in the patterns.\nThis algorithm combines elements of an automaton and a prefix tree (Trie).\nThe main steps are:\n1. Building a prefix tree (Trie). First, a prefix tree is built for all patterns. Each node in the tree represents a character, and the path from the root to the node corresponds to a prefix of one of the patterns.\n2. Adding transitions. At each level of the prefix tree, transitions are added for characters that do not match existing paths, using \"failure states\" to indicate where to go in case of failure.\n3. Building failure states. Each state (node) is assigned a failure state, indicating which node to go to if the corresponding symbol is not found. This allows for more efficient processing of the text, avoiding repeated checks. 4. Search: the text is processed in one pass. When reading each symbol, possible transitions in the automaton are checked, and when the end of the pattern is found, the corresponding entry is fixed."
    },
    {
        "question": "Describe the Burrows-Wheeler algorithm.",
        "answer": "\n\nAn algorithm used to pre-process data before compression, designed to improve the efficiency of subsequent coding.\n\nThe Burrows-Wheeler transform changes the order of characters in the input string so that repeated substrings form consecutive sequences of identical characters at the output.\nAlgorithm Description\nThe transformation is performed in three stages:\n1. A table of all cyclic shifts of the input string is compiled.\n2. The rows of the table are lexicographically (alphabetically) sorted.\n3. The last column of the transformation table and the number of the row that matches the original are selected as the output row."
    },
    {
        "question": "Describe the Burrows-Wheeler algorithm.",
        "answer": "\n\nAn algorithm used for pre-processing data before compression, designed to improve the efficiency of subsequent encoding.\n\nThe Burrows-Wheeler transform changes the order of characters in the input string so that repeated substrings form consecutive sequences of identical characters at the output.\nAlgorithm Description\nThe transformation is performed in three stages:\n1. A table of all cyclic shifts of the input string is compiled.\n2. The rows of the table are sorted lexicographically (alphabetically).\n3. The last column of the transformation table and the row number corresponding to the original are selected as the output row."
    },
    {
        "question": "Describe the Burrows-Wheeler algorithm.",
        "answer": "\n\nAn algorithm used to pre-process data before compression to improve the efficiency of subsequent encoding.\n\nThe Burrows-Wheeler transform changes the order of characters in an input string so that repeated substrings produce successive sequences of identical characters in the output.\nAlgorithm Description\nThe transformation is performed in three stages:\n1. A table of all cyclic shifts of the input string is compiled.\n2. The rows of the table are sorted lexicographically (alphabetically).\n3. The last column of the transform table and the row number corresponding to the original are selected as the output row."
    },
    {
        "question": "Describe the Burrows-Wheeler algorithm.",
        "answer": "\n\nAn algorithm used to pre-process data before compression, designed to improve the efficiency of subsequent encoding.\n\nThe Burrows-Wheeler transform changes the order of characters in the input string so that repeated substrings form consecutive sequences of identical characters in the output.\nAlgorithm Description\nThe transformation occurs in three stages:\n1. A table of all cyclic shifts of the input string is compiled.\n2. The rows of the table are sorted lexicographically (alphabetically).\n3. The last column of the transformation table and the row number corresponding to the original are selected as the output row."
    },
    {
        "question": "Write a lemma on expansion for CF grammars.",
        "answer": "\n\nLet L be a context-free language over an alphabet Σ, then there exists n such that for any word ω∈L of length at least n there are words u,v,x,y,z∈Σ∗ for which uvxyz=ω,vy≠ε,|vxy|⩽n and ∀k⩾0 uv_kxy_kz∈L."
    },
    {
        "question": "Write a lemma about the extension of CF grammars.",
        "answer": "\n\nLet L be a context-free language over an alphabet Σ, then there exists n such that for any word ω∈L of length at least n there exist words u,v,x,y,z∈Σ∗ for which uvxyz=ω,vy≠ε,|vxy|⩽n and ∀k⩾0 uv_kxy_kz∈L."
    },
    {
        "question": "Write a lemma about the extension of CF grammars.",
        "answer": "\n\nLet L be a context-free language of alphabet Σ, then there exists n such that for every word ωεL of length at least n there exist words u,v,x,y,zاΣ∗ for which uvxyz=ω ,vy≠ ε,|vxy|⩽n and ∀k⩾0 uv_kxy_kzاL."
    },
    {
        "question": "Write a lemma about the construction of CF grammars.",
        "answer": "\n\nLet L be a context-free language in the alphabet Σ, then there exists n such that for every word ωεL of length at least n there exist words u,v,x,y,zاΣ∗ for which uvxyz=ω , vy≠ε,|vxy|⩽n and ∀k⩾0 uv_kxy_kzاL."
    },
    {
        "question": "Describe the methods for obtaining minimum machines.",
        "answer": "\n\nTransition table\n\nIf a transition table and an equivalent partition Σ1..Σň of an automaton S are given, then the transition table Š can be constructed as follows:\n1. Replace the designation of each state in the table S with the designation of the class to which this state belongs.\n2. From each group of rows with the same designations in the cells of the main column, cross out all rows except one.\nThe resulting table is the transition table Š.\n\nTransition graph\n\nIf a transition graph (state diagram) and an equivalent partition Σ1..Σň of an automaton S are given, then the transition graph Š can be constructed as follows:\n1. Replace the designation of each state in the transition graph S with the designation of the class to which this state belongs.\n2. Combine all states with the same designation (considering the graph arcs as \"flexible connections\") and represent the combined states as a single state with a common designation.\n3. From each group of arcs with a common initial and common final state, delete all but one.\nThe resulting graph will be graph Š.\n\nTransition matrix\n\nIf the transition matrix and equivalent partition Σ1..Σň of the automaton S are given, then the transition matrix Š can be constructed as follows:\n1. Perform a symmetric permutation and symmetric partition [S] so that the rows (and columns) are grouped according to the equivalence classes of S (the same matrix can be obtained using the matrix method of equivalent partition).\n2. Replace all the designations of the rows (and columns) of each group representing an equivalence class with a single designation of this class.\n3. Replace each submatrix in the split matrix with a single cell containing all input-output pairs that are present in any row of that submatrix (all rows in any such submatrix contain the same set of input-output pairs).\nThe resulting matrix is ​​the transition matrix Š."
    },
    {
        "question": "Describe the methods for obtaining the minimum number of machines.",
        "answer": "\n\nTransition table\n\nIf a transition table and an equivalent partition Σ1..Σň of an automaton S are given, then the transition table Š can be constructed as follows:\n1. Replace the designation of each state in table S with the designation of the class to which this state belongs.\n2. From each group of rows with the same designations in the cells of the main column, cross out all rows except one.\n\nThe resulting table is the transition table Š.\n\nTransition graph\n\nIf a transition graph (state diagram) and an equivalent partition Σ1..Σň of an automaton S are given, then the transition graph Š can be constructed as follows:\n1. Replace the designation of each state in the transition graph S with the designation of the class to which this state belongs.\n2. Combine all states with the same designation (considering the graph arcs as \"flexible connections\") and represent the combined states as a single state with a common designation.\n3. From each group of arcs with a common initial and common final state, remove all but one.\nThe resulting graph will be graph Š.\n\nTransition matrix\n\nIf the transition matrix and the equivalent partition Σ1..Σň of the automaton S are given, then the transition matrix Š can be constructed as follows:\n1. Perform a symmetric permutation and a symmetric partition [S] so that the rows (and columns) are grouped according to the equivalence classes of S (the same matrix can be obtained using the matrix method of equivalent partitioning).\n2. Replace all the row (and column) designations of each group representing an equivalence class with a single designation of this class.\n3. Replace each submatrix in the partitioned matrix with a single cell containing all input-output pairs that appear in any row of that submatrix (all rows in any such submatrix contain the same set of input-output pairs).\nThe resulting matrix is ​​the transition matrix Š."
    },
    {
        "question": "Describe the methods for obtaining the minimum number of cars.",
        "answer": "\n\nTransition table\n\nIf a transition table and an equivalent partition Σ1..Σň of an automaton S are given, then the transition table Š can be constructed as follows:\n1. Replace the designation of each state in table S with the designation of the class to which this state belongs.\n2. In each group of rows with the same designations in the cells of the main column, cross out all rows except one.\n\nThe resulting table is the transition table Š.\n\nTransition diagram\n\nIf a transition graph (state diagram) and an equivalent partition Σ1..Σň of an automaton S are given, then the transition graph Š can be constructed as follows:\n1. Replace the designation of each state in the transition graph S with the designation of the class to which this state belongs.\n2. Combine all states with the same designation (considering the graphic arcs as \"flexible connections\") and represent the combined states as a single state with a common designation.\n3. From each group of edges that have a common initial and common final state, remove all but one.\nThe resulting graph will be the graph of Š.\n\nTransition matrix\n\nIf the transition matrix and the equivalent partition Σ1..Σň of an automaton S are given, then the transition matrix Š can be constructed as follows:\n1. Perform a symmetric permutation and a symmetric partition [S] so that the rows (and columns) are grouped according to equivalence classes of S (the same matrix can be obtained using the matrix equivalent partitioning method).\n2. Replace all row (and column) notations in each group representing an equivalence class with the notations of that class.\n3. Replace each submatrix of the partitioned matrix with a single cell containing all input-output pairs present in any row of this submatrix (all rows of such a submatrix contain the same set of input-output pairs).\nThe resulting matrix is ​​the transition matrix Š."
    },
    {
        "question": "Describe the methods for obtaining the minimum number of machines.",
        "answer": "\n\nTransition table\n\nIf we have a transition table and an equivalent partition Σ1..Σň of an automaton S, we can construct a transition table Š as follows:\n1. Replace the designation of each state in the table S with the designation of the class to which this state belongs.\n2. In each group of rows with the same designation in the cells of the main column, cross out all rows except one.\nThe resulting table is a transition table Š.\n\nTransition graph\n\nIf we have a transition graph (state diagram) and an equivalent partition Σ1..Σň of an automaton S, we can construct a transition graph Š as follows:\n1. Replace the designation of each state of the transition graph S with the designation of the class to which this state belongs.\n2. Combine all states with the same designation (considering the arcs of the graph as \"flexible connections\") and represent the combined states as a single state with a common designation.\n3. From each group of edges that have a common initial and final state, remove all but one.\nThe resulting graph is the graph of Š.\n\nTransition matrix\n\nIf the transition matrix and its equivalent partition Σ1..Σň of an automaton S are given, then the transition matrix Š can be constructed as follows:\n1. Perform a symmetric permutation and a symmetric partition [S] so that the rows (and columns) are grouped according to equivalence classes of S (the same matrix can be obtained using the equivalent partition matrix method).\n2. Replace all row (and column) designations in each group representing an equivalence class with a single designation of this class.\n3. Replace each submatrix of the partitioned matrix with a single cell containing all input-output pairs present in any row of this submatrix (all rows of such a submatrix contain the same set of input-output pairs).\nThe resulting matrix is ​​the transition matrix Š."
    },
    {
        "question": "Write the properties of a minimal automaton.",
        "answer": "\n\nIf Š is a minimal form of an automaton S, then:\n1. Š is the only minimal form up to the designation of states\n2. Š=S\n3. No two states of Š are equivalent\n4. There is no automaton equivalent to S and smaller (with fewer states) than Š."
    },
    {
        "question": "Write the properties of a minimal automaton.",
        "answer": "\n\nIf Š is a minimal form of an automaton S, then:\n1. Š is a unique minimal form up to the designation of states\n2. Š=S\n3. No two states of Š are equivalent\n4. There is no automaton equivalent to S and smaller (with fewer states) than Š."
    },
    {
        "question": "Write the properties of a minimal automaton.",
        "answer": "\n\nIf Š is a minimal form of the automaton S, then:\n1. Š is the only minimal form before the designation of states.\n2. Ш=С\n3. No two states are equivalent\n4. There is no automaton equivalent to S and smaller (with fewer states) than Š."
    },
    {
        "question": "Write the properties of a minimal automaton.",
        "answer": "\n\nIf Š is a minimal form of the automaton S, then:\n1. Š is the only minimal form before the designation of states.\n2. Ш=С\n3. No state of Š is equivalent.\n4. There is no automaton equivalent to S and smaller (with fewer states) than Š."
    },
    {
        "question": "Write a definition of right-context grammar.",
        "answer": "\n\nA right-context grammar is a type of grammar in formal language theory where the rewriting rules depend on the context of the symbols to the right of the nonterminal being rewritten.\n\nSuch grammars are formalized as follows:\nThe grammar rule has the form: αAβ→αγβ, where A is a nonterminal symbol, α and β are strings of terminals and nonterminals, and γ is a string that replaces A.\n\nIt is important that the replaced symbol A can be rewritten only if there is a string β to the right of it, which gives context to the rules."
    },
    {
        "question": "Write a definition of right context grammar.",
        "answer": "\n\nA right-content grammar is a type of grammar in formal language theory where the rewriting rules depend on the context of symbols to the right of the nonterminal being rewritten.\n\nSuch grammars are formalized as follows:\nA grammar rule has the form: αAβ→αγβ, where A is a nonterminal symbol, α and β are strings of terminals and nonterminals, and γ is a string that replaces A.\n\nIt is important that the symbol A being replaced can only be rewritten if there is a string β to the right of it that provides context for the rules."
    },
    {
        "question": "Write a definition of correct contextual grammar.",
        "answer": "\n\nA right-content grammar is a type of grammar in formal language theory where the rewriting rules depend on the context of symbols to the right of the nonterminal being rewritten.\n\nSuch grammars are formalized as follows:\nGrammar rule: αAβ→αγβ, where A is a nonterminal symbol, α and β are terminal and nonterminal strings, and γ is a string replacing A.\n\nIt is important to note that the replaced symbol A can be rewritten only if there is a string β to the right of it that provides context for the rules."
    },
    {
        "question": "Write the definition of grammar in the correct context.",
        "answer": "\n\nRight context grammar is a type of grammar in formal language theory where the rewriting rules depend on the context of symbols to the right of the nonterminal being rewritten.\n\nSuch grammars are formalized as follows:\nThe grammar rule is of the form: αAβ→αγβ, where A is a nonterminal symbol, α and β are strings of terminals and nonterminals, and γ is a string replacing A.\n\nIt is important that the symbol A being replaced can be rewritten only if there is a string β to the right of it that provides context to the rules."
    },
    {
        "question": "Describe the Cocke-Younger-Kasami algorithm.",
        "answer": "\n\nThe Cocke-Younger-Kasami algorithm is an algorithm that allows one to find out, given a word, whether it is derivable in a given CFG in Chomsky normal form. Any CFG can be reduced to NFH, so the algorithm is universal for any CFG.\nAlgorithm steps:\n1. Initialization of the table:\n\nLet the string w have length n. Create a two-dimensional table T of size n×n, where T[i,j] will contain a set of nonterminals that can generate a substring of the string w, starting from position i and having length j.\n\n2. Filling the table for substrings of length 1:\n\nFor each substring of length 1 (each character of the string), if there is a rule of the form A→a, where a is the character of the substring, then fill the corresponding cell of the table T[i,1] with the nonterminal A. This initializes the table, filling it for the smallest substrings (individual characters).\n\n3. Filling the table for substrings of greater length:\n\nFor each l from 2 to n (the length of the substring), for each position i (the starting position of the substring), and for each k from 1 to l−1, check for possible splits of the substring into two parts.\nIf there is a rule A→BC in the grammar, and if nonterminal B can produce the left part of the substring (i.e. T[i,k] contains B), and C can produce the right part of the substring (i.e. T[i+k,l−k] contains C), then add A to T[i,l].\n\n4. Result:\n\nIf the cell T[1,n] (which corresponds to the entire string of length n) contains the start symbol of the grammar, then the string belongs to the language. Otherwise, the string cannot be produced by the grammar."
    },
    {
        "question": "Describe the Cockey-Younger-Kasami algorithm.",
        "answer": "\n\nThe Cocke-Younger-Kasami algorithm is an algorithm that finds whether a given word is derivable in a given CFG in Chomsky normal form. Any CFG can be reduced to NFH, so the algorithm is universal for any CFG.\nAlgorithm steps:\n1. Initialize the table:\n\nLet w be a string of length n. Create a two-dimensional table T of size n×n, where T[i,j] will contain the set of nonterminals that can generate a substring of w starting at position i and having length j.\n\n2. Fill the table for substrings of length 1:\n\nFor each substring of length 1 (each character of the string), if there is a rule of the form A→a, where a is a character of the substring, then fill the corresponding cell of the table T[i,1] with the nonterminal A. This initializes the table, filling it for the smallest substrings (individual characters).\n\n3. Filling the table for longer substrings:\n\nFor each l from 2 to n (the length of the substring), for each position i (the starting position of the substring), and for each k from 1 to l−1, check possible splits of the substring into two parts.\nIf the grammar has a rule A→BC, and if the nonterminal B can produce the left part of the substring (i.e. T[i,k] contains B), and C can produce the right part of the substring (i.e. T[i+k,l−k] contains C), then add A to T[i,l].\n\n4. Result:\n\nIf the cell T[1,n] (corresponding to the entire string of length n) contains the start symbol of the grammar, then the string belongs to the language. Otherwise, the string cannot be produced by the grammar."
    },
    {
        "question": "Describe the Cockey-Younger-Kasami algorithm.",
        "answer": "\n\nThe Cocke-Younger-Kasami algorithm is an algorithm that determines whether a given word can be printed in a given CFG in Chomsky normal form. Any CFG can be reduced to NFH, so the algorithm is universal for any CFG.\nAlgorithm steps:\n\n1. Initialize the table:\n\nLet w be a string of length n. Create a two-dimensional array T of size n×n, where T[i,j] will contain the set of nonterminals that can generate a substring of w starting at position i and having length j.\n\n2. Fill the table for substrings of length 1:\n\nFor each substring of length 1 (each character of the string), if there is a rule of the form A→a, where a is a character of the substring, then fill the corresponding cell of the table T[i,1] with the nonterminal A. This initializes the table by filling it with the smallest substrings (individual characters).\n\n3. Filling the table for longer substrings:\n\nFor each l from 2 to n (the length of the substring), for each position i (the starting position of the substring), and for each k from 1 to l−1, check possible splits of a substring consisting of two parts.\nIf the grammar obeys the rule A→BC, and the nonterminal B can produce the left part of the substring (i.e. T[i,k] contains B), and C can produce the right part of the substring (i.e. T[i++ k] ,l−k] contains C), then we add A to T[i,l].\n\n4. Result:\n\nIf the cell T[1,n] (corresponding to the entire string of length n) contains the start symbol of the grammar, then the string belongs to the language. Otherwise, the string cannot be produced by the grammar."
    },
    {
        "question": "Describe the Cockey-Younger-Kasami algorithm.",
        "answer": "\n\nThe Cock-Younger-Kasami algorithm is an algorithm that allows us to find out, given a word, whether it is differentiable in a given CFG in Chomsky normal form. Any CFG can be reduced to NFH, so the algorithm is universal for any CFG.\nAlgorithm steps:\n1. Initialize the table:\n\nLet w be a string of length n. Create a two-dimensional array T of size n×n, where T[i,j] will contain the set of nonterminals that can generate a substring of w starting at position i and having length j.\n\n2. Filling the table for substrings of length 1:\n\nFor each substring of length 1 (each character of the string), if there is a rule of the form A→a, where a is a character of the substring, then fill the corresponding cell of the table T[i,1] with the nonterminal A. It initializes the array by filling it with the smallest substrings (individual characters).\n\n3. Filling the table for substrings of greater length:\n\nFor each l from 2 to n (the length of the substring), for each position i (the starting position of the substring), and for each k from 1 to l−1, check the possible splits of a substring consisting of two parts.\nIf there is a rule A→BC in the grammar and nonterminal B can produce the left part of a substring (i.e. T[i,k] contains B) and nonterminal C can produce the right part of a substring (i.e. T[i+k,l−k] contains C), then add A to T[i,l].\n\n4. Result:\n\nIf the cell T[1,n] (which corresponds to the entire string of length n) contains the start symbol of the grammar, then the string belongs to the language. Otherwise, the string cannot be produced by the grammar."
    },
    {
        "question": "Write a general definition of the expansion lemma.",
        "answer": "\n\nIf L is a regular language, then there exists a number n⩾1 such that for any word uwv in L, where |w|⩾n can be written in the form uwv=uxyzv, where x, y, and z are such that |xy|⩽n, |y|⩾1, and ux(y^i)zv belongs to L for any integer i⩾0."
    },
    {
        "question": "Write a general definition of the extension lemma.",
        "answer": "\n\nIf L is a regular language, then there exists a number n⩾1 such that for any word uwv in L, where |w|⩾n, it can be written as uwv=uxyzv, where x, y, and z are such that |xy|⩽n, |y|⩾1, and ux(y^i)zv belongs to L for any integer i⩾0."
    },
    {
        "question": "Write a general definition of the extension lemma.",
        "answer": "\n\nIf L is a regular language, then there exists a number n⩾1 such that for every word uwv in L, where |w|⩾n, we can write uwv=uxyzv, where x, y, and z are such that |xy | ⩽ n, |y|⩾1, and ux(y^i)zv belongs to L for every integer i⩾0."
    },
    {
        "question": "Write a general definition of the decomposition lemma.",
        "answer": "\n\nIf L is a regular language, then there exists a number n⩾1 such that for every word uwv in L, where |w|⩾n, we can write uwv=uxyzv, where x, y, and z are such that |xy| ⩽n, |y|⩾1, and ux(y^i)zv belongs to L for every integer i⩾0."
    },
    {
        "question": "Context-free (CF) grammar is",
        "answer": "\nA context-free grammar is a grammar in which the left-hand sides of all rules contain only single nonterminals.\nA language defined by a context-free grammar is called a context-free language.\nA context-free grammar: G = <N, ∑, P, S>, where the rules are of the form: A → β, where β ∈ (N + ∑)*"
    },
    {
        "question": "A context-free (CF) grammar is",
        "answer": "\nA context-free grammar is a grammar in which the left-hand sides of all rules contain only single nonterminals.\nA language defined by a context-free grammar is called a context-free language.\nA context-free grammar: G = <N, ∑, P, S>, where the rules are of the form: A → β, where β ∈ (N + ∑)*"
    },
    {
        "question": "Context-free grammar (CF) is",
        "answer": "\nA context-free grammar is a grammar in which the left-hand sides of all rules contain only one nonterminal.\nA language defined by a context-free grammar is called a context-free language.\nA grammar without context: G = <N, ∑, P, S>, where the rules are: A → β, where β ∈ (N + ∑)*"
    },
    {
        "question": "Context-free grammar (CF) is",
        "answer": "\nA context-free grammar is a grammar in which the left-hand sides of all rules contain only simple nonterminals.\nA language defined by a context-free grammar is called a context-free language.\nA context-free grammar: G = <N, ∑, P, S>, where the rules are of the form: A → β, where β ∈ (N + ∑)*"
    },
    {
        "question": "Left-handed (Left) inference in CS grammar is",
        "answer": "\nThe output in a CFG is left-handed (left) if at each output step the leftmost of all occurrences of non-terminal symbols is replaced, \nthat is, each output step has the form: uAθ —> uβθ, where (A → β) ∈ P, A ∈ N and θ ∈ (N U ∑)*, u ∈ ∑*.\nLeft-handed output is uniquely reconstructed from the output tree."
    },
    {
        "question": "Left-handed (left-handed) inference in CS grammar is",
        "answer": "\nThe output of a CFG is left-handed if at each output step the leftmost of all occurrences of non-terminal symbols is replaced, \nthat is, each output step has the form: uAθ —> uβθ, where (A → β) ∈ P, A ∈ N and θ ∈ (N U ∑)*, u ∈ ∑*.\nThe left-handed output is uniquely recovered from the output tree."
    },
    {
        "question": "Left (left) derivation in CS grammar:",
        "answer": "\nThe output of a CFG is left-handed (left-handed) if at each output step the leftmost of all occurrences of non-terminal symbols is replaced, \ni.e. each output step has the form: uAθ -> uβθ, where (A → β) € P, A € N and θ € (N U ∑)*, u € ∑*.\nThe output of the left-hand side is uniquely reconstructed from the output tree."
    },
    {
        "question": "Left (left) derivation in CS grammar:",
        "answer": "\nAn output in a CFG is left-handed (left-handed) if at each output step the leftmost occurrence of all occurrences of non-terminal symbols is replaced, \ni.e. each output step has the form: uAθ —> uβθ, where (A → β) ∈ P, A ∈ N and θ ∈ (N U ∑)*, u ∈ ∑*.\nThe left-hand output is uniquely reconstructed from the output shaft."
    },
    {
        "question": "Left recursive CS grammar is",
        "answer": "\nA left-recursive grammar is a grammar in which there is a non-terminal that can be replaced by itself in the left direction, that is, a grammar that has at least one left-recursive non-terminal."
    },
    {
        "question": "Left recursive CS grammar is",
        "answer": "\nA left-recursive grammar is a grammar in which there is a non-terminal that can be replaced by itself in the left direction, that is, a grammar that has at least one left-recursive non-terminal."
    },
    {
        "question": "Left recursive CS grammar:",
        "answer": "\nA left-recursive grammar is a grammar that has a non-terminal that can be replaced by itself in the left direction, that is, a grammar that has at least one left-recursive non-terminal."
    },
    {
        "question": "Left recursive CS grammar:",
        "answer": "\nA left-recursive grammar is one in which there exists a non-terminal that can be replaced by itself in the left direction, that is, a grammar that has at least one left-recursive non-terminal."
    },
    {
        "question": "Right recursive CS grammar is",
        "answer": "\nA right-recursive grammar is a grammar that has a non-terminal that can be replaced by itself in the right direction, that is, a grammar that has at least one right-recursive non-terminal."
    },
    {
        "question": "Right recursive CS grammar is",
        "answer": "\nA right-recursive grammar is a grammar in which there is a nonterminal that can be replaced by itself in the right direction, that is, a grammar in which there is at least one right-recursive nonterminal."
    },
    {
        "question": "Correct recursive CS grammar:",
        "answer": "\nA right-recursive grammar is a grammar that has a non-terminal that can be replaced by itself in the right direction, that is, a grammar that has at least one right-recursive non-terminal."
    },
    {
        "question": "Right recursive CS grammar",
        "answer": "\nA right-recursive grammar is a grammar that has a non-terminal that can be replaced by itself in the right direction, that is, a grammar that has at least one right-recursive non-terminal."
    },
    {
        "question": "Recursive CS grammar is",
        "answer": "\nA recursive grammar is a grammar in which all nonterminals are recursive (except perhaps S). A nonterminal A of a CF grammar is called recursive if A → αAβ for some α and β. \nIf α = ε, then A is called left-recursive; if β = ε, then A is called right-recursive."
    },
    {
        "question": "Recursive CS grammar is",
        "answer": "\nA recursive grammar is a grammar in which all nonterminals are recursive (except possibly S). A nonterminal A of CF is called recursive if A → αAβ for some α and β. \nIf α = ε, then A is called left-recursive; if β = ε, then A is called right-recursive."
    },
    {
        "question": "Recursive CS Grammar",
        "answer": "\nA recursive grammar is a grammar in which all nonterminals are recursive (except perhaps S). A nonterminal A of a CF grammar is called recursive if A → αAβ for some α and β. \nIf α = ε, then A is called left-recursive; if β = ε, then A is called right-recursive."
    },
    {
        "question": "Recursive CS Grammar",
        "answer": "\nA recursive grammar is a grammar in which all nonterminals are recursive (except perhaps S). A nonterminal A of a CF grammar is called recursive if A → αAβ for some α and β. \nIf α = ε, then A is called left-recursive; if β = ε, then A is called right-recursive."
    },
    {
        "question": "A useless symbol (nonterminal) in CS grammar is",
        "answer": "\nThe symbol D (nonterminal) is useless unless the grammar has an output of the form S → *wDy → *wvy, where v, w, y ∈ ∑*"
    },
    {
        "question": "A useless symbol (nonterminal) in CS grammar is",
        "answer": "\nThe symbol D (nonterminal) is useless unless the grammar has an output of the form S → *wDy → *wvy, where v, w, y ∈ ∑*"
    },
    {
        "question": "An unnecessary (non-terminal) symbol in the CS grammar is",
        "answer": "\nThe symbol D (non-terminal) is useless unless the grammar has a derivation of the form S → *wDy → *wvy, where v, w, y ∈ ∑*"
    },
    {
        "question": "An unnecessary (non-terminal) symbol in the CS grammar is",
        "answer": "\nThe symbol D (non-terminal) is useless unless the grammar has an output of the form S → *wDy → *wvy, where v, w, y ∈ ∑*"
    },
    {
        "question": "Non-contracting KS grammar is",
        "answer": "\nA non-contracting CS grammar is a grammar without ε-rules. An example of a non-contracting CS grammar: S → aSb | a."
    },
    {
        "question": "Non-contractual KS grammar is",
        "answer": "\nA non-contractual CS grammar is a grammar without ε-rules. An example of a non-contractual CS grammar is: S → aSb | a."
    },
    {
        "question": "The non-contractual grammar of KS is",
        "answer": "\nA non-contractual CS grammar is a grammar without ε-rules. An example of a non-contractual CS grammar is: S → aSb | A."
    },
    {
        "question": "Non-contractual grammar KS",
        "answer": "\nA non-compressible CS grammar is a grammar without ε rules. An example of a non-compressible CS grammar is: S → aSb | has."
    },
    {
        "question": "KS grammar with chain rule is",
        "answer": "\nA chain rule is a rule in which one nonterminal is replaced by another nonterminal without the appearance of terminal symbols.\nFor example, rules of the form A → B and B → C."
    },
    {
        "question": "KS Grammar with Chain Rule",
        "answer": "\nA chain rule is a rule in which one nonterminal is replaced by another nonterminal without introducing terminal symbols.\nFor example, rules of the form A → B and B → C."
    },
    {
        "question": "KS Grammar with Chain Rule",
        "answer": "\nA chain rule is a rule in which a nonterminal is replaced by another nonterminal without creating terminal symbols.\nFor example, rules such as A → B and B → C."
    },
    {
        "question": "KS grammar with chain rule:",
        "answer": "\nA chain rule is a rule in which a nonterminal is replaced by another nonterminal without introducing terminal symbols.\nFor example, rules of the form A → B and B → C."
    },
    {
        "question": "KS grammar without cycles",
        "answer": "\nA loop-free KS grammar is a KS grammar that does not have the rule: A → +A. That is, nonterminals cannot be replaced by themselves after any number of steps.\nAn example of a loop-free KS grammar:\nS → aAb | ε\nA → b | c"
    },
    {
        "question": "Grammar of CS without cycles",
        "answer": "\nA loop-free KS grammar is a KS grammar that does not have the rule: A → +A. That is, nonterminals cannot be replaced by themselves after any number of steps.\nAn example of a loop-free KS grammar:\nS → aAb | ε\nA → b | c"
    },
    {
        "question": "CS grammar without loops",
        "answer": "\nA KS grammar without cycles is a KS grammar that does not have the rule: A → +A. That is, nonterminals cannot be replaced by themselves after a certain number of steps.\nAn example of a KS grammar without cycles:\nS → аАб | ε\nA → б | с"
    },
    {
        "question": "Grammar of CS without cycles",
        "answer": "\nA KS grammar without a loop is a KS grammar that does not have the rule: A → +A. That is, nonterminals cannot be replaced by themselves after a certain number of steps.\nAn example of a KS grammar without a loop:\nS → аАб | ε\nA → б | с"
    },
    {
        "question": "The grammar given by KS is",
        "answer": "\nThe grammar given by KS is a grammar without cycles, without useless symbols and without chain rules."
    },
    {
        "question": "The grammar given by the KS is as follows:",
        "answer": "\nThe grammar given by CS is a grammar without loops, without useless symbols, and without chained rules."
    },
    {
        "question": "The grammar specified by QS is:",
        "answer": "\nThe grammar given by KS is a grammar without loops, without unnecessary symbols and without rules of sequence."
    },
    {
        "question": "The grammar given by KS,",
        "answer": "\nThe grammar given by K.S. is a grammar without cycles, without unnecessary symbols and without chain rules."
    },
    {
        "question": "Algorithm for removing useless (non-generating) symbols in KS grammar",
        "answer": "\n1. Take all the rules that have only terminals in their right-hand side. Take from there\nthe set of all non-terminals on the left.\n2. Take all the rules that have only non-terminals from our\nset in their right-hand side. Add non-terminals on the left to our set. Repeat.\n3. At some step, the set stabilizes (all the rules that are derived will be taken into account). In our\nset, there are terminals and all non-useful non-terminals left.\n4. Take the difference between all non-terminals and non-terminals from our constructed set. These\nwill be useless non-terminals.\n5. Remove all the rules that have useless non-terminals"
    },
    {
        "question": "Algorithm for removing useless (non-generating) symbols in the KS grammar",
        "answer": "\n1. Take all rules that have only terminals on the right side. Take the set of all non-terminals on the left from there.\n2. Take all rules that have only non-terminals from our set on the right side. Add the non-terminals on the left to our set. Repeat.\n3. At some point, the set will stabilize (all the rules that were derived will be taken into account). Our set will contain terminals and all the useless non-terminals.\n4. Take the difference between all the non-terminals and the non-terminals from our constructed set. These will be the useless non-terminals.\n5. Remove all the rules that have useless non-terminals"
    },
    {
        "question": "Algorithm for removing unnecessary (non-generating) symbols in CS grammar",
        "answer": "\n1. Take all rules that have only terminals on the right side. From there, take the set of all nonterminals on the left.\n2. Take all rules that have only nonterminals from our set on the right side. Add the left nonterminals to our set. Repeat.\n3. At some point, everything will stabilize (all derived rules will be taken into account). Our set still contains terminals and all unnecessary nonterminals.\n4. Differentiate all nonterminals and nonterminals in our constructed set. These will be useless nonterminals.\n5. Delete all rules that contain unnecessary nonterminals."
    },
    {
        "question": "Algorithm for removing unnecessary (non-generating) symbols in a CF grammar",
        "answer": "\n1. Take all rules that have terminals only on the right side. Take this from there\nthe set of all nonterminals on the left.\n2. Take all rules that have only nonterminals from our\ntogether on their right side. Add nonterminals on the left to our set. Repeat.\n3. At a certain point, everything will stabilize (all derived rules will be taken into account). In our\ntogether, there are still terminals and all nonterminals that are useless.\n4. Take the difference between all nonterminals and nonterminals in our constructed set. These\nwill be useless nonterminals.\n5. Remove all rules that have unnecessary nonterminals."
    },
    {
        "question": "Algorithm for removing unreachable characters",
        "answer": "\n1. Take a set consisting of S.\n2. Add to our set all nonterminals and terminals that are on the right side\nof the rules that have nonterminals from our set on the left side.\n3. At some step, the set stabilizes (all the rules being derived will be taken into account). Only reachable symbols (terminals and nonterminals) remain in our\nset.\n4. Take the difference between all nonterminals and nonterminals from our constructed set. These\nwill be useless nonterminals. The same with terminals.\n5. Remove all rules that contain useless nonterminals. The same with terminals."
    },
    {
        "question": "Algorithm for removing unavailable characters",
        "answer": "\n1. Take a set consisting of S.\n2. Add to our set all nonterminals and terminals that are to the right of the rules that have nonterminals from our set on the left.\n3. At some step, the set will stabilize (all the rules being output will be taken into account). Only reachable symbols (terminals and nonterminals) will remain in our set.\n4. Take the difference between all nonterminals and nonterminals from our constructed set. These will be useless nonterminals. The same with terminals.\n5. Remove all rules that contain useless nonterminals. The same with terminals."
    },
    {
        "question": "Algorithm for removing unavailable characters",
        "answer": "\n1. Take a set consisting of S.\n2. Add to our set all nonterminals and terminals located to the right of the rules that have nonterminals from our set to the left.\n3. At this point, the whole stabilizes (all derived rules will be taken into account). Only accessible symbols (terminal and nonterminal) will remain in our set.\n4. Consider the difference between all nonterminals and nonterminals in our constructed set. These will be useless nonterminals. The same with terminals.\n5. Remove all rules containing unnecessary nonterminals. The same with terminals."
    },
    {
        "question": "Algorithm for removing unavailable characters",
        "answer": "\n1. Take a set consisting of S.\n2. Add to our set all non-terminals and terminals that are on the right side\nof the rule that have non-terminals of our set on the left side.\n3. At a certain point, everything will stabilize (all derived rules will be taken into account). Only accessible symbols (terminal and non-terminal) remain in our system.\ntogether.\n4. Take the difference between all non-terminals and non-terminals in our constructed set. These\nwill be useless non-terminals. The same with terminals.\n5. Remove all rules containing unnecessary non-terminals. The same with terminals."
    },
    {
        "question": "Algorithm for removing ε-rules",
        "answer": "\n1. Defining ε-nonterminals:\nInitialize the set of ε-nonterminals.\nIf the starting symbol S is an ε-nonterminal, add it to the set of ε-nonterminals.\nFind all nonterminals that can output the empty string (ε). Start with rules of the form A → ε and add A to the set of ε-nonterminals.\nThen, while possible, for each rule B → α, if all symbols in α are ε-nonterminals, add B to the set of ε-nonterminals.\n2. Removing ε-rules:\nRemove all rules of the form A → ε from the grammar.\n3. Create new rules:\nFor each rule of the form A → α, where α has an ε-nonterminal:\nCreate new rules, e.g. if you have a rule A → B C and B is an ε-nonterminal, add a rule A → C.\n4. Repeat the process:\nRepeat steps 1-3 until you have removed all ε-rules."
    },
    {
        "question": "Algorithm for removing ε-rules",
        "answer": "\n1. Defining ε-nonterminals:\nInitialize the set of ε-nonterminals.\nIf the initial symbol S is an ε-nonterminal, add it to the set of ε-nonterminals.\nFind all nonterminals that can output the empty string (ε). Start with rules of the form A → ε and add A to the set of ε-nonterminals.\nThen, while possible, for each rule B → α, if all symbols in α are ε-nonterminals, add B to the set of ε-nonterminals.\n2. Removing ε-rules:\nRemove all rules of the form A → ε from the grammar.\n3. Create new rules:\nFor each rule of the form A → α, where α has an ε-nonterminal:\nCreate new rules, e.g. if you have a rule A → B C, and B is an ε-nonterminal, add a rule A → C.\n4. Repeat the process:\nRepeat steps 1–3 until you have removed all ε-rules."
    },
    {
        "question": "Algorithm for removing ε rules",
        "answer": "\n1. Defining ε-nonterminals:\nInitialize the set of ε-nonterminals.\nIf a source symbol S is an ε-nonterminal, add it to the set of ε-nonterminals.\nFind all nonterminals that can generate the empty string (ε). Start with rules of the form A → ε and add A to the set of ε-nonterminals.\nThen, as far as possible, for each rule B → α, if all symbols α are ε-nonterminals, add B to the set of ε-nonterminals.\n2. Removing ε rules:\nRemove all rules of the form A → ε from the grammar. 3. Create new rules:\nFor each rule of the form A → α, where α has a nonterminal ε:\nCreate new rules, e.g. if you have a rule A → B C and B is an ε-nonterminal, add a rule A → C.\n4. Repeat the process:\nRepeat steps 1–3 until you have removed all ε rules."
    },
    {
        "question": "Algorithm for removing ε rules",
        "answer": "\n1. Defining ε-nonterminals:\nInitialize all ε-nonterminals.\nIf the initial symbol of S is a nonterminal ε, add it to the set of nonterminal εs.\nFind all nonterminals that can yield the empty string (ε). Start with rules of the form A → ε and add A to the set of ε-nonterminals.\nThen, if possible, for each rule B → α, if all symbols in α are ε-nonterminals, add B to the set of ε-nonterminals.\n2. Removing ε-rules:\nRemove all rules of the form A → ε from the grammar. 3. Create new rules:\nFor each rule of the form A → α, where α has a nonterminal ε:\nCreate new rules, e.g. if you have a rule A → B C and B is an ε-nonterminal, add a rule A → C.\n4. Repeat the process:\nRepeat steps 1–3 until you have removed all ε rules."
    },
    {
        "question": "Chain Rule Removal Algorithm",
        "answer": "\nAlgorithm stages:\n1. Search for all chain rules.\n2. For each found chain rule, construct a closure by finding out which terminal symbols can be derived from a given non-terminal.\n3. After completing the closure construction, replace the original chain rules with new ones.\n\nConsider the grammar:\nS → A | b\nA → B | a\nB → c\n1. Search for chain rules:\nFound chain rules: S → A and A → B.\n2. Construction of closure:\nFor S → A:\n- Start with A:\nA → B\nFrom B we get B → c.\nA → a\nWe get S → a.\nAdd S → c and S → a to the set.\n- For A → B:\nWe already have B → c, so we add A → c.\n3. Replacing chain rules:\nAdd new rules:\nS → a\nS → c\nA → c\nRemove the original chain rules.\n4. Final grammar:\nS → b | a | c\nA → a | c\nB → c"
    },
    {
        "question": "Chain Rule Removal Algorithm",
        "answer": "\nAlgorithm steps:\n1. Find all chain rules.\n2. For each found chain rule, construct a closure by finding out which terminal symbols can be obtained from a given nonterminal.\n3. After completing the closure construction, replace the original chain rules with new ones.\n\nConsider the grammar:\nS → A | b\nA → B | a\nB → c\n1. Find chain rules:\nFound chain rules: S → A and A → B.\n2. Construct the closure:\nFor S → A:\n- Start with A:\nA → B\nFrom B we get B → c.\nA → a\nWe get S → a.\nAdd S → c and S → a to the set.\n- For A → B:\nWe already have B → c, so we add A → c.\n3. Replacing chain rules:\nAdding new rules:\nS → a\nS → c\nA → c\nRemoving original chain rules.\n4. Final grammar:\nS → b | a | c\nA → a | c\nB → c"
    },
    {
        "question": "Algorithm for removing chain rules",
        "answer": "\nAlgorithm steps:\n1. Find all channel rules.\n2. For each found chain rule, construct a closure by finding which terminal symbols can be obtained from the given non-terminal.\n3. After closure, replace the original chain rules with new ones.\n\nLet's look at the grammar:\nC → A | b\nA → B | A\nB → c\n1. Find channel rules:\nThe following chain rules were found: S → A and A → B.\n2. Construction of closure:\nFor C → A:\n- Let's start with A:\nA → B\nFrom B we get B → c.\nA → a\nWe get S → a.\nAdd to the set S → c and S → a.\n- For A → B:\nWe already have B → c, so we add A → c.\n3. Channel Rules Override:\nAdded new rules:\nC → a\nC → c\nA → c\nRemoved original channel rules.\n4. Final Grammar:\nC → b | a | c\nA → a | c\nB → c"
    },
    {
        "question": "Algorithm for removing chain rules",
        "answer": "\nAlgorithm steps:\n1. Find all channel rules.\n2. For each found chain rule, construct a closure by determining which terminal symbols can be obtained from a given non-terminal.\n3. After completing the construction of the closure, replace the original chain rules with new ones.\n\nConsider the grammar:\nC → A | b\nA → B | has\nB → c\n1. Find channel rules:\nFound chain rules: S → A and A → B.\n2. Construction of the overlap:\nFor C → A:\n- Start with A:\nA → B\nFrom B we get B → c.\nA → a\nWe get S → a.\nAdd to the set S → c and S → a.\n- For A → B:\nWe already have B → c, so we add A → c.\n3. Overriding channel rules:\nAdd new rules:\nC → a\nC → c\nA → c\nDelete original channel rules.\n4. Final grammar:\nC → b | a | c\nA → a | c\nB → c"
    },
    {
        "question": "What is Lukasiewicz's language?",
        "answer": "\nA Lukasiewicz language over n + 1 letters is a context-free\nlanguage over the alphabet {a0, a1,...,an} generated by the grammar\nS -> a0, S -> a1S, S -> a2SS, ..., S -> anS^n\nNote: For any n ∈ N the grammar is unambiguous."
    },
    {
        "question": "What language does Lukasiewicz speak?",
        "answer": "\nLukasiewicz language over n + 1 letters is context-free\nlanguage over alphabet {a0, a1,...,an} generated by grammar\nS -> a0, S -> a1S, S -> a2SS, ..., S -> anS^n\nNote: for any n ∈ N the grammar is unambiguous."
    },
    {
        "question": "What language does Lukasiewicz speak?",
        "answer": "\nThe language of Lukasiewicz on n + 1 letters is context-free.\nlanguage on the alphabet {a0, a1,...,an} generated by the grammar\nS -> a0, S -> a1S, S -> a2SS, ..., S -> anS^n\nNote: for all n ∈ N the grammar is unambiguous."
    },
    {
        "question": "What language does Lukasiewicz speak?",
        "answer": "\nLukasiewicz language with n+1 letters is a language\nwithout context in the alphabet {a0, a1,...,an} generated by the grammar\nS -> a0, S -> a1S, S -> a2SS, ..., S -> anS^n\nNote: for all n ∈ N the grammar is unambiguous."
    },
    {
        "question": "What is Dick's language",
        "answer": "\nA Dik language over 2n\nletters is a context-free language over the alphabet\n{a1, b1, a2, b2,...,an, bn} generated by the grammar S → ε,\nS → a1Sb1S, ..., S → anSbnS.\nNote: The words of this language are sequences of correctly nested parentheses of n types.\nNote: For any n ∈ N the grammar is unambiguous."
    },
    {
        "question": "What language does Dick speak?",
        "answer": "\nThe language Dik over 2n\nletters is a context-free language over the alphabet\n{a1, b1, a2, b2,...,an, bn} generated by the grammar S → ε,\nS → a1Sb1S, ..., S → anSbnS.\nNote: The words of this language are sequences of regularly nested parentheses of n types.\nNote: For any n ∈ N the grammar is unambiguous."
    },
    {
        "question": "What language does Dick speak?",
        "answer": "\nA member with a 2n-letter language is a context-free language in the alphabet\n{a1, b1, a2, b2,...,an, bn} generated by the grammar S → ε,\nS → a1Sb1S, ..., S → anSbnS.\nNote: Words in this language are sequences of regularly nested parentheses of n types.\nNote: For all n ∈ N, the grammar is unambiguous."
    },
    {
        "question": "What is Dick's language?",
        "answer": "\nA wild language with 2n letters is a language without context in the alphabet\n{a1, b1, a2, b2,...,an, bn}, generated by the grammar S → ε,\nS → a1Sb1S, ..., S → anSbnS.\nNote: Words in this language are sequences of regularly nested parentheses of n types.\nNote: for all n ∈ N the grammar is unambiguous."
    },
    {
        "question": "What is a Mealy machine?",
        "answer": "\nA Mealy machine is a 6-tuple (Q, Σ, Λ, δ, λ, q₀), where:\nQ is the set of states of the machine\nΣ is the finite input alphabet\nΛ is the finite output alphabet\nδ is the transition function that takes the current state and an input symbol and returns the next state\nλ is the output function that takes the current state and an input symbol and returns the output symbol\nq₀ is the initial state"
    },
    {
        "question": "What is a Miles machine?",
        "answer": "\nA Mealy automaton is a 6-tuple (Q, Σ, Λ, δ, λ, q₀), where:\nQ is the set of states of the automaton\nΣ is a finite input alphabet\nΛ is a finite output alphabet\nδ is a transition function that takes the current state and an input symbol and returns the next state\nλ is an output function that takes the current state and an input symbol and returns an output symbol\nq₀ is the initial state"
    },
    {
        "question": "What is a Miles machine?",
        "answer": "\nA Mealy machine is a set of 6 elements (Q, Σ, Λ, δ, λ, q₀), where:\nQ is the set of states of the machine\nΣ is the final input alphabet\nΛ is the final output alphabet\nδ is the transition function that takes the current state and an input symbol and returns the next state.\nλ is the output function that takes the current state and an input symbol and returns the output symbol.\nq₀ is the initial state"
    },
    {
        "question": "What is a Miles machine?",
        "answer": "\nA Mealy machine is a set of 6 elements (Q, Σ, Λ, δ, λ, q₀), where:\nQ is the set of states of the machine\nΣ is the finite input alphabet\nΛ is the finite output alphabet\nδ is the transition function that takes the current state and an input symbol and returns the next state.\nλ is the output function that takes the current state and an input symbol and returns the output symbol.\nq₀ is the initial state"
    },
    {
        "question": "What are the properties of the class of deterministic context-free languages?",
        "answer": "\nTheorem: Every automaton language is a deterministic context-free language.\nTheorem: A language L ⊆ Σ∗ is a deterministic context-free language if and only if\nthere exists a deterministic MP-automaton M' = <Q', Σ, Γ', ∆', I', F'> such that\nL={w∈Σ∗ | <s,w,ε> |*-- <q,ε,α> for some s∈I', q∈F', α∈Γ'∗}.\nTheorem: Let L be a deterministic context-free language. Then L is not essentially\nambiguous.\nTheorem: The complement of every deterministic context-free language is a deterministic context-free language.\nExample: The language L = {a^kb^mc^n | k != m or m != n}\nover the alphabet {a, b, c} is not a deterministic context-free language, since its complement is not\ncontext-free.\nTheorem: It is not true that for any deterministic context-free languages ​​L1 and L2, the language L1 ∩ L2\nis also a deterministic context-free\nlanguage.\nTheorem: It is not true that for any deterministic context-free languages ​​L1 and L2, the language L1 ∪ L2\nis also a deterministic context-free language."
    },
    {
        "question": "What are the properties of the class of deterministic context-free languages?",
        "answer": "\nTheorem: Every automaton language is a deterministic context-free language.\nTheorem: A language L ⊆ Σ∗ is a deterministic context-free language if and only if\nthere exists a deterministic MP-automaton M' = <Q', Σ, Γ', ∆', I', F'> such that\nL={w∈Σ∗ | <s,w,ε> |*-- <q,ε,α> for some s∈I', q∈F', α∈Γ'∗}.\nTheorem: Let L be a deterministic context-free language. Then L is not essentially ambiguous.\nTheorem: The complement of every deterministic context-free language is a deterministic context-free language.\nExample: Language L = {a^kb^mc^n | k != m or m != n}\nover an alphabet {a, b, c} is not a deterministic context-free language, since its complement is not\ncontext-free.\nTheorem: It is not true that for any deterministic context-free languages ​​L1 and L2, the language L1 ∩ L2\nis also a deterministic context-free\nlanguage.\nTheorem: It is not true that for any deterministic context-free languages ​​L1 and L2, the language L1 ∪ L2\nis also a deterministic context-free\nlanguage."
    },
    {
        "question": "What are the properties of the class of context-free deterministic languages?",
        "answer": "\nTheorem: Every automaton language is a context-free deterministic language.\nTheorem: A language L ⊆ Σ∗ is a context-free deterministic language if and only if\nthere exists a deterministic MP-automaton M' = <Q', Σ, Γ', ∆', I', F'> such that\nL={wεΣ∗ | <s,w,ε> |*-- <q,ε,α> for some sεI', qεF', αεΓ'∗}.\nTheorem: Let L be a deterministic language out of context. Then L is not essentially ambiguous.\nTheorem: The complement of every context-free deterministic language is a context-free deterministic language.\nExample: Language L = {a^kb^mc^n | k != m or m != n}\nin the alphabet {a, b, c} is not a context-free deterministic language, since its complement is not a context-free\nlanguage.\nTheorem: It is not true that for any context-free deterministic language L1 and L2, the language L1 ∩ L2\nis also a context-free deterministic\nlanguage.\nTheorem: It is not true that for any context-free deterministic language L1 and L2, the language L1 ∪ L2\nis also a context-free deterministic\nlanguage."
    },
    {
        "question": "What are the properties of the class of deterministic context-free languages?",
        "answer": "\nTheorem: Every automaton language is a deterministic context-free language.\nTheorem: A language L ⊆ Σ∗ is a deterministic context-free language if and only if\nthere exists a deterministic MP automaton M' = <Q', Σ, Γ', ∆', I', F'> such that\nL={wεΣ∗ | <s,w,ε> |*-- <q,ε,α> for some sεI', qεF', αεΓ'∗}.\nTheorem: Let L be a deterministic context-free language. Then L is not essentially\nambiguous.\nTheorem: The complement of every deterministic context-free language is a deterministic context-free language.\nExample: A language L = {a^kb^mc^n | k != m or m != n}\nover alphabet {a, b, c} is not a context-free deterministic language, since its complement is not\ncontext-free.\nTheorem: It is not true that for any context-free deterministic language L1 and L2, the language L1 ∩ L2\nor also a context-free deterministic language.\nTheorem: It is not true that for any context-free deterministic language L1 and L2, the language L1 ∪ L2\nor also a context-free deterministic language."
    },
    {
        "question": "What is aflavite homomorphism",
        "answer": "\nLet Σ1 and Σ2 be alphabets. If a mapping h: Σ1∗ → Σ2∗ satisfies the condition h(x y) = h(x) h(y)\nfor all words x ∈ Σ1∗ and y ∈ Σ1∗, then h is called a\nhomomorphism (morphism).\nRemark: If h is a homomorphism, then h(ε) = ε.\nExample: Let Σ1 = {a, b} and Σ2 = {c}. Then\nthe mapping h: Σ1∗ → Σ2∗ given by h(w) = c^(2|w|) is a homomorphism.\nRemark: Each homomorphism is uniquely determined by its values ​​on single-letter words.\nDefinition: If h: Σ1∗ → Σ2∗ is a homomorphism and L ⊆ Σ1∗, then h(L) denotes the language {h(w) | w ∈ L}.\nExample: Let Σ = {a, b} and let h: Σ∗ → Σ∗ be a homomorphism defined by h(a) = abba and h(b) = ε. Then\nh({baa, bb}) = {abbaabba, ε}.\nDefinition: If h: Σ1∗ → Σ2∗ is a homomorphism and L ⊆ Σ2∗, then h^(−1)(L) denotes the language {w ∈ Σ1∗ | h(w) ∈ L}.\nExample: Consider the alphabet Σ = {a, b}. Let the homomorphism h: Σ∗ → Σ∗ be given by the equalities h(a) = ab and h(b) = abb.\nThen h^(−1)({ε, abbb, abbab, ababab}) = {ε, ba, aaa}."
    },
    {
        "question": "What is aflavite homomorphism?",
        "answer": "\nLet Σ1 and Σ2 be alphabets. If a mapping h: Σ1∗ → Σ2∗ satisfies the condition h(x y) = h(x) h(y)\nfor all words x ∈ Σ1∗ and y ∈ Σ1∗, then h is called a\nhomomorphism (morphism).\nRemark: If h is a homomorphism, then h(ε) = ε.\nExample: Let Σ1 = {a, b} and Σ2 = {c}. Then\nthe mapping h: Σ1∗ → Σ2∗ given by h(w) = c^(2|w|) is a homomorphism.\nRemark: Each homomorphism is uniquely determined by its values ​​on single-letter words.\nDefinition: If h: Σ1∗ → Σ2∗ is a homomorphism and L ⊆ Σ1∗, then h(L) denotes the language {h(w) | w ∈ L}.\nExample: Let Σ = {a, b} and let h: Σ∗ → Σ∗ be the homomorphism defined by h(a) = abba and h(b) = ε. Then\nh({baa, bb}) = {abbaabba, ε}.\nDefinition: If h: Σ1∗ → Σ2∗ is a homomorphism and L ⊆ Σ2∗, then h^(−1)(L) denotes the language {w ∈ Σ1∗ | h(w) ∈ L}.\nExample: Consider the alphabet Σ = {a, b}. Let the homomorphism h: Σ∗ → Σ∗ be given by the equalities h(a) = ab and h(b) = abb. Then h^(−1)({ε, abbb, abbab, ababab}) = {ε, ba, aaa}."
    },
    {
        "question": "What is aflavite homomorphism?",
        "answer": "\nLet Σ1 and Σ2 be alphabets. If an application h: Σ1∗ → Σ2∗ satisfies the condition h(x y) = h(x) h(y)\nfor all words x ∈ Σ1∗ and y ∈ Σ1∗ then h is called a\nhomomorphism (morphism).\nNote. If h is a homomorphism, then h(ε) = ε.\nExample: Let Σ1 = {a, b} and Σ2 = {c}. SO\nthe mapping h : Σ1∗ → Σ2∗ given by h(w) = c^(2|w|) is a homomorphism.\nNote. Each homomorphism is uniquely determined by its value in single-letter words.\nDefinition: If h : Σ1∗ → Σ2∗ is a homomorphism and L ⊆ Σ1∗, then h(L) denotes the language {h(w) | w ∈ L}.\nExample: Let Σ = {a, b} and let h: Σ∗ → Σ∗ be the homomorphism defined by h(a) = abba and h(b) = ε. SO\nh({baa, bb}) = {abbaabba, ε}.\nDefinition: If h : Σ1∗ → Σ2∗ is a homomorphism and L ⊆ Σ2∗, then h^(−1)(L) denotes the language {w ∈ Σ1∗ | h(w) ∈L}.\nExample. Consider an alphabet Σ = {a, b}. Let the homomorphism h: Σ∗ → Σ∗ be defined by the equalities h(a) = ab and h(b) = abb. Then h^(−1)({ε, abbb, abbab, abab}) = {ε, ba, aaa}."
    },
    {
        "question": "What is aflavite homomorphism?",
        "answer": "\nLet Σ1 and Σ2 be alphabets. If a mapping h: Σ1∗ → Σ2∗ satisfies the condition h(x y) = h(x) h(y)\nfor all words x ∈ Σ1∗ and y ∈ Σ1∗ then h is called a\nhomomorphism (morphism).\nNote. If h is a homomorphism, then h(ε) = ε.\nExample: Let Σ1 = {a, b} and Σ2 = {c}. SO\nthe mapping h : Σ1∗ → Σ2∗ given by h(w) = c^(2|w|) is a homomorphism.\nNote. Each homomorphism is uniquely determined by its values ​​in single-letter words.\nDefinition: If h : Σ1∗ → Σ2∗ is a homomorphism and L ⊆ Σ1∗, then h(L) denotes the language {h(w) | w ∈ L}.\nExample: Let Σ = {a, b} and let h: Σ∗ → Σ∗ be the homomorphism defined by h(a) = abba and h(b) = ε. SO\nh({baa, bb}) = {abbaabba, ε}.\nDefinition: If h : Σ1∗ → Σ2∗ is a homomorphism and L ⊆ Σ2∗, then h^(−1)(L) denotes the language {w ∈ Σ1∗ | h(w) ∈ L}.\nExample. Consider the alphabet Σ = {a, b}. Let the homomorphism h: Σ∗ → Σ∗ be defined by the equalities h(a) = ab and h(b) = abb.\nThen h^(−1)({ε, abbb, abbab, abab}) = {ε, ba, aaa}."
    },
    {
        "question": "What is context-free language algebra?",
        "answer": "\nAlgebraic properties of context-free languages:\nIf L is a context-free language, then\nL∗ is also a context-free language.\nIf L1 and L2 are context-free languages ​​over the alphabet Σ, then L1 · L2 is also a context-free\nlanguage.\nIf L1 and L2 are context-free languages ​​over the alphabet Σ, then L1 ∪ L2 is also a context-free\nlanguage.\nIf L is a context-free language, then\nL^R is also a context-free language.\nIf L is a context-free language and R is a regular language, then L ∩ R is a context-free language.\nThe complement of a context-free language is not a context-free language in general."
    },
    {
        "question": "What is context-free language algebra?",
        "answer": "\nAlgebraic properties of context-free languages:\nIf L is a context-free language, then\nL∗ is also a context-free language.\nIf L1 and L2 are context-free languages ​​over an alphabet Σ, then L1 L2 is also a context-free\nlanguage.\nIf L1 and L2 are context-free languages ​​over an alphabet Σ, then L1 ∪ L2 is also a context-free\nlanguage.\nIf L is a context-free language, then\nL^R is also a context-free language.\nIf L is a context-free language and R is a regular language, then L ∩ R is a context-free language.\nThe complement of a context-free language is not a context-free language in general."
    },
    {
        "question": "What is context-free language algebra?",
        "answer": "\nAlgebraic properties of context-free languages:\nIf L is a context-free language, then\nL∗ is also a context-free language.\nIf L1 and L2 are context-free languages ​​on the alphabet Σ, then L1 L2 is also a context-free.\nlanguage.\nIf L1 and L2 are context-free languages ​​on the alphabet Σ, then L1 ∪ L2 are also context-free.\nlanguage.\nIf L is a context-free language, then\nL^R is also a context-free language.\nIf L is a context-free language and R is a regular language, then L ∩ R is a context-free language.\nThe complement of a context-free language is usually not a context-free language."
    },
    {
        "question": "What is context-free language algebra?",
        "answer": "\nAlgebraic properties of context-free languages:\nIf L is a context-free language, then\nL∗ is also a context-free language.\nIf L1 and L2 are context-free languages ​​on the alphabet Σ, then L1 L2 is also a context-free language.\nIf L1 and L2 are context-free languages ​​on the alphabet Σ, then L1 ∪ L2 is also a context-free language.\nIf L is a context-free language, then\nL^R is also a context-free language.\nIf L is a context-free language and R is a regular language, then L ∩ R is a context-free language.\nThe complement of a context-free language is not a context-free language at all."
    },
    {
        "question": "What are derivation trees in formal grammar",
        "answer": "\nIn a context-free grammar, the outputs correspond to the so-called output trees (or parse trees) — some ordered trees whose vertices are labeled with symbols from the alphabet N ∪ Σ. \nSymbols from the terminal alphabet label the leaves of the tree. All vertices labeled with nonterminals have children. The root of the tree corresponds to the initial symbol. Each symbol of the word w1, \nwhich the initial symbol is replaced with at the first step of the output, is assigned a vertex of the tree, and an arc is drawn to it from the root. The immediate descendants of the root obtained in this way are ordered according to the order of their labels in the word w1. For those of the obtained vertices that are labeled with symbols from the set N, a similar construction is made, etc. \nThe crown of the output tree is the word written in the vertices labeled with symbols from the alphabet Σ. \n\nThe leaves of the tree are the final symbols that we obtain as a result of successive \napplication of the rules.\n\nExample: Consider the context-free grammar\nS -> SS, S -> ab, S -> aSb. The output S -> SS -> Sab -> SSab -> abSab -> ababab corresponds to the following output tree:\nS\n/ \\\nS S\n/ \\ |\\\nS S a b\n/| |\\\na b a b"
    },
    {
        "question": "What are derivation trees in formal grammar?",
        "answer": "\nIn a context-free grammar, outputs correspond to so-called output trees (or parse trees) — some ordered trees whose vertices are labeled with symbols from the alphabet N ∪ Σ. \nSymbols from the terminal alphabet label the leaves of the tree. All vertices labeled with nonterminals have descendants. The root of the tree corresponds to the initial symbol. Each symbol of the word w1, which replaces the initial symbol at the first step of the output, is assigned a vertex of the tree, and an arc is drawn from the root to it. The immediate descendants of the root obtained in this way are ordered according to the order of their labels in the word w1. For those of the obtained vertices that are labeled with symbols from the set N, a similar construction is performed, etc. \nThe crown of the output tree is a word written in the vertices labeled with symbols from the alphabet Σ. \n\nThe leaves of the tree are the final symbols that we obtain as a result of successive application of the rules.\n\nExample: Consider the context-free grammar\nS -> SS, S -> ab, S -> aSb. The output S -> SS -> Sab -> SSab -> abSab -> ababab corresponds to the following output tree:\nS\n/ \\\nS S\n/ \\ |\\\nS S a b\n/| |\\\na b a b"
    },
    {
        "question": "What are derivation trees in formal grammar?",
        "answer": "\nIn a context-free grammar, outputs correspond to so-called output trees (or parse trees) — ordered trees whose vertices are labeled with symbols of the alphabet N ∪ Σ. \nSymbols of the terminal alphabet denote the leaves of the tree. All vertices labeled as non-terminal have children. The root of the tree corresponds to the initial character. Each symbol in the word w1, which is used to replace the original symbol at the first stage of output, is assigned a tree vertex, and an arc is drawn from the root to it. The resulting immediate children of the root are ordered according to the order of their labels in the word w1. For those of the resulting vertices that are labeled with symbols from the set N, a similar construction is made, etc. \nThe crown of the output shaft is the word written in the vertices labeled with symbols of the alphabet Σ. \n\nThe leaves of the tree are the final symbols that we obtain as a result of successive application of the rules. \n\nExample. Consider a context-free grammar.\nS -> SS, S -> ab, S -> aSb. The output S -> SS -> Sab -> SSab -> abSab -> ababab corresponds to the following output tree:\nС\n/\\\nSS\n/\\|\\\nС С а б\n/| |\\\nа б а б"
    },
    {
        "question": "What are derivation trees in formal grammar?",
        "answer": "\nIn a context-free grammar, the output data correspond to so-called output trees (or parse trees) — ordered trees whose vertices are labeled with symbols of the alphabet N ∪ Σ. \nThe symbols of the terminal alphabet denote the leaves of the tree. All vertices labeled with nonterminals have children. The root of the tree corresponds to the original symbol. Each symbol of the word w1, \nwhich replaces the original symbol at the first step of output, is assigned a vertex of the tree and an arc is drawn from the root to it. The immediate descendants of the root obtained in this way are classified by the order of their labels in the word w1. For those of the obtained vertices that are labeled with symbols from the set N, a similar construction is carried out, etc. \nThe crown of the output shaft is the word written in the vertices labeled with symbols of the alphabet Σ. \n\nThe leaves of the tree are the final symbols that we obtain as a result of applying \nsequential rules. \n\nExample. Consider the grammar without context.\nS -> SS, S -> ab, S -> aSb. The output S -> SS -> Sab -> SSab -> abSab -> ababab corresponds to the following output tree:\nС\n/ \\\nSS\n/ \\ |\\\nС С а б\n/| |\\\nа б а б"
    },
    {
        "question": "What are non-contracting context-sensitive grammars?",
        "answer": "\nA context-sensitive grammar (a context-dependent grammar, a grammar of immediate\nconstituents, a NS-grammar, a grammar of type 1)\nis a generative grammar in which each rule has the form\nηAθ → ηαθ, where A ∈ N, η ∈ (N ∪ Σ)^∗, θ ∈ (N ∪ Σ)^∗, α ∈ (N ∪ Σ)^+.\nDefinition: A generative grammar is called\nnon-contracting if for each rule (α -> β) ∈ P\nthe inequality |α| <= |β| holds.\nTheorem: There is an algorithm that, given a non-contracting grammar G and a word w, finds whether w ∈ L(G).\nTheorem: Every contextual grammar is non-contracting. Every non-contracting grammar is equivalent to some contextual grammar.\nExample: The grammar S -> AST A, S -> AbA, A -> a,\nbT -> bb, AT -> TA is equivalent to the contextual grammar S -> ASTA, S -> AbA, A -> a,\nbT -> bb, AT -> UT, UT -> UV, UV -> TV, TV -> TA."
    },
    {
        "question": "What are non-contractual context-sensitive grammars?",
        "answer": "\nA context-sensitive grammar (context-sensitive grammar, immediate constituent grammar, NS grammar, type 1 grammar)\nis a generative grammar in which each rule has the form\nηAθ → ηαθ, where A ∈ N, η ∈ (N ∪ Σ)^∗, θ ∈ (N ∪ Σ)^∗, α ∈ (N ∪ Σ)^+.\nDefinition: A generative grammar is called\nnon-reducing if for each rule (α -> β) ∈ P\nthe inequality |α| <= |β| holds.\nTheorem: There is an algorithm that, given a non-reducing grammar G and a word w, finds whether w ∈ L(G).\nTheorem: Every contextual grammar is non-contractual. Every non-contractual grammar is equivalent to some contextual grammar.\nExample: The grammar S -> AST A, S -> AbA, A -> a,\nbT -> bb, AT -> TA is equivalent to the contextual grammar S -> ASTA, S -> AbA, A -> a,\nbT -> bb, AT -> UT, UT -> UV, UV -> TV, TV -> TA."
    },
    {
        "question": "What are non-contractual contextual grammars?",
        "answer": "\nContextual grammar (contextual grammar, immediate grammar\ncomponents, NS-grammar, grammar of the 1st type)\nis a generative grammar in which each rule has the form\nηAθ → ηαθ, where A ∈ N, η ∈ (N ∪ Σ)^∗, θ ∈ (N ∪ Σ)^∗, α ∈ (N ∪ Σ)^+.\nDefinition: a generative grammar is called\nnon-cancellation if for each rule (α -> β) ∈ P\nthe inequality |α| <= |β|.\nTheorem: There exists an algorithm that, given a non-reductive grammar G and a word w, determines whether w ∈ L(G).\nTheorem: All contextual grammars are non-contractual. All non-contractual grammars are equivalent to contextual grammars.\nExample: The grammar S -> AST A, S -> AbA, A -> a,\nbT -> bb, AT -> TA is equivalent to the contextual grammar S -> ASTA, S -> AbA, A -> a,\nbT -> bb, AT -> UT, UT -> UP, UP -> TV, TV -> TA."
    },
    {
        "question": "What are non-contractual contextual grammars?",
        "answer": "\nContextual grammar (grammar dependent on context, grammar\nimmediate constituents, NS-grammar, type 1 grammar)\nis a generative grammar in which each rule has the form\nηAθ → ηαθ, where A ∈ N, η ∈ (N ∪ Σ)^∗, θ ∈ (N ∪ Σ)^∗, α ∈ (N ∪ Σ)^+.\nDefinition: a generative grammar is called\nnon-compressive if for each rule (α -> β) ∈ P\nthe inequality |α| <= |β| is verified.\nTheorem: There exists an algorithm that, given a non-compressive grammar G and a word w, determines whether w ∈ L(G).\nTheorem: All contextual grammars are non-contractual. Any non-contractual grammar is equivalent to a contextual grammar.\nExample: The grammar S -> AST A, S -> AbA, A -> a,\nbT -> bb, AT -> TA is equivalent to the contextual grammar S -> ASTA, S -> AbA, A -> a,\nbT -> bb, AT -> UT, UT -> UV, UV -> TV, TV -> TA."
    },
    {
        "question": "List the properties of regular expressions",
        "answer": "\nRegular expressions form an associative semiring with operations (0, +, 1, ), i.e. for any\nregular expressions e, f, and g the following identities hold:\nNote: In these identities, + is used to denote union.\n1. e+f = f+e,\n2. e+0 = e,\n3. (e+f)+g = e+(f+g),\n4. e·1 = e,\n5. 1·e = e,\n6. (e·f)·g = e·(f·g),\n7. e·(f+g) = e·f+e·g,\n8. (f+g)·e = f·e+g·e,\n9. e·0=0,\n10. 0·e = 0.\nEquality is understood as equality of the languages ​​defined by\nregular expressions.\nFor any regular expressions e and f,\nthe following identities hold:\n1. e+e = e,\n2. (1+e+ee+ ... +e^(n−1))(e^n)∗ = e∗ for any n >= 1,\n3. (e∗f)∗e∗ = (e+f)∗,\n4. 1+e(fe)∗f = (ef)∗.\nLemma: For any regular expressions e, f, and g,\nif e = ef+g and ε !∈ L(f), then e = gf∗. (or e = ef|g )"
    },
    {
        "question": "List the properties of regular expressions.",
        "answer": "\nRegular expressions form an associative semiring with operations (0, +, 1, ), i.e. for any\nregular expressions e, f, and g the following identities hold:\nNote: in these identities + is used to denote union.\n1. e+f = f+e,\n2. e+0 = e,\n3. (e+f)+g = e+(f+g),\n4. e 1 = e,\n5. 1 e = e,\n6. (e f) g = e (f g),\n7. e (f+g) = e f+e g,\n8. (f+g) e = f e+g e,\n9. e 0=0,\n10. 0 e = 0.\nEquality is understood as equality of the languages ​​defined by\nregular expressions.\nFor any regular expressions e and f,\nthe following identities hold:\n1. e+e = e,\n2. (1+e+ee+ ... +e^(n−1))(e^n)∗ = e∗ for any n >= 1,\n3. (e∗f)∗e∗ = (e+f)∗,\n4. 1+e(fe)∗f = (ef)∗.\nLemma: For any regular expressions e, f, and g,\nif e = ef+g and ε !∈ L(f), then e = gf∗. (or e = ef|g )"
    },
    {
        "question": "List the properties of regular expressions.",
        "answer": "\nRegular expressions form an associative semiring with operations (0, +, 1, ), i.e. for all\nregular expressions e, f, and g satisfy the following identities:\nNote: In these identifiers, + is used to denote union.\n1. e + e = e + e,\n2. e + 0 = e,\n3. (e + e) ​​+ g = e + (e + g),\n4. e 1 = e,\n5. 1 e = e,\n6. (eff) g = e (eff g),\n7. e (f + g) = e f + e g,\n8. (f + g) e = fe + g,\n9. e 0 = 0,\n10. 0 e = 0.\nEquality refers to equality of certain languages.\nregular expressions.\nFor all regular expressions e and f,\nthe following identities hold:\n1. e + e = e,\n2. (1 + e + ee + ... + e^(n−1))(e^n)∗ = e∗ for all n >= 1,\n3. (e∗f)∗e∗ = (e+f)∗,\n4. 1 + e(fe)∗f = (ef)∗.\nLemma: for any regular expression e, f, and g,\nif e = ef+g and ε !ε L(f), then e = gf∗. (or e = ef|g)"
    },
    {
        "question": "List of regular expression properties",
        "answer": "\nRegular expressions form an associative semiring with operations (0, +, 1, ), i.e. for any\nregular expression e, f, and g, the following identities hold:\nNote: In these identities, + is used to denote union.\n1. e + e = e + e,\n2. e + 0 = e,\n3. (e + e) ​​+ g = e + (e + g),\n4. e 1 = e,\n5. 1 e = e,\n6. (e f) g = e (f g),\n7. e (f + g) = e f + e g,\n8. (f + g) e = f e + g e,\n9. e 0 = 0,\n10. 0 e = 0.\nEquality is understood as equality of languages ​​defined by\nregular expressions.\nFor all regular expressions e and f,\nthe following identities hold:\n1. e + e = e,\n2. (1 + e + ee + ... + e^(n−1))(e^n)∗ = e∗ for all n >= 1,\n3. (e∗f)∗e∗ = (e+f)∗,\n4. 1+e(fe)∗f = (ef)∗.\nLemma: for all regular expressions e, f, and g,\nif e = ef+g and ε !ε L(f), then e = gf∗. (or e = ef|g)"
    },
    {
        "question": "What is the translation of a finite state machine into a regular expression, give an example of such a translation",
        "answer": "\nThis is the process of transforming a finite automaton into a regular expression,\nwhich describes the same language as the automaton.\n\nA regular expression over an alphabet Σ={c1,c2,…,ck} is a way of generating a language over Σ.\nIt is defined recursively as follows:\nFor any i, the word ci is a regular expression defining the language of the single word ci.\n\nε is a regular expression defining the language of the single empty string, and ∅ is the empty language.\n\nIf α1 and α2 are regular expressions defining languages ​​L1 and L2, respectively,\nthen (α1)|(α2) is a regular expression defining L1 ⋃ L2.\n\nIf α1 and α2 are regular expressions defining languages ​​L1 and L2 respectively,\nthen (α1)(α2) is a regular expression defining L1L2.\n\nIf α1 is a regular expression defining language L1,\nthen (α1)^∗ is a regular expression defining L1∗.\n\nThe operations are listed in ascending order of precedence, with parentheses increasing precedence similar to arithmetic expressions.\n\nExample: Let Σ = {a, b}. Then (((ab)^∗)(1+a)) is a\nregular expression over alphabet Σ.\n\nDefinition: A recognizing automaton (or simply a finite automaton) is a quintuplet A = {Q, Σ, δ, F, q0}, in which:\nQ is a finite set of states;\nΣ is a finite alphabet;\nδ : Q × Σ → Q is the transition function;\nF ⊆ Q is the set of accepting (or terminal) states;\nq0 ∈ Q is the initial state.\n\nDefinition: A deterministic finite automaton (DFA) is a finite automaton in which for each state and each symbol of the input alphabet there is exactly one transition.\n\nOne of the ways to transform a finite automaton into a regular expression is the Brzozowski algebraic method (a method for transforming deterministic finite automata into regular expressions).\n\nDescription of the method:\nWhen transforming a DFA into a regular expression, a system of regular expressions is created for each state in the DFA, and then it is solved for regular expressions Ri associated with the terminal states qi. The equation is constructed as follows: for each state qi, the equation Ri\nis the union of transitions leading to this state. The transition a from qi to qj is denoted by aRi. If qi\nis a terminal state, then ε is added to Ri. This leads to a system of equations of the form:\n\nNote: + is equivalent to | in a regular expression\n\n⎧R1=a1R1+a2∗R2+a3R3+... \n⎪R2=a1R1+a2∗R2+a3R3+...+ε (R2 is a terminal state)\n...\n⎩Rm=a1R1+a2R2+a3R3+...\n\nwhere ak = ∅ if there is no transition from Ri to Rj. The system can be solved by simple substitution, \nexcept when the unknown appears on both the right and left sides of the equation. \nThis solves an equation of the form R=Q+RP, where P!=ε, has the solution R=QP^∗. ( + is equivalent to | )\n\nUse example: \nProblem: You need to construct a regular expression that satisfies a DFA.\nQ={q1,q2,q3,q4}\nX={a,b}\nF={q1,q4}\nδ = {<q1, b> -> q3; <q1, a> -> q2; <q3, a> -> q1; <q3, b> -> q4; <q2, a> -> q4;\n<q2, b> -> q1; <q4, b> -> q4; <q4,a> -> q4}\n\nSolution using Brzozowski's method:\n\n⎧R1=bR2+aR3+ε\n⎪R2=aR1\n⎨R3=bR1\n⎩R4=aR2+bR3+aR4+bR4+ε\n\nLet's consider the first terminal state.\n\nR1=ε+abR1+baR1=ε+R1(ab+ba)\n\nWe use an equation of the form R=Q+RP.\n\nR1=(ab+ba)^∗\n\nLet's consider the second terminal state.\n\nR4=R1(aa+bb)+R4(a+b)= R1(aa+bb)(a+b)^∗ = (ab+ba)∗(aa+bb)(a+b)^∗\n\nLet's combine the expressions for the terminal states and obtain the desired regular expression:\n\nR=R1+R4=(ab+ba)^∗(ε+(aa+bb)(a+b)^∗).\nOr R=(ab|ba)^∗(ε|(aa|bb)(a|b)^∗)."
    },
    {
        "question": "What is the translation of a finite automaton into a regular expression, give an example of such a translation.",
        "answer": "\nThis is the process of converting a finite automaton into a regular expression\nthat describes the same language as the automaton.\n\nA regular expression over an alphabet Σ={c1,c2,…,ck} is a way of generating a language over Σ.\n\nIt is defined recursively as follows:\nFor any i, a word ci is a regular expression that defines the language of a single word ci.\n\nε is a regular expression that defines the language of a single empty string, and ∅ is the empty language.\n\nIf α1 and α2 are regular expressions that define languages ​​L1 and L2, respectively,\nthen (α1)|(α2) is a regular expression that defines L1 ⋃ L2.\n\nIf α1 and α2 are regular expressions defining languages ​​L1 and L2 respectively,\nthen (α1)(α2) is a regular expression defining L1L2.\n\nIf α1 is a regular expression defining language L1,\nthen (α1)^∗ is a regular expression defining L1∗.\n\nThe operations are listed in order of increasing precedence, with parentheses increasing precedence, similar to arithmetic expressions.\n\nExample: Let Σ = {a, b}. Then (((ab)^∗)(1+a)) is a regular expression over the alphabet Σ.\n\nDefinition: A recognizing automaton (or simply a finite automaton) is a quintuplet A = {Q, Σ, δ, F, q0}, in which:\nQ is a finite set of states;\nΣ is a finite alphabet;\nδ : Q × Σ → Q is the transition function;\nF ⊆ Q is the set of accepting (or terminal) states;\nq0 ∈ Q is the initial state.\n\nDefinition: A deterministic finite automaton (DFA) is a finite automaton in which for each state and each symbol of the input alphabet there is exactly one transition.\n\nOne way to transform a finite automaton into a regular expression is the Brzozowski algebraic method (a method for transforming deterministic finite automata into regular expressions).\n\nDescription of the method:\nWhen transforming a DFA into a regular expression, a system of regular expressions is created for each state in the DFA, and then it is solved for the regular expressions Ri associated with the terminal states qi. The equation is constructed as follows: for each state qi, the equation Ri\nis the union of the transitions leading to this state. The transition a from qi to qj is denoted as aRi. If qi\nis a final state, then ε is added to Ri. This leads to a system of equations of the form:\n\nNote: + is equivalent to | in regular expression\n\n⎧R1=a1R1+a2∗R2+a3R3+... \n⎪R2=a1R1+a2∗R2+a3R3+...+ε (R2 is a final state)\n...\n⎩Rm=a1R1+a2R2+a3R3+...\n\nwhere ak = ∅ if there is no transition from Ri to Rj. The system can be solved by simple substitution, \nexcept when the unknown appears on both the right and left sides of the equation. \nThis solves an equation of the form R=Q+RP, where P!=ε, having the solution R=QP^∗. ( + is equivalent to | )\n\nUsage example:\nProblem: You need to build a regular expression that satisfies the DFA.\nQ={q1,q2,q3,q4}\nX={a,b}\nF={q1,q4}\nδ = {<q1, b> -> q3; <q1, a> -> q2; <q3, a> -> q1; <q3, b> -> q4; <q2, a> -> q4;\n<q2, b> -> q1; <q4, b> -> q4; <q4,a> -> q4}\n\nSolution using Brzozowski's method:\n\n⎧R1=bR2+aR3+ε\n⎪R2=aR1\n⎨R3=bR1\n⎩R4=aR2+bR3+aR4+bR4+ε\n\nConsider the first final state.\n\nR1=ε+abR1+baR1=ε+R1(ab+ba)\n\nUse an equation of the form R=Q+RP.\n\nR1=(ab+ba)^∗\n\nConsider the second final state.\n\nR4=R1(aa+bb)+R4(a+b)= R1(aa+bb)(a+b)^∗ = (ab+ba)∗(aa+bb)(a+b)^∗\n\nLet's combine the expressions for the terminal states and get the desired regular expression:\n\nR=R1+R4=(ab+ba)^∗(ε+(aa+bb)(a+b)^∗).\nOr R=(ab|ba)^∗(ε|(aa|bb)(a|b)^∗)."
    },
    {
        "question": "What is the translation of a finite automaton into a regular expression, give an example of such a translation.",
        "answer": "\nThis is the process of converting a finite automaton into a regular expression.\nwhich describes the same language as the automaton.\n\nA regular expression over an alphabet Σ={c1,c2,…,ck} is a way of creating a language on Σ.\n\nIt is defined recursively as follows:\nFor any i, the word ci is a regular expression defining the language of a single word ci.\n\nε is a regular expression defining the language of a single empty string, and ∅ is the empty language.\n\nIf α1 and α2 are regular expressions defining languages ​​L1 and L2, respectively,\nthen (α1)|(α2) is a regular expression defining L1 ⋃ L2.\n\nIf α1 and α2 are regular expressions defining languages ​​L1 and L2, respectively,\nthen (α1)(α2) is a regular expression defining L1L2.\n\nIf α1 is a regular expression defining a language L1,\nthen (α1)^∗ is a regular expression defining L1∗.\n\nThe operations are listed in order of increasing precedence, with parentheses increasing precedence, like arithmetic expressions.\n\nExample: Let Σ = {a, b}. Then (((ab)^∗)(1+a)) is a regular expression over the alphabet Σ.\n\nDefinition: A recognition automaton (or simply a finite automaton) is a quintuplet A = {Q, Σ, δ, F, q0}, in which:\nQ is a finite set of states;\nΣ is the final alphabet;\nδ: Q × Σ → Q is the transition function;\nF ⊆ Q is the set of accepting (or terminal) states;\nq0 ∈ Q is the initial state.\n\nDefinition: A deterministic finite automaton (DFA) is a finite automaton in which there is exactly one transition for each state and each symbol of the input alphabet.\n\nOne way to transform a finite automaton into a regular expression is Brzozowski's algebraic method (method of transforming deterministic finite automata into regular expressions).\n\nDescription of the method:\nBy transforming a DFA into a regular expression, a system of regular expressions is created for each state of the DFA, and then solved for the regular expressions Ri associated with the terminal states qi. The equation is constructed as follows: for each state qi, the equation Ri\nis the union of the transitions leading to this state. The transition from a from qi to qj is denoted aRi. If qi\nis a terminal state, then ε is added to Ri. This leads to a system of equations of the form:\n\nNote: + is equal to | in a regular expression\n\n⎧R1=a1R1+a2∗R2+a3R3+... \n⎪R2=a1R1+a2∗R2+a3R3+...+ε (R2 is the final state)\n...\n⎩Rm=a1R1+a2R2+a3R3+...\n\nwhere ak = ∅ if there is no transition from Ri to Rj. The system is solved by a simple substitution: \nexcept when the unknown appears on the right and left sides of the equation. \nThis solves an equation of the form R=Q+RP, where P!=ε, has a solution R=QP^∗. ( + is equivalent to | )\n\nUsage example: \nProblem: You need to create a regular expression that satisfies the DFA.\nQ={q1,q2,q3,q4}\nХ={а,б}\nF={q1,q4}\nδ = {<q1, b> -> q3; <q1, а> -> q2; <q3, а> -> q1; <q3, b> -> q4; <q2, а> -> q4;\n<q2, b> -> q1; <q4, b> -> q4; <q4,a> -> q4}\n\nBrzozowski solution:\n\n⎧R1=bR2+aR3+ε\n⎪R2=aR1\n⎨R3=bR1\n⎩R4=aR2+bR3+aR4+bR4+ε\n\nConsider the first final state.\n\nR1=ε+abR1+baR1=ε+R1(ab+ba)\n\nWe use an equation of the form R=Q+RP.\n\nR1=(ab+ba)^∗\n\nLet's consider the second final state.\n\nR4=R1(aa+bb)+R4(a+b)= R1(aa+bb)(a+b)^∗ = (ab+ba)∗(aa+bb)(a+b)^∗\n\nLet's combine the expressions for the terminal states and get the desired regular expression:\n\nR=R1+R4=(ab+ba)^∗(ε+(aa+bb)(a+b)^∗).\nOr R=(ab|ba)^∗(ε|(aa|bb)(a|b)^∗)."
    },
    {
        "question": "What is the translation of a finite automaton into a regular expression, give an example of such a translation",
        "answer": "\nThis is the process of converting a finite automaton into a regular expression\nthat describes the same language as the automaton.\n\nA regular expression over an alphabet Σ={c1,c2,…,ck} is a way of creating a language on Σ.\nIt is defined recursively as follows:\nFor any i, a word ci is a regular expression defining the language of a single word ci.\n\nε is a regular expression defining the single language of empty strings, and ∅ is the empty language.\n\nIf α1 and α2 are regular expressions defining languages ​​L1 and L2, respectively,\nthen (α1)|(α2) is a regular expression defining L1 ⋃ L2.\n\nIf α1 and α2 are regular expressions defining languages ​​L1 and L2, respectively,\nthen (α1)(α2) is a regular expression defining L1L2.\n\nIf α1 is a regular expression defining a language L1,\nthen (α1)^∗ is a regular expression defining L1∗.\n\nThe operations are listed in order of increasing precedence, with parentheses increasing in precedence, as do arithmetic expressions.\n\nExample: Let Σ = {a, b}. Then (((ab)^∗)(1+a)) is a\nregular expression over the alphabet Σ.\n\nDefinition: A recognition automaton (or simply a finite automaton) is a quintuplet A = {Q, Σ, δ, F, q0}, in which:\nQ is a finite set of states;\nΣ is a finite alphabet;\nδ: Q × Σ → Q is the transition function;\nF ⊆ Q is the set of accepting (or terminal) states;\nq0 ∈ Q is the initial state.\n\nDefinition: A deterministic finite automaton (DFA) is a finite automaton in which for each state and each symbol of the input alphabet there is exactly one transition.\n\nOne way to transform a finite automaton into a regular expression is the Brzozowski algebraic method (a method for transforming deterministic finite automata into regular expressions).\n\nDescription of the method:\nWhen transforming a DFA into a regular expression, a system of regular expressions is created for each state of the DFA, and then it is solved for regular expressions Ri associated with terminal states qi. The equation is constructed as follows: for each state qi, the equation Ri\nis the union of transitions leading to this state. The transition a from qi to qj is denoted aRi. If ci\nis a terminal state, then ε is added to Ri. This leads to a system of equations of the form:\n\nNote: + is equivalent to | in a regular expression\n\n⎧R1=a1R1+a2∗R2+a3R3+... \n⎪R2=a1R1+a2∗R2+a3R3+...+ε (R2 is a terminal state)\n...\n⎩Rm=a1R1+a2R2+a3R3+...\n\nwhere ak = ∅ if there is no transition from Ri to Rj. The system is solved by a simple substitution: \nexcept when the unknown appears on both the right and left sides of the equation. \nThis solves an equation of the form R=Q+RP, where P!=ε, has a solution R=QP^∗. ( + is equivalent to | )\n\nUse an example: \nProblem: You need to create a regular expression that satisfies the DFA.\nQ={q1,q2,q3,q4}\nХ={а,б}\nF={q1,q4}\nδ = {<q1, b> -> q3; <q1, а> -> q2; <q3, а> -> q1; <q3, b> -> q4; <q2, а> -> q4;\n<q2, b> -> q1; <q4, b> -> q4; <q4,a> -> q4}\n\nBrzozowski solution:\n\n⎧R1=bR2+aR3+ε\n⎪R2=aR1\n⎨R3=bR1\n⎩R4=aR2+bR3+aR4+bR4+ε\n\nConsider the first terminal state.\n\nR1=ε+abR1+baR1=ε+R1(ab+ba)\n\nWe use an equation of the form R=Q+RP.\n\nR1=(ab+ba)^∗\n\nLet's consider the second terminal state.\n\nR4=R1(aa+bb)+R4(a+b)= R1(aa+bb)(a+b)^∗ = (ab+ba)∗(aa+bb)(a+b)^∗\n\nCombine the expressions for the terminal states and get the desired regular expression:\n\nR=R1+R4=(ab+ba)^∗(ε+(aa+bb)(a+b)^∗).\nOr R=(ab|ba)^∗(ε|(aa|bb)(a|b)^∗)."
    },
    {
        "question": "Binary prefix coding",
        "answer": "\nis a homomorphism h : Σ+ → {0, 1}+ such that ∀ a, b ∈ Σ ∀ w ∈ {0, 1}∗(h(a) != h(b)w)."
    },
    {
        "question": "Binary prefix coding",
        "answer": "\nis a homomorphism h : Σ+ → {0, 1}+ such that ∀ a, b ∈ Σ ∀ w ∈ {0, 1}∗(h(a) != h(b)w)."
    },
    {
        "question": "Binary prefix encoding",
        "answer": "\nis a homomorphism h : Σ+ → {0, 1}+ such that ∀ a, b ∈ Σ ∀ w ∈ {0, 1}∗(h(a) != h(b)w)."
    },
    {
        "question": "Binary prefix encoding",
        "answer": "\nis a homomorphism h : Σ+ → {0, 1}+ such that ∀ a, b ∈ Σ ∀ w ∈ {0, 1}∗(h(a) != h(b)w)."
    },
    {
        "question": "Formal grammar",
        "answer": "\nis a way of describing a formal language, which is a quadruple\nΓ=⟨Σ,N,S∈N,P⊂((Σ∪N)∗N(Σ∪N)∗)×(Σ∪N)∗⟩, where:\nΣ is an alphabet whose elements are called terminals;\nN is a set whose elements are called nonterminals;\nS is the start symbol of the grammar;\nP is a set of production rules (or productions) α -> β."
    },
    {
        "question": "Formal grammar",
        "answer": "\n— a way of describing a formal language, which is a quadruple\nΓ=⟨Σ,N,S∈N,P⊂((Σ∪N)∗N(Σ∪N)∗)×(Σ∪N)∗⟩, where:\nΣ is an alphabet whose elements are called terminals;\nN is a set whose elements are called nonterminals;\nS is the initial symbol of the grammar;\nP is the set of production rules (or productions) α -> β."
    },
    {
        "question": "Formal grammar",
        "answer": "\n- a way of describing a formal language representing a quadruple\nΓ=⟨Σ,N,SεN,P⊂((Σ∪N)∗N(Σ∪N)∗)×(Σ∪N)∗⟩, where:\nΣ is an alphabet whose elements are called terminals;\nN is a set whose elements are called non-terminal;\nS is an initial grammatical symbol;\nP is a set of production rules (or productions) α -> β."
    },
    {
        "question": "Formal grammar",
        "answer": "\nis a way of describing a formal language, which is a quadruple\nΓ=⟨Σ,N,SεN,P⊂((Σ∪N)∗N(Σ∪N)∗)×(Σ∪N)∗⟩, where:\nΣ is an alphabet whose elements are called terminals;\nN is a set whose elements are called non-terminal;\nS is the symbol for the beginning of grammar;\nP is a set of production rules (or productions) α -> β."
    },
    {
        "question": "Sentential form",
        "answer": " is a sequence of terminals and nonterminals derived from the start symbol."
    },
    {
        "question": "Form of proposal",
        "answer": " is a sequence of terminals and nonterminals derived from the start symbol."
    },
    {
        "question": "Form of proposal",
        "answer": " is a sequence of terminals and nonterminals derived from the original symbol."
    },
    {
        "question": "Form of proposal",
        "answer": " is a sequence of terminals and nonterminals originating from a start symbol."
    },
    {
        "question": "Grammar for correct bracket sequence",
        "answer": "\nΣ={(,)}\nS -> (S)\nS -> SS\nS -> ε"
    },
    {
        "question": "Grammar for correct bracket sequence",
        "answer": "\nΣ={(,)}\nS -> (S)\nS -> SS\nS -> ε"
    },
    {
        "question": "Grammar of correct bracket sequence",
        "answer": "\nΣ={(,)}\nC -> (C)\nC -> CC\nS -> ε"
    },
    {
        "question": "Grammar of correct bracket sequence",
        "answer": "\nΣ={(,)}\nC -> (C)\nC -> CC\nS -> ε"
    },
    {
        "question": "Regular (right-linear) grammar is",
        "answer": " is a grammar that contains rules of the form S -> ε (where S does not occur in the right-hand sides of any rules), T_i -> a_i, T_i -> a_iT_j."
    },
    {
        "question": "Regular (right-linear) grammar is",
        "answer": " is a grammar that contains rules of the form S -> ε (where S does not occur on the right-hand side of any rule), T_i -> a_i, T_i -> a_iT_j."
    },
    {
        "question": "Regular (right-linear) grammar is",
        "answer": " is a grammar containing rules of the form S -> ε (where S does not appear on the right-hand side of any rule), T_i -> a_i, T_i -> a_iT_j."
    },
    {
        "question": "Regular (right-linear) grammar is",
        "answer": " is a grammar containing rules of the form S -> ε (where S does not appear on the right-hand side of any rule), T_i -> a_i, T_i -> a_iT_j."
    },
    {
        "question": "G_1 and G_2 are right-linear. Construct G: L(G) = L(G_1) U L(G_2) (union):",
        "answer": "\n1. Rename nonterminals from N_1 and N_2 so that N_1 ∩ N_2 = ∅ (perform α-transformation). Apply renaming to rules G_1 and G_2\n2. Declare a fresh nonterminal S as the starting symbol and for all rules of G_1 of the form S_1 -> α and rules of G_2 of the form S_2 -> β, add rules S -> α, S-> β to the rules of G\n3. Add the remaining rules from G_1 and G_2 to the rules of G."
    },
    {
        "question": "G_1 and G_2 are rectilinear. Construct G: L(G) = L(G_1) U L(G_2) (union):",
        "answer": "\n1. Rename nonterminals from N_1 and N_2 so that N_1 ∩ N_2 = ∅ (perform α-transformation). Apply renaming to rules G_1 and G_2\n2. Declare a new nonterminal S as a start symbol and for all rules of G_1 of the form S_1 -> α and rules of G_2 of the form S_2 -> β, add rules S -> α, S-> β to the rules of G\n3. Add the remaining rules from G_1 and G_2 to the rules of G."
    },
    {
        "question": "G_1 and G_2 are straight lines. Construct G: L(G) = L(G_1) UL(G_2) (union):",
        "answer": "\n1. Rename nonterminals N_1 and N_2 so that N_1 ∩ N_2 = ∅ (perform α-transformation). Apply renaming to rules G_1 and G_2\n2. Declare a new nonterminal S as the initial symbol and for all rules of G_1 of the form S_1 -> α and rules of G_2 of the form S_2 -> β, add rules S -> α, S-> β to the rules of G.\n3. Add the remaining rules from G_1 and G_2 to the rules of G."
    },
    {
        "question": "G_1 and G_2 are right linear. Construct G: L(G) = L(G_1) UL(G_2) (the union):",
        "answer": "\n1. Rename nonterminals N_1 and N_2 so that N_1 ∩ N_2 = ∅ (perform α-transformation). Apply renaming to rules G_1 and G_2\n2. Declare new nonterminal S as start symbol and to all rules of G_1 of the form S_1 -> α and rules of G_2 of the form S_2 -> β add rules S -> α, S- > β to rules of G\n3. Add remaining rules of G_1 and G_2 to rules of G."
    },
    {
        "question": "G_1 and G_2 are right-linear. Construct G: L(G) = L(G_1)L(G_2) (concatenation):",
        "answer": "\n1. Rename nonterminals from N_1 and N_2 so that N_1 ∩ N_2 = ∅ (perform α-transformation).\n2. Construct its version without ε-rules from G_1\n3. For every rule from G_1 of the form A -> a, construct a rule G of the form A -> aS_2, where S_2 is the starting nonterminal of G_2\n4. Add the remaining rules from G_1 and G_2 to the rules of G. Declare S_1 as the starting one\n5. If ε ∈ L(G_1) (before step 2), then for all S_2 -> β, add the rule S_1 -> β."
    },
    {
        "question": "G_1 and G_2 are straight. Construct G: L(G) = L(G_1)L(G_2) (concatenation):",
        "answer": "\n1. Rename nonterminals from N_1 and N_2 so that N_1 ∩ N_2 = ∅ (perform α-transformation).\n2. Construct its version without ε-rules from G_1\n3. For each rule from G_1 of the form A -> a, construct a rule of G of the form A -> aS_2, where S_2 is the initial nonterminal of G_2\n4. Add the remaining rules from G_1 and G_2 to the rules of G. Declare S_1 as the initial\n5. If ε ∈ L(G_1) (before step 2), then for all S_2 -> β, add the rule S_1 -> β."
    },
    {
        "question": "G_1 and G_2 are straight lines. Construct G: L(G) = L(G_1)L(G_2) (concatenation):",
        "answer": "\n1. Rename the nonterminals N_1 and N_2 so that N_1 ∩ N_2 = ∅ (perform the α-transformation).\n2. Build your version without ε-rules from G_1.\n3. For each rule G_1 of the form A -> a, build a rule G of the form A -> aS_2, where S_2 is the initial nonterminal of G_2.\n4. Append the remaining rules of G_1 and G_2 to the rules of G. Declare S_1 the original rule.\n5. If ε ∈ L(G_1) (before step 2), then for all S_2 -> β, add the rule S_1 -> β."
    },
    {
        "question": "G_1 and G_2 are linear to the right. Construct G: L(G) = L(G_1)L(G_2) (concatenation):",
        "answer": "\n1. Rename the nonterminals N_1 and N_2 so that N_1 ∩ N_2 = ∅ (perform the α-transformation).\n2. Create your version without ε-rules from G_1.\n3. For each rule G_1 of the form A -> a, construct a rule G of the form A -> aS_2, where S_2 is the starting nonterminal of G_2.\n4. Add the remaining rules of G_1 and G_2 to the rules of G. Declare S_1 the starting nonterminal.\n5. If ε ∈ L(G_1) (before step 2), then for all S_2 -> β, add the rule S_1 -> β."
    },
    {
        "question": "G_1 is right-linear. Construct G: L(G) = L(G_1)+ (positive Kleene iteration):",
        "answer": "\n1. Build its version without ε-rules from G_1\n2. For every rule from G_1 of the form A → a, build a rule G of the form A → aS_1, where S_1 is the starting nonterminal of G_1\n3. Add all rules from G_1 (including those of the form A → a) to the rules of G. Declare S_1 as the starting one\n4. If ε ∈ L(G_1) (before step 2), add the rule S_1 → ε and derive S1 from the recursion"
    },
    {
        "question": "G_1 is right-linear. Construction of G: L(G) = L(G_1)+ (positive Kleene iteration):",
        "answer": "\n1. Build its version without ε-rules from G_1\n2. For each rule from G_1 of the form A → a, build a rule of G of the form A → aS_1, where S_1 is the initial nonterminal of G_1\n3. Add all rules from G_1 (including rules of the form A → a) to the rules of G. Declare S_1 as the initial\n4. If ε ∈ L(G_1) (before step 2), add the rule S_1 → ε and derive S1 from the recursion"
    },
    {
        "question": "G_1 is rectilinear. Construction of G: L(G) = L(G_1)+ (positive Kleene iteration):",
        "answer": "\n1. Build your version without ε-rules from G_1.\n2. For each rule G_1 of the form A → a, build a rule G of the form A → aS_1, where S_1 is the initial nonterminal of G_1.\n3. Add all rules of G_1 (including rules of the form A → a) to the rules of G. Declare S_1 the initial\n4. If ε ∈ L(G_1) (before step 2), add a rule S_1 → ε and derive S1 from the recursion"
    },
    {
        "question": "G_1 is linear to the right. Construct G: L(G) = L(G_1)+ (positive Kleene iteration):",
        "answer": "\n1. Create your version without ε-rules from G_1.\n2. For each rule G_1 of the form A → a, construct a rule G of the form A → aS_1, where S_1 is the starting nonterminal of G_1.\n3. Add all rules of G_1 (including rules of the form A → a) to the rules of G. Declare S_1 the starting rule.\n4. If ε ∈ L(G_1) (before step 2), add a rule S_1 → ε and derive S1 from the recursion"
    },
    {
        "question": "G is right-linear. Construct G′ without rules of the form A → ε such that L(G′) = L(G) or L(G′) ∪ {ε} = L(G):",
        "answer": "\n1. Transfer to G′ all rules of G that do not have the form A → ε\n2. If there is a rule A → ε, then for all rules of the form B → aA we additionally construct rules B → a."
    },
    {
        "question": "G is right-linear. Construct G′ without rules of the form A → ε such that L(G′) = L(G) or L(G′) ∪ {ε} = L(G):",
        "answer": "\n1. We transfer to G′ all rules of G that do not have the form A → ε\n2. If there is a rule A → ε, then for all rules of the form B → aA we additionally construct rules B → a."
    },
    {
        "question": "G is linear right. We construct G′ without rules of the form A → ε such that L(G′) = L(G) or L(G′) ∪ {ε} = L(G):",
        "answer": "\n1. We transfer to G′ all rules of G that do not have the form A → ε\n2. If there is a rule A → ε, then for all rules of the form B → aA we additionally construct rules B → a."
    },
    {
        "question": "G is right linear. Let us construct G′ without rules of the form A → ε such that L(G′) = L(G) or L(G′) ∪ {ε} = L(G):",
        "answer": "\n1. Transfer to G′ all rules of G that do not have the form A → ε\n2. If there is a rule A → ε, then for all rules of the form B → aA we additionally construct rules B → a."
    },
    {
        "question": "G_1, G_2 are right-linear. Construct G′ such that L(G′) = L(G_1) ∩ L(G_2):",
        "answer": "\n1. Construct the starting symbol G′ — a pair ⟨S_1, S_2⟩, where S_i is the starting symbol of the grammar G_i\n2. Place ⟨S_1, S_2⟩ in the set U of unparsed nonterminals. Declare the set T of parsed nonterminals empty\n3. For each successive nonterminal ⟨A_1, A_2⟩ ∈ U:\n- if A_1 → a ∈ G_1, A_2 → a ∈ G_2, then add the rule ⟨A_1, A_2⟩ → a to G′;\n- if A_1 → aA_3 ∈ G_1, A_2 → aA_4 ∈ G_2, then add to G′ the rule ⟨A_1, A_2⟩ → a⟨A_3, A_4⟩, and to U the non-terminal ⟨A_3, A_4⟩, if it is not yet in the set T;\n- if all pairs of rules specified above have been processed, then move ⟨A_1, A_2⟩ from U to T\n4 Repeat step 3 until the set U is empty\n5 If ε ∈ L(G_1) & ε ∈ L(G_2), then add to G′ the rule ⟨S_1, S_2⟩ → ε."
    },
    {
        "question": "G_1, G_2 are right-linear. Let us construct G′ so that L(G′) = L(G_1) ∩ L(G_2):",
        "answer": "\n1. Construct the initial symbol of G′ — a pair ⟨S_1, S_2⟩, where S_i is the initial symbol of the grammar G_i\n2. Place ⟨S_1, S_2⟩ in the set U of unparsed nonterminals. Declare the set T of parsed nonterminals empty\n3. For each subsequent nonterminal ⟨A_1, A_2⟩ ∈ U:\n- if A_1 → a ∈ G_1, A_2 → a ∈ G_2, then add the rule ⟨A_1, A_2⟩ → a to G′;\n- if A_1 → aA_3 ∈ G_1, A_2 → aA_4 ∈ G_2, then add the rule ⟨A_1, A_2⟩ → a⟨A_3, A_4⟩ to G′, and the nonterminal ⟨A_3, A_4⟩ to U if it is not yet in the set T;\n- if all pairs of rules specified above have been processed, then move ⟨A_1, A_2⟩ from U to T\n4 Repeat step 3 until the set U is empty\n5 If ε ∈ L(G_1) & ε ∈ L(G_2), then add the rule ⟨S_1, S_2⟩ → ε to G′."
    },
    {
        "question": "G_1, G_2 - linear right. Let us construct G′ such that L(G′) = L(G_1) ∩ L(G_2):",
        "answer": "\n1. Construct the initial symbol G′ - the pair ⟨S_1, S_2⟩, where S_i is the initial symbol of the grammar G_i.\n2. Place ⟨S_1, S_2⟩ in the set U of unparsed non-terminals. Declare the set T of analyzed non-terminals empty\n3. For each following non-terminal ⟨A_1, A_2⟩ ∈ U:\n- if A_1 → a ∈ G_1, A_2 → a ∈ G_2, then add the rule ⟨A_1, A_2⟩ → a to G′;\n- if A_1 → aA_3 ∈ G_1, A_2 → aA_4 ∈ G_2, then add the rule ⟨A_1, A_2⟩ → a⟨A_3, A_4⟩ to G, and the non-terminal ⟨A_3, A_4⟩ to U if n ' is not yet in T;\n- if all the above pairs of rules are processed, then move ⟨A_1, A_2⟩ from U to T\n4 Repeat step 3 until U is empty.\n5 If ε ∈ L(G_1) and ε ∈ L(G_2), then we add the rule ⟨S_1, S_2⟩ → ε to G′."
    },
    {
        "question": "G_1, G_2 are linear to the right. Let us construct G′ such that L(G′) = L(G_1) ∩ L(G_2):",
        "answer": "\n1. Construct the starting symbol of G′ — a pair ⟨S_1, S_2⟩, where S_i is the starting symbol of the grammar G_i.\n2. Place ⟨S_1, S_2⟩ in the set U of unanalyzed nonterminals. Declare the set T of analyzed nonterminals empty.\n3. For each subsequent nonterminal ⟨A_1, A_2⟩ ∈ U:\n- if A_1 → a ∈ G_1, A_2 → a ∈ G_2, then add the rule ⟨A_1, A_2⟩ → a to G′;\n- if A_1 → aA_3 ∈ G_1, A_2 → aA_4 ∈ G_2, then add the rule ⟨A_1, A_2⟩ → a⟨A_3, A_4⟩ to G′, and the non-terminal ⟨A_3, A_4⟩ to U if it is not yet in the set T ;\n- if all the above pairs of rules are processed, then move ⟨A_1, A_2⟩ from U to T\n4 Repeat step 3 until the set U is empty.\n5 If ε ∈ L(G_1) and ε ∈ L(G_2), then add the rule ⟨S_1, S_2⟩ → ε to G."
    },
    {
        "question": "Automata are called isomorphic",
        "answer": "\nif there is a bijection between their vertices such that all transitions are preserved, the terminal states correspond to the terminal ones, and the initial ones correspond to the initial ones."
    },
    {
        "question": "Automata are called isomorphic.",
        "answer": "\nif there is a bijection between their vertices in which all transitions are preserved, then the final states correspond to the final ones, and the initial ones correspond to the initial ones."
    },
    {
        "question": "Automata are called isomorphic.",
        "answer": "\nif there is a bijection between their vertices in which all transitions are preserved, then the final states correspond to the final states, and the initial states correspond to the initial states."
    },
    {
        "question": "Automata are called isomorphic.",
        "answer": "\nif there is a bijection between their vertices such that all transitions are preserved, then the terminal states correspond to the terminal states, and the initial states correspond to the initial states."
    },
    {
        "question": "Given two deterministic finite automata, determine whether they are isomorphic to each other. It is guaranteed that all states of the automata are reachable.",
        "answer": "\nAlgorithm\nIt follows from the definition that if the automata are isomorphic, then their states can be numbered in one way so that the vertices from different automata with the same numbers are equal - that is, in each of these two states there is a transition to some state with the same number as the transition by the same letter in the other state. Therefore, we can fix some numbering, for example, in the order of depth-first traversal by symbols in lexicographic order and simply check the states with the same numbers for equality. If all states are equal, then the automata will be equal, in our case there will be an isomorphism of two automata. The asymptotics of the algorithm coincides with the asymptotics of depth-first traversal, that is, O(N+M), where N is the total number of vertices in the automata, M is the total number of edges.\n\nPseudocode\nTransitions is a set of pairs ⟨a, T⟩ , where a∈Σ, T∈Q\nAssotiations is an array, where each state of the first automaton corresponds to the found state of the second automaton.\n\nboolean dfs(u: Vertex, v: Vertex): \nvisited[u] = true // note that only one visited array is enough for two automata\n\nif (v.terminal != u.terminal)\nreturn false \nassociations[u] = v\nboolean result = true\nfor (⟨c,q⟩ : u.transitions) \nVertex t1 = u.transitions.getVertex(c)\nVertex t2 = v.transitions.getVertex(c)\nif one of the vertices t1, t2 is devilish, and the other is not\nreturn false\nif (visited[t1]) \nresult = result and t2 == associations[t1]\nelse\nresult = result and dfs(t1, t2) \n\nreturn result"
    },
    {
        "question": "Given two deterministic finite automata, determine whether they are isomorphic to each other. It is guaranteed that all states of the automata are reachable.",
        "answer": "\nAlgorithm\nIt follows from the definition that if the automata are isomorphic, then their states can be numbered in one way so that the vertices from different automata with the same numbers are equal - that is, in each of these two states there was a transition to some state with the same number as the transition by the same letter in the other state. Therefore, we can fix some numbering, for example, in the order of depth-first traversal by symbols in lexicographic order and simply check the states with the same numbers for equality. If all states are equal, then the automata will be equal, in our case there will be an isomorphism of two automata. The asymptotics of the algorithm coincides with the asymptotics of depth-first traversal, that is, O(N+M), where N is the total number of vertices in the automata, M is the total number of edges.\n\nPseudocode\nTransitions are a set of pairs ⟨a, T⟩, where a∈Σ, T∈Q\nAssociations are an array, where each state of the first automaton corresponds to the found state of the second automaton.\n\nboolean dfs(u: Vertex, v: Vertex): \nvisited[u] = true // note that for two automata only one visited array is enough\n\nif (v.terminal != u.terminal)\nreturn false \nassociations[u] = v\nboolean result = true\nfor (⟨c,q⟩ : u.transitions) \nVertex t1 = u.transitions.getVertex(c)\nVertex t2 = v.transitions.getVertex(c)\nif one of the vertices t1, t2 is devilish and the other is not\nreturn false\nif (visited[t1]) \nresult = result and t2 == associations[t1]\nelse\nresult = result and dfs(t1, t2) \n\nreturn result"
    },
    {
        "question": "Given two deterministic finite automata, determine whether they are isomorphic to each other. It is guaranteed that all states of the automata are accessible.",
        "answer": "\nAlgorithm\nIt follows from the definition that if the automata are isomorphic, then their states can be numbered so that the vertices of different automata with the same numbers are equal, i.e. in each of these two states there was a transition to a state with the same number as a transition by the same letter to another state. Therefore, we can adjust the numbering, for example, in the order of depth-first traversal by symbols in lexicographic order, and simply check the equality of states with the same numbers. If all states are equal, then the automata will be equal; in our case, there will be an isomorphism of two automata. The asymptotics of the algorithm coincides with the asymptotics of depth-first traversal, i.e. O(N+M), where N is the total number of vertices in the automata, M is the total number of edges.\n\nPseudocode\nTransitions are a set of pairs ⟨a, T⟩, where aεΣ, TεQ\nAssociations are a table, where each state of the first automaton corresponds to the found state of the second automaton.\n\nboolean dfs(u: node, v: node): \nvisit[u] = true // note that for two machines one visited table is enough\n\nif (v.terminal != u.terminal)\nreturn false\nassociations[u] = v\nboolean result = true\nfor (⟨c,q⟩: u.transitions)\n\nNode t1 = u.transitions.getVertex(c)\nNode t2 = v.transitions.getVertex(c)\nif one of nodes t1, t2 is evil and other is not\nreturn false\nif (visit[t1])\nresult = result and t2 == associations[t1]\nother\nresult = result and dfs(t1, t2)\n\nreturn result"
    },
    {
        "question": "Given two deterministic finite automata, determine whether they are isomorphic to each other. It is guaranteed that all states of the automata are reachable.",
        "answer": "\nAlgorithm\nIt follows from the definition that if the automata are isomorphic, then their states can be numbered so that the vertices of different automata with the same numbers are equal, that is, in each of these two states, a transition to a state with the same number occurs, as does a transition to the same letter in another state. Therefore, we can specify the numbering, for example, in the order of traversal by symbols in lexicographic order and simply check the equality of states with the same numbers. If all states are equal, then the automata will be equal, in our case there will be an isomorphism of two automata. The asymptotics of the algorithm coincides with the asymptotics of depth-first traversal, i.e. O(N+M), where N is the total number of vertices in the automata, M is the total number of edges.\n\nPseudocode\nTransitions are a set of pairs ⟨a, T⟩, where aεΣ, TεQ\nAssociations are an array where each state of the first automaton corresponds to the found state of the second automaton.\n\nboolean dfs(u: Vertex, v: Vertex): \nvisit[u] = true // note that one visited array is enough for two automata\n\nif (v.terminal != u.terminal)\nreturns false\nassociations[u] = v\nboolean result = true\nfor (⟨c,q⟩: u.transitions)\n\nVertex t1 = u.transitions.getVertex(c)\nVertex t2 = v.transitions.getVertex(c)\nif one of t1, t2 is evil and the other is not\nreturns false\nif (visit[t1])\n\nresult = result and t2 == associations[t1]\nOtherwise\nresult = result and dfs(t1, t2)\n\nreturn result"
    },
    {
        "question": "Semiring S = ⟨A, ⊕, ⊗, 0⟩ over support A",
        "answer": "\nis an algebraic structure such that:\nS is a commutative monoid with respect to ⊕;\nS is a semigroup with respect to ⊗;\n0 is id with respect to addition and zero with respect to multiplication;\nleft and right distributivities hold."
    },
    {
        "question": "Semiring S = ⟨A, ⊕, ⊗, 0⟩ over support A",
        "answer": "\nis an algebraic structure such that:\nS is a commutative monoid with respect to ⊕;\nS is a semigroup with respect to ⊗;\n0 is id with respect to addition and zero with respect to multiplication;\nleft and right distributivities hold."
    },
    {
        "question": "Semiring S = ⟨A, ⊕, ⊗, 0⟩ on support A",
        "answer": "\nis an algebraic structure such that:\nS is a monoid commutative with respect to ⊕;\nS is a semigroup with respect to ⊗;\n0 is the identifier under addition and zero under multiplication;\nboth left and right distributions are valid."
    },
    {
        "question": "Semiring S = ⟨A, ⊕, ⊗, 0⟩ on support A",
        "answer": "\nis an algebraic structure such that:\nS is a monoid commutative under ⊕;\nS is a semigroup under ⊗;\n0 is zero under addition and zero under multiplication;\nleft and right distributions are respected."
    },
    {
        "question": "Kleene algebra ⟨Σ, |, ·, ∗, ∅, ε⟩",
        "answer": "\nidempotent semiring with unity satisfying the following axioms:\n1) x∗x + 1 = x∗ = 1 + xx∗(unfolding axiom)\n2) (Salomaa's formalization): ∀p, q, x ((p | qx = x ⇒ x = q∗p) & (p | xq = x ⇒ x = pq∗)), where q does not recognize ε — the left and right Arden lemmas;\n3) (Kozen's formalization): ∀p, q, x ((q | px <= x ⇒ p∗q <= x) & (q | xp <= x ⇒ qp∗ <= x)), where x <= y ⇔ x | y = y, x = y ⇔ x <= y & y <= x.\n\nAxiom -- Abbreviation\nx | x = x -- (Idm)\nε | xx∗ = x∗, ε | x∗x = x∗ -- (Unfold) \n(x | y)z = xz | yz, x(y | z) = xy | xz -- (Dstr) \nq | px <= x ⇒ p∗q <= x, q | xp <= x ⇒ qp∗ <= x -- (Koz)"
    },
    {
        "question": "Kleene algebra ⟨Σ, |, ·, ∗, ∅, ε⟩",
        "answer": "\nidempotent semiring with unity satisfying the following axioms:\n1) x∗x + 1 = x∗ = 1 + xx∗(axiom of unfolding)\n2) (Salomaa's formalization): ∀p, q, x ((p | qx = x ⇒ x = q∗p) & (p | xq = x ⇒ x = pq∗)), where q does not recognize ε — the left and right lemmas of Arden;\n3) (Cozen's formalization): ∀p, q, x ((q | px <= x ⇒ p∗q <= x) & (q | xp <= x ⇒ qp∗ <= x)), where x <= y ⇔ x | y = y, x = y ⇔ x <= y & y <= x.\n\nAxiom – abbreviation\nх | х = х -- (Same)\nε | xx∗ = x∗, ε | x∗x = x∗ -- (Expand) \n(х | y)z = xz | yz, x(y | z) = xy | xz -- (Дстр) \nд | px <= x ⇒ p∗q <= x, q | xp <= x ⇒ qp∗ <= x -- (Коз)"
    },
    {
        "question": "Kleene algebra ⟨Σ, |, ·, ∗, ∅, ε⟩",
        "answer": "\nidempotent semiring with unity satisfying the following axioms:\n1) x∗x + 1 = x∗ = 1 + xx∗ (unfolding axiom)\n2) (Salomaa's formalization): ∀p, q, x ((p | qx = x ⇒ x = q∗p) & (p | xq = x ⇒ x = pq∗)), where q does not recognize ε - on the left and Arden's lemma is correct;\n3) (Cozen's formalization): ∀p, q, x ((q | px <= x ⇒ p∗q <= x) & (q | xp <= x ⇒ qp∗ <= x)), where x <= y ⇔ x | y = y, x = y ⇔ x <= y & y <= x.\n\nAxiom – Abbreviation\nX | x = x -- (Same)\nε | xx∗ = x∗, ε | x∗x = x∗ -- (Expand) \n(х | y)z = xz | yz, x(y | z) = xy | xz -- (Дстр) \nд | px <= x ⇒ p∗q <= x, q | xp <= x ⇒ qp∗ <= x -- (Goat)"
    },
    {
        "question": "Kleene algebra ⟨Σ, |, ·, ∗, ∅, ε⟩",
        "answer": "\nidempotent semiring with unity satisfying the following axioms:\n1) x∗x + 1 = x∗ = 1 + xx∗ (unfolding axiom)\n2) (Salomaa's formalization): ∀p, q, x ((p | qx = x ⇒ x = q∗p) & (p | xq = x ⇒ x = pq∗)), where q does not recognize ε — on the left and Arden's lemmas hold;\n3) (Cozen's formalization): ∀p, q, x ((q | px <= x ⇒ p∗q <= x) & (q | xp <= x ⇒ qp∗ <= x)), where x < = y ⇔ x | y = y, x = y ⇔ x <= y & y <= x.\n\nAxiom – abbreviation\nX | x = x -- (Same)\nε | xx∗ = x∗, ε | x∗x = x∗ -- (Expand) \n(х | y)z = xz | yz, x(y | z) = xy | xz -- (Дстр) \nд | px <= x ⇒ p∗q <= x, q | xp <= x ⇒ qp∗ <= x -- (Коз)"
    },
    {
        "question": "Some theorems of Kleene algebra and their applications",
        "answer": "\n(Bsm) ax = xb ⇒ a∗x = xb∗ (Bisimulation)\n(Sld) x(yx)∗ = (xy)∗x (Shift)\n(Dnst) x∗(yx∗)∗ = (x | y)∗ (Flattening)\n\nShift and flattening laws are used in optimizations of\nregular events: the shift law allows loops with post-computations to be rebuilt into loops with pre-computations; the flattening law allows nested loops and loops with conditions inside an iteration to be rebuilt into each other."
    },
    {
        "question": "Some theorems of Kleene algebra and their applications",
        "answer": "\n(Bsm) ax = xb ⇒ a∗x = xb∗ (Bisimulation)\n(Sld) x(yx)∗ = (xy)∗x (Shift)\n(Dnst) x∗(yx∗)∗ = (x | y)∗ (Smoothing)\n\nShift and alignment laws are used in optimizing\nregular events: the shift law allows loops with subsequent computations to be rebuilt into loops with preliminary computations; the alignment law allows nested loops and loops with conditions inside an iteration to be rebuilt into each other."
    },
    {
        "question": "Some theorems of Kleene algebra and their applications",
        "answer": "\n(Bsm) ax = xb ⇒ a∗x = xb∗ (bisimulation)\n(Sld) x(yx)∗ = (xy)∗x (Shift)\n(Dnst) x∗(yx∗)∗ = (x | y)∗ (Smoothing)\n\nShift and alignment laws are used in optimization.\nregular events: the shift law allows loops with subsequent calculations to be reorganized into loops with previous calculations; the alignment law allows nested loops and loops with conditions inside an iteration to be rearranged within each other."
    },
    {
        "question": "Some theorems of Kleene algebra and their applications",
        "answer": "\n(Bsm) ax = xb ⇒ a∗x = xb∗ (bisimulation)\n(Sld) x(yx)∗ = (xy)∗x (Shift)\n(Dnst) x∗(yx∗)∗ = (x | y)∗ (smoothing)\n\nShift and kurtosis laws are used in event optimization.\nregular: the shift law allows to reconstruct loops with post-computations into loops with pre-computations; the smoothing law allows to reconstruct nested loops and loops with conditions inside the iteration in each other."
    },
    {
        "question": "Completeness theorem of Sal and Koz (formalization of Salomaa and Kozen)",
        "answer": "\nAny equality of regular expressions is deducible from the Sal axiomatics and the Koz axiomatics."
    },
    {
        "question": "Completeness theorem of Salomaa and Koza (formalization of Salomaa and Kozana)",
        "answer": "\nAny equality of regular expressions is derived from the Sala axiomatics and the Koza axiomatics."
    },
    {
        "question": "Salomaa and Koza's Completeness Theorem (Salomaa and Koza's formalization)",
        "answer": "\nAll equalities of regular expressions are derived from the Sala axiomatics and the Koza axiomatics."
    },
    {
        "question": "Sala and Koza's Completion Theorem (Salomaa and Kozana's formalization)",
        "answer": "\nAny equality of regular expressions is derived from the Sala axiomatics and the Koza axiomatics."
    },
    {
        "question": "The meaning of the Arden and Cozen lemma",
        "answer": "\nA fixed point of f(x) is an x ​​such that f(x) = x.\nLet X = (pX) | q, where X is an unknown RE, and p, q are known, and ε does not belong to L(p). Then X = (p)∗q.\nThat is, p∗q is the smallest (but not the only) fixed point of px | q with respect to <=, and the only one if ε does not belong to L(p).\nConsider the system of equations:\nX1 = (A_11X_1) | (A_12X_2) | . . . | B_1\nX2 = (A_21X_1) | (A_22X_2) | . . . | B_2\n. . .\nXn = (A_n1X_1) | (A_n2X_2) | . . . | B_n\nSuppose ε does not belong to A_ij. Let us express X_1 through X_2, . . . , X_n, X_2 through X_3. . . X_n, etc. We obtain a regular expression for X_n (and after reverse substitutions also for X_n-1,. . . ,X_1)."
    },
    {
        "question": "The meaning of the Arden and Cozen lemma",
        "answer": "\nA fixed point of f(x) is x such that f(x) = x.\nLet X = (pX) | q, where X is an unknown RE, p, q are known, and ε does not belong to L(p). Then X = (p)∗q.\nThat is, p∗q is the smallest (but not the only) fixed point of px | q with respect to <=, and the only one if ε does not belong to L(p).\nConsider the system of equations:\nX1 = (A_11X_1) | (A_12X_2) | . . . | B_1\nX2 = (A_21X_1) | (A_22X_2) | . . . | B_2\n. . .\nXn = (A_n1X_1) | (A_n2X_2) | . . . | B_n\nSuppose that ε does not belong to A_ij. Let us express X_1 through X_2, . . , X_n, X_2 through X_3. . . X_n, etc. We obtain a regular expression for X_n (and after reverse substitutions, for X_n-1, . . ,X_1)."
    },
    {
        "question": "The meaning of the Arden and Cozen lemma.",
        "answer": "\nA fixed point of f(x) is x such that f(x) = x.\nLet X = (pX) | q, where X is an unknown RE, p, q are known, and ε does not belong to L(p). Then X = (p)∗q.\nIn other words, p∗q is the smallest (but not the only) fixed point of px | q is relative <= and unique if ε does not belong to L(p).\nConsider the system of equations:\nX1 = (A_11X_1) | (A_12X_2) | . . . | B_1\nX2 = (A_21X_1) | (A_22X_2) | . . . | B_2\n. . .\nXп = (A_n1X_1) | (A_n2X_2) | . . . | B_n\nSuppose that ε does not belong to A_ij. Express X_1 in X_2, . . , X_n, from X_2 to X_3. . . X_n, etc. We obtain a regular expression for X_n (and after reverse substitutions to X_n-1,...,X_1)."
    },
    {
        "question": "The meaning of the Arden and Cozen lemma.",
        "answer": "\nA fixed point of f(x) is an x ​​such that f(x) = x.\nLet X = (pX) | q, where X is an unknown RE, p, q are known, and ε does not belong to L(p). Then X = (p)∗q.\nIn other words, p∗q is the smallest (but not the only) fixed point of px | q with respect to <= and the only one if ε does not belong to L(p).\nConsider the system of equations:\nX1 = (A_11X_1) | (A_12X_2) | . . . | B_1\nX2 = (A_21X_1) | (A_22X_2) | . . . | B_2\n. . .\nXп = (A_n1X_1) | (A_n2X_2) | . . . | B_n\nSuppose that ε does not belong to A_ij. Let us express X_1 through X_2, . . . , X_n, X_2 on X_3. . . Х_н etc. We obtain a regular expression for X_n (and after reverse substitutions, for X_n-1,...,X_1)."
    },
    {
        "question": "formal definitions of LR(k)-grammars",
        "answer": "\n\nDefinition:\n```\nLet Γ=⟨Σ,N,E,P⟩ be a context-free grammar. The augmented grammar obtained from Γ is the grammar Γ′=⟨Σ′,N′,E0,P′⟩, where Σ′=Σ;N′=N∪{E0};E0∉N;P′=P∪{E0→E}\n```\n```\nDefinition:\nLet Γ′=⟨Σ′,N′,E0,P′⟩ be the augmented grammar for the context-free grammar Γ. A grammar Γ is an LR(k)-grammar if for any two right-handed derivations it holds that:\nE0⇒∗βAtz⇒βαtz⇒∗w if |t|=k or |t|<k,|z|=0(z=ε)\nE0⇒∗γBtz′⇒γξtz′⇒∗w′ if |t|=k or |t|<k,|z′|=0(z′=ε) \nit follows that βα=γξ ,\nthen β=γ and A=B\n.\n```\nInformally speaking, we right-fold our string to the starting nonterminal. If, given no more than k symbols of the unparsed part of the string, we can unambiguously determine what the tail of the derived rule folds into, then the grammar is LR(k).\n\nLR(k) means that:\nthe input chain is processed from left to right (left-to-right parse),\nrightmost derivation is performed,\nno more than k symbols of the chain are used to make a decision (k-token lookahead)."
    },
    {
        "question": "formal definitions of LR(k)-grammars",
        "answer": "\n\nDefinition:\n```\nLet Γ=⟨Σ,N,E,P⟩ be a context-free grammar. The augmented grammar obtained from Γ is the grammar Γ′=⟨Σ′,N′,E0,P′⟩, where Σ′=Σ;N′=N∪{E0};E0∉N;P′=P∪{ Е0→Е}\n```\n```\nDefinition:\nLet Γ′=⟨Σ′,N′,E0,P′⟩ be the augmented grammar for the context-free grammar Γ. A grammar Γ is an LR(k)-grammar if for any two right-hand derivations the following holds:\nE0⇒∗βAtz⇒βαtz⇒∗w if |t|=k or |t|<k,|z|=0 (z=ε)\nE0⇒∗γBtz′⇒γξtz′⇒∗w′ if |t|=k or |t|<k,|z′|=0(z′=ε) \nit follows that βα=γξ ,\nthen β=γ and A=B\n.\n```\nInformally speaking, we right-fold our string to the initial non-terminal. If, given no more than k characters of the unparsed part of the string, we can unambiguously determine what the tail of the resulting rule folds into, then the grammar is LR(k).\n\nLR(k) means that:\nthe input chain is processed from left to right (left-to-right parsing),\nthe rightmost output is performed,\nno more than k characters of the chain are used to make a decision (look ahead by k tokens)."
    },
    {
        "question": "formal definitions of LR(k)-grammars",
        "answer": "\n\nDefinition:\n```\nLet Γ=⟨Σ,N,E,P⟩ be a context-free grammar. The augmented grammar obtained from Γ is the grammar Γ′=⟨Σ′,N′,E0,P′⟩, where Σ′=Σ;N′=N∪{E0};E0∉N;P′=P ∪ { Е0→Е}\n```\n```\nDefinition:\nLet Γ′=⟨Σ′,N′,E0,P′⟩ be the augmented grammar for the context-free grammar Γ. A grammar Γ is an LR(k)-grammar if for two right derivations the following holds:\nE0⇒∗βAtz⇒βαtz⇒∗w if |t|=k or |t|<k,|z|=0 (z=ε)\nE0⇒∗γBtz′⇒γξtz′⇒∗w′ if |t|=k or |t|<k,|z′|=0(z′=ε) \nit follows that βα=γξ,\nthen β=γ and A=B\n.\n```\nInformally, we abbreviate our string to the right of the initial nonterminal. If, given no more than k symbols from the unparsed part of the string, we can unambiguously determine what the tail of the resulting rule is folded into, then the grammar is LR(k).\n\nLR(k) means that:\nthe input string is processed from left to right (left-to-right parsing),\nthe rightmost output is performed,\nNo more than k symbols from the chain are used to make a decision (prediction by k tokens)."
    },
    {
        "question": "formal definitions of LR(k)-grammars",
        "answer": "\n\nDefinition:\n```\nLet Γ=⟨Σ,N,E,P⟩ be a context-free grammar. The augmented grammar obtained from Γ is the grammar Γ′=⟨Σ′,N′,E0,P′⟩, where Σ′=Σ;N′=N∪{E0};E0∉N;P′=P ∪{ Е0→Е}\n```\n```\nDefinition:\nLet Γ′=⟨Σ′,N′,E0,P′⟩ be the augmented grammar of the context-free grammar Γ. A grammar Γ is an LR(k)-grammar if for any two forward deductions it has the value:\nE0⇒∗βAtz⇒βαtz⇒∗w if |t|=k or |t|<k,|z|=0 (z=ε)\nE0⇒∗γBtz′⇒γξtz′⇒∗w′ if |t|=k or |t|<k,|z′|=0(z′=ε) \nit follows that βα=γξ,\nthen β=γ and A=B\n.\n```\nInformally, we right-fold our chain to the initial nonterminal. If, given at most k symbols from the unparsed part of the string, we can unambiguously determine what the tail of the resulting rule folds into, then the grammar is LR(k).\n\nLR(k) means that:\nthe input string is processed from left to right (left-to-right parsing),\nthe rightmost output is performed,\nAt most k symbols from the chain (k-token Lookahead) are used to make a decision."
    },
    {
        "question": "Does LR(k) grammar guarantee unambiguous parsing?",
        "answer": "\nAny LR(k) grammar by definition guarantees\nunambiguous parsing for a certain length of\nlookahead string, so no grammar with\nambiguous parsing is LR(k) for any\nvalue of k."
    },
    {
        "question": "Does LR(k) grammar guarantee unambiguous parsing?",
        "answer": "\nAny LR(k) grammar by definition guarantees\nunambiguous parsing for a certain lookahead string length, so no grammar with\nambiguous parsing is LR(k) for any\nvalue of k."
    },
    {
        "question": "Does LR(k) grammar guarantee unambiguous parsing?",
        "answer": "\nAny LR(k) grammar by definition guarantees\nunambiguous parsing for a particular string length\nanticipation, so no grammar with\nambiguous parsing is LR(k) for any\nvalue of k."
    },
    {
        "question": "Does LR(k) grammar guarantee unambiguous parsing?",
        "answer": "\nAny LR(k) grammar by definition guarantees\nunambiguous parsing for a certain\nexpected length of the search string, so no grammar with\nambiguous parsing is LR(k) for all\nvalues ​​of K."
    },
    {
        "question": "Pumping Lemma for DCFL",
        "answer": "\nLet L be a DCFL. Then there exists a pumping length p such that for all pairs of words w, w′ ∈ L such that\nw = xy & w′ = xz, |x| > p and the first letters y, z coincide,\none of the following two holds:\n1 there exists a pumping of only the prefix x (in the usual\nsense);\n2 \"\"\"\nthere exists a partition x = x1x2x3, y = y1y2y3,\nz = z1z2z3 such that |x2x3| ⩽ p, |x2| > 0, and\n∀i(x1(x2)^i x3y1(y2)^i y3 ∈ L & x1x2^ix3z1z2^iz3 ∈ L).\n\"\"\""
    },
    {
        "question": "Pumping Lemma for DCFL",
        "answer": "\nLet L be a DCFL. Then there exists a pumping length p such that for all pairs of words w, w′ ∈ L such that\nw = xy & w′ = xz, |x| > p and the first letters of y, z coincide,\none of the following two holds:\n1 there exists a pumping of only the prefix x (in the usual\nsense);\n2 \"\"\"\nthere exists a partition x = x1x2x3, y = y1y2y3,\nz = z1z2z3 such that |x2x3| ⩽ p, |x2| > 0, and\n∀i(x1(x2)^i x3y1(y2)^i y3 ∈ L & x1x2^ix3z1z2^iz3 ∈ L).\n\"\"\""
    },
    {
        "question": "Pumping Lemma for DCFL",
        "answer": "\nLet L DCFL. Then there exists a pumping length p such that for all pairs of words w, w′ ∈ L such that\nw = xy & w′ = xz, |x| > p and the first letters y, z are the same,\none of two things happens:\n1 only the prefix x is pumped (in the usual sense);\n2 \"\"\"\nthere is a partition x = x1x2x3, y = y1y2y3,\nz = z1z2z3 such that |x2x3| ⩽p, |x2| > 0, and\n∀i(x1(x2)^i x3y1(y2)^i y3 ∈ L & x1x2^ix3z1z2^iz3 ∈ L).\n\"\"\""
    },
    {
        "question": "Pumping Lemma for DCFL",
        "answer": "\nLet L be a DCFL. Then there exists a pumping length p such that for all pairs of words w, w′ ∈ L such that\nw = xy & w′ = xz, |x| > p and the first letters y, z coincide,\none of the following two conditions is satisfied:\n1 only the prefix x is pumped (in the usual sense);\n2 \"\"\"\nthere exists a partition x = x1x2x3, y = y1y2y3,\nz = z1z2z3 such that |x2x3| ⩽ p, |x2| > 0, and\n∀i(x1(x2)^i x3y1(y2)^i y3 ∈ L & x1x2^ix3z1z2^iz3 ∈ L).\n\"\"\""
    },
    {
        "question": "Degree of nondeterminism of language",
        "answer": "\nIf a PDA A can be decomposed into DPDAs between which there are at most k non-deterministic\ntransitions, but does not allow such a decomposition for i < k\ntransitions, we say that A defines a CF language with\nk-non-determinism"
    },
    {
        "question": "Degree of indeterminacy of language",
        "answer": "\nIf a PDA A can be decomposed into DPDAs between which there are at most k non-deterministic\ntransitions, but does not admit such a decomposition for i < k\ntransitions, we say that A defines a CF language with\nk-non-determinism"
    },
    {
        "question": "Degree of indeterminacy of language",
        "answer": "\nIf a PDA A can be decomposed into a DPDA between which there are at most k nondeterministic\ntransitions, but does not admit such a decomposition for i < k\ntransitions, we say that A defines a CF language with\nk-uncertainty"
    },
    {
        "question": "Degree of indeterminacy of language",
        "answer": "\nIf a CPC A can be decomposed into a DPDA between which there are at most k\nnondeterministic transitions, but does not admit such a decomposition for i < k\ntransitions, we say that A defines a CF language with\nk-nondeterminism"
    },
    {
        "question": "Fixing the non-determinism of the CS language",
        "answer": "\nLet L be a nondeterministic KS language and k > 0. Language L\n— k-correctable if there is an alphabet ∆, ∆ ∩ Σ = ∅\nand DCFL L(k) ⊆ (Σ ∪ ∆)∗ such that for h(∆) = ε,\nh(L(k)) = L and all words of the language L(k) contain at most k\nletters from ∆.\n\nLanguage L has the kth degree of nondeterminism ⇔ L\nk-correctable, but not k − 1-correctable."
    },
    {
        "question": "Fixing CS language non-determinism",
        "answer": "\nLet L be a nondeterministic language KS and k > 0. Language L\n— k-correctable if there is an alphabet ∆, ∆ ∩ Σ = ∅\nand DCFL L(k) ⊆ (Σ ∪ ∆)∗ such that for h(∆) = ε,\nh(L(k)) = L and all words of the language L(k) contain at most k\nletters from ∆.\n\nLanguage L has the kth degree of indeterminacy ⇔ L\nk-correctable, but not k − 1-correctable."
    },
    {
        "question": "Fixing CS language non-determinism",
        "answer": "\nLet L be a nondeterministic language KS and k > 0. Language L\n— k-correctable if there is an alphabet ∆, ∆ ∩ Σ = ∅\nand DCFL L(k) ⊆ (Σ ∪ ∆)∗ such that for h(∆) = ε\nh(L(k)) = L and all words of the language L(k) contain at most k\nletters ∆.\n\nLanguage L has the kth degree of uncertainty ⇔ L\nk-correctable, but not k − 1-correctable."
    },
    {
        "question": "Fix non-determinism of CS language.",
        "answer": "\nLet L be a nondeterministic language KS and k > 0. Language L\n— k-correctable if there is an alphabet ∆, ∆ ∩ Σ = ∅\nand DCFL L(k) ⊆ (Σ ∪ ∆)∗ such that for h(∆) = ε\nh(L(k)) = L and all words of the language L(k) contain at most k\nletters ∆.\n\nLanguage L has the kth degree of indeterminacy ⇔ L\nk-correctable, but not k − 1-correctable."
    },
    {
        "question": "Alternating Finite Automata",
        "answer": "\nAlternating Finite Automata\ncontain nodes of two types: conjunctive and disjunctive\n(∀-nodes and ∃-nodes, respectively).\n\nAll AFA languages ​​are regular, since regular languages\nare closed under intersections.\n\nThey allow easy modeling of lookahead expressions\nunder iteration, or lookahead expressions inside\nlookahead expressions. For example, the automaton \n(a | b(? = (b∗ab∗a)∗b∗$))∗."
    },
    {
        "question": "Finite state machines with alternation",
        "answer": "\nAlternating finite automata\ncontain nodes of two types: conjunctive and disjunctive\n(∀-nodes and ∃-nodes, respectively).\n\nAll AFA languages ​​are regular, since regular languages\nare intersection-closed.\n\nThey allow easy modeling of lookahead expressions\nduring iteration, or lookahead expressions inside lookahead expressions. For example, the automaton\n(a | b(? = (b∗ab∗a)∗b∗$))∗."
    },
    {
        "question": "Alternating finite state machines",
        "answer": "\nAlternating finite automata\ncontain two types of nodes: junctions and disjunctives.\n(∀-nodes and ∃-nodes, respectively).\n\nAll AFA languages ​​are regular, because ordinary languages\nare closed under intersections.\n\nThey allow easy modeling of search expressions.\nby iterating or previewing expressions in lookahead expressions. For example, automatic\n(a | b(? = (b∗ab∗a)∗b∗$))∗."
    },
    {
        "question": "Variable finite state machines",
        "answer": "\nVariable finite automata\ncontain nodes of two types: junction and disjunctive.\n(∀-nodes and ∃-nodes, respectively).\n\nAll AFA languages ​​are regular, because ordinary languages\nare closed under intersections.\n\nThey simplify modeling of expectation expressions.\nin iteration or lookahead expressions inside\nan expectation expression. For example, the automaton\n(a | b(? = (b∗ab∗a)∗b∗$))∗."
    },
    {
        "question": "Theoretical collapse of linear parsers",
        "answer": "\nTheorem:\nFor every language from class DCFL, there is an SLR(1)-grammar that recognizes it.\nIt follows from the theorem:\nFor every language from class DCFL, there is an LR(k)-grammar that recognizes it.\n\nThe theorem that for every language from class DCFL (Deterministic Context-Free Languages) there is an SLR(1)-grammar that recognizes it is really important in the theory of formal languages ​​and automata. It states that it is possible to construct a deterministic parser for languages ​​that can be described using context-free grammars."
    },
    {
        "question": "Theoretical collapse of linear parsers",
        "answer": "\nTheorem:\nFor every language in class DCFL, there is an SLR(1)-grammar that recognizes it.\nThis follows from the theorem:\nFor every language in class DCFL, there is an LR(k)-grammar that recognizes it.\n\nThe theorem that for every language in class DCFL (Deterministic Context-Free Languages) there is an SLR(1)-grammar that recognizes it is really important in the theory of formal languages ​​and automata. It states that it is possible to build a deterministic parser for languages ​​that can be described using context-free grammars."
    },
    {
        "question": "The theoretical collapse of linear analyzers",
        "answer": "\nTheorem:\nFor every DCFL language, there is an SLR(1) grammar that recognizes it.\nThis follows from the theorem:\nFor every DCFL language, there is an LR(k) grammar that recognizes it.\n\nThe theorem that for every DCFL language (Deterministic Context-Free Languages) there is an SLR(1) grammar that recognizes it is really important in the theory of formal languages ​​and automata. It states that one can build a deterministic parser for languages ​​that can be described by context-free grammars."
    },
    {
        "question": "The theoretical collapse of linear analyzers",
        "answer": "\nTheorem:\nFor every DCFL language, there is an SLR(1) grammar that recognizes it.\nThis follows from the theorem:\nFor every DCFL language, there is an LR(k) grammar that recognizes it.\n\nThe theorem that for every DCFL language (Deterministic Context-Free Languages) there is an SLR(1) grammar that recognizes it is really important in formal language and automata theory. It says that one can build a deterministic parser for languages ​​that can be described by context-free grammars."
    },
    {
        "question": "Trellis machine gun",
        "answer": "\nIf the rewriting starts from the \"leaves\", but the structure\nis a network, then we get the so-called \"Trellis automaton\".\nThe languages ​​recognized by such automata are precisely\nlinear conjunctive languages.\nTransformation of the Trellis automaton into a conjunctive\ngrammar (it will be linear, since in each basic right-hand\npart there can be, at most, only one non-terminal):\nS → Afinal\nAinit(a) → a, a ∈ Σ\nAδ(q1,q2) → Aq1 c & bAq2 , ∀b, c ∈ Σ"
    },
    {
        "question": "Machine gun \"Trellis\"",
        "answer": "\nIf the rewriting starts with \"leaves\", but the structure\nis a network, then we get the so-called \"Trellis automaton\".\nThe languages ​​recognized by such automata are precisely\nlinear conjunctive languages.\nTransformation of the Trellis automaton into a conjunctive\ngrammar (it will be linear, since each basic right-hand side can have, at most, only one nonterminal):\nS → Afinal\nAinit(a) → a, a ∈ Σ\nAδ(q1,q2) → Aq1 c & bAq2 , ∀b, c ∈ Σ"
    },
    {
        "question": "Machine gun \"Trellis\"",
        "answer": "\nIf the rewriting starts with \"leaves\", but the structure\nis a network, then we get what is called a \"lattice machine\".\nThe languages ​​recognized by such machines are precisely\nlinear copula languages.\nTransformation of a lattice automaton into a copula.\ngrammar (it will be linear, since each basic right member can have at most one nonterminal):\nC → Final\nAinit(a) → a, a € Σ\nAδ(q1,q2) → Aq1 c & bAq2 , ∀b, c ∈ Σ"
    },
    {
        "question": "Lattice machine gun",
        "answer": "\nIf the rewriting starts with \"sheets\", but the structure\nis a network, then we get what we call a \"lattice automaton\".\nThe languages ​​recognized by such automata are precisely\nlinear copula languages.\nTransformation of a lattice automaton into a grammar\nconjunctive (it will be linear, since each main direct part can have at most one nonterminal):\nC → Final\nAinit(a) → a, a € Σ\nAδ(q1,q2) → Aq1 c & bAq2 , ∀b, c ∈ Σ"
    },
    {
        "question": "Jumping Lemma",
        "answer": "\nk − MFA is deterministic if\n∀q ∈ Q, b ∈ Σ(| $$\\bigcup_{i=1}^{k} δ(q, i)$$|\n+ |δ(q, b)| ⩽ 1). DMFL is a language for which there exists a DMFA\n\nA language L ∈REGEX is deterministic if either it\nis regular or ∀m∃n, pn, vn such that n ⩾ m,\npn, vn ∈ Σ+, where:\n|vn| = n;\nvn is a subword of pn;\npnvn is a prefix of some word in L ;\n∀u ∈ Σ+(pnu ∈ L ⇒ vn is a prefix of u)."
    },
    {
        "question": "Lemma on jumps",
        "answer": "\nk − MFA is deterministic if\n∀q ∈ Q, b ∈ Σ(| $$\\bigcup_{i=1}^{k} δ(q, i)$$|\n+ |δ(q, b)| ⩽ 1). DMFL is the language for which there is a DMFA\n\nA language L ∈ REGEX is deterministic if it\neither regular or ∀m∃n, pn, vn such that n ⩾ m,\npn, vn ∈ Σ+, where:\n|vn| = n;\nvn — subword pn;\npnvn is a prefix of some word in L;\n∀u ∈ Σ+(pnu ∈ L ⇒ vn is the prefix of u)."
    },
    {
        "question": "Lemma on the jump",
        "answer": "\nk - MFA is deterministic if\n∀q ∈ Q, b ∈ Σ(| $$\\bigcup_{i=1}^{k} δ(q, i)$$|\n+ |δ(q,b)| ⩽1). DMFL is a language for which there is a DMFA.\n\nA language L ∈ REGEX is deterministic if it is\neither regular or ∀m∃n, pn, vn such that n ⩾ m,\npn, vn ∈ Σ+, where:\n|vn| = n;\nvn is a subword of pn;\npnvn is a prefix of an L-word;\n∀u ∈ Σ+ (pnu ∈ L ⇒ vn is a prefix of u)."
    },
    {
        "question": "Transition Lemma",
        "answer": "\nk — MFA is deterministic if\n∀q ∈ Q, b ∈ Σ(| $$\\bigcup_{i=1}^{k} δ(q, i)$$|\n+ |δ(q, b)| ⩽ 1). DMFL is a language for which there is a DMFA.\n\nA language L ∈REGEX is deterministic if it is\nregular or ∀m∃n, pn, vn such that n ⩾ m,\npn, vn ∈ Σ+, where:\n|вн| = п;\nvn is a subword of pn;\npnvn is a one-word prefix of L;\n∀u ∈ Σ+ (pnu ∈ L ⇒ vn is a prefix of u)."
    },
    {
        "question": "Attribute grammars",
        "answer": "\nLet A0 → A1 . . . An be a rule of a CFG. We assign a finite number of attribute properties to it.\nSynthetic attributes are calculated for A0 by attributes\nA1,. . . ,An;\nInherited attributes are calculated for Ai by attributes\nA0,. . . , Ai−1,Ai+1,. . . ,An. Usually — by attributes A0 and A1,\n. . . , Ai−1 (left-hand attribute grammars).\nRepeated nonterminals when assigning attributes\nare indexed by occurrences in the rule from left to right. That is,\nfor example, if the rule N → N − N is given, then the equation for\nattributes N0.attr = N1.attr − N2.attr will mean that\nthe parent attribute is the attribute of the left child minus the attribute\nof the right child, labeled by non-terminals N.\nNon-repeated non-terminals in equations for attributes are usually not\nindexed\n\nExample AG for {anbncn}\nThe attribute of the non-terminal iter semantically means the number\nof iterations. In order not to mix synthetic and inherited\nattributes, we will also introduce the attribute inh_iter, which means the same\nthing, but inherited from top to bottom along the parse tree, and not from bottom\nup. Here == is a predicate; := is an assignment operation.\nSynthetic version:\nS → AT ; T .iter == A.iter\nA → aA ; A0.iter := A1.iter + 1\nA → ε; A.iter := 0\nT → bT c ; T0.iter := T1.iter + 1\nT → ε; T .iter := 0\nInheritance option:\nS → AT; B.inh_iter := A.iter\nA → aA ; A0.iter := A1.iter + 1\nA → ε; A.iter := 0\nT → bT c ; T1.inh_iter := T0.inh_iter − 1\nT → ε; T .inh_iter == 0"
    },
    {
        "question": "Attributive grammars",
        "answer": "\nLet A0 → A1 . . . An be a CFG rule. Assign it a finite number of attribute properties.\nSynthetic attributes are computed for A0 by attributes\nA1,. . . ,An;\nInherited attributes are computed for Ai by attributes\nA0,. . . , Ai−1,Ai+1,. . . ,An. Usually — by attributes A0 and A1,\n. . . , Ai−1 (left attribute grammars).\nRepeated nonterminals in attribute assignment\nare indexed by occurrences in the rule from left to right. That is,\nfor example, if the rule N → N − N is given, then the equation for\nattributes N0.attr = N1.attr − N2.attr will mean that\nthe parent attribute is the attribute of the left child minus the attribute\nof the right child labeled by nonterminals of N.\nDistinct nonterminals in equations for attributes are usually not\nindexed\n\nExample AG for {anbncn}\nThe attribute of the nonterminal iter semantically means the number of\niterations. To avoid mixing synthetic and inherited\nattributes, we also introduce the attribute inh_iter, which means the same thing,\nbut is inherited from top to bottom of the parse tree, not from bottom to top. Here == is a predicate; := is an assignment operator.\nSynthetic version:\nS → AT ; T .iter == A.iter\nA → aA; A0.iter := A1.iter + 1\nA → ε; A.iter := 0\nT → bT c ; T0.iter := T1.iter + 1\nT → ε; T .iter := 0\nInheritance variant:\nC → AT; B.inh_iter := A.iter\nA → aA; A0.iter := A1.iter + 1\nA → ε; A.iter := 0\nT → bT c ; T1.inh_iter := T0.inh_iter - 1\nT → ε; T .inh_iter == 0"
    },
    {
        "question": "Attributive grammars",
        "answer": "\nLet A0 → A1. . . This will be a CFG rule. Let's assign a finite number of attribute properties to it.\nSynthetic attributes are calculated for A0 by attributes\nA1,. . . ,A;\nInherited attributes are calculated for Ai by attributes\nA0,. . . ,Ai−1,Ai+1,. . . ,A. Usually – by attributes A0 and A1,\n. . . , Ai−1 (left attribute grammars).\nRepeat nonterminals when assigning attributes\nindexed by rule occurrences from left to right. This,\nfor example, given the rule N → N − N, then the equation for\nattributes N0.attr = N1.attr − N2.attr will mean that\nthe parent attribute is the left child attribute minus the right child attribute, marked by the nonterminals of N.\nDistinct nonterminals in attribute equations are usually not\nindexed\n\nGA example for {anbnnc}\nThe nonterminal attribute iter semantically means the number of.\niterations. To avoid mixing synthetic and inherited\nattributes, we also introduce the attribute inh_iter, which means the same thing,\nbut is inherited up the parse tree instead of down. Here == is a predicate; := is an assignment operator.\nSynthetic version:\nC → K; T .iter == A.iter\nA → aA; A0.iter := A1.iter + 1\nA → ε; A.iter := 0\nT → bTc; T0.iter := T1.iter + 1\nT → ε; T .iter := 0\nInheritance Possibility:\nC → Where; B.inh_iter := A.iter\nA → aA; A0.iter := A1.iter + 1\nA → ε; A.iter := 0\nT → bTc; T1.inh_iter := T0.inh_iter - 1\nT → ε; T .inh_iter == 0"
    },
    {
        "question": "Attribute grammars",
        "answer": "\nLet A0 → A1. . . Year be a CFG rule. It is assigned a finite number of attribute properties.\nSynthetic attributes are calculated for A0 by the attributes\nA1,. . . ,Year;\nInherited attributes are calculated for Ai by the attributes\nA0,. . . , Ai−1,Ai+1,. . . ,Year. In the general case, by the attributes A0 and A1,\n. . . , Ai−1 (left attribute grammars).\nRepeated nonterminals are indexed by occurrences in the rule from left to right when assigning attributes. That is,\nfor example, if the rule N → N − N is given, then the equation for\nattributes N0.attr = N1.attr − N2.attr will mean that\nthe parent attribute is the left child attribute minus the attribute\nof the right child element labeled by nonterminals of N.\nNonterminals that are not repeated in equations for attributes are usually not\nindexed\n\nExample AG for {anbnnc}\nThe nonterminal attribute iter semantically means the number of\niterations. To avoid mixing synthetic and inherited attributes, we will also introduce the attribute inh_iter, which means the same\nthing, but is inherited from top to bottom in the parse tree, rather than bottom to top. Here == is a predicate; := is an assignment operator.\nSynthetic version:\nC → AT; T .iter == A.iter\nA → aA; A0.iter := A1.iter + 1\nA → ε; A.iter := 0\nT → bT c ; T0.iter := T1.iter + 1\nT → ε; T .iter := 0\nInheritance:\nC → K; B.inh_iter := A.iter\nA → aA; A0.iter := A1.iter + 1\nA → ε; A.iter := 0\nT → bT c ; T1.inh_iter := T0.inh_iter - 1\nT → ε; T .inh_iter == 0"
    },
    {
        "question": "Is DMFL closed under intersection with a regular language?",
        "answer": "\nClosed under the intersection with the regular language, and\nalso under the complement, but only if it is allowed\nto introduce unsuccessful transitions by backreferences;"
    },
    {
        "question": "Is DMFL closed under the intersection with ordinary language?",
        "answer": "\nClosed under regular language intersection, and also under addition, but only if it is allowed\nto enter unsuccessful transitions on backlinks;"
    },
    {
        "question": "Are DMFLs closed to common language?",
        "answer": "\nClosed when crossing with normal language, and also when supplementing, but only if allowed.\nintroduce unsuccessful transitions on backlinks;"
    },
    {
        "question": "Is DMFL closed when it crosses with normal language?",
        "answer": "\nClosed under the crossroad with the usual language, and\nalso under the addition, but only if it is allowed\nto enter unsuccessful transitions on backlinks;"
    },
    {
        "question": "Is DMFL closed under union?",
        "answer": "\nThey are not closed under union (and even under union with regular languages), and under intersection with each other."
    },
    {
        "question": "Is DMFL closed by union decision?",
        "answer": "\nThey are not closed with respect to the union (and even with respect to the union with ordinary languages) and with respect to intersection with each other."
    },
    {
        "question": "Is DMFL closed by union decision?",
        "answer": "\nThey are not closed with respect to union (or even with respect to union with ordinary languages) and with respect to intersection with each other."
    },
    {
        "question": "Is DMFL closed under the union?",
        "answer": "\nThey are not closed either in the case of unification (and even in the case of unification with ordinary languages), or in the case of intersection between them."
    },
    {
        "question": "Linearization of regular expressions is",
        "answer": "\n\nIf a regular expression r contains n occurrences of letters of the alphabet ∑, then the linearized regular expression Linearize(r) is obtained from r by assigning index i to the ith letter in r.\nConsider the regular expression:(ba | b)aa(a | ab)*\nIts linearized version:(b_1a_2 | b_3)a_4a_5(a_6 | a_7b_8)*"
    },
    {
        "question": "Linearization of regular expressions is",
        "answer": "\n\nIf a regular expression r contains n occurrences of letters of the alphabet ∑, then the linearized regular expression Linearize(r) is obtained from r by assigning index i to the i-th letter in r.\nConsider the regular expression:(ba | b)aa(a | ab)*\nIts linearized version:(b_1a_2 | b_3)a_4a_5(a_6 | a_7b_8)*"
    },
    {
        "question": "Linearization of regular expressions",
        "answer": "\n\nIf a regular expression r contains n occurrences of letters of the alphabet ∑, then the linearized regular expression Linearize(r) is obtained from r by assigning index i to the i-th letter of r.\nConsider the regular expression:(ba | b)aa(a | ab)*\nIts linearized version: (b_1a_2 | b_3)a_4a_5(a_6 | a_7b_8)*"
    },
    {
        "question": "Linearization of regular expressions",
        "answer": "\n\nIf a regular expression r contains n occurrences of letters of the alphabet ∑, then the linearized regular expression Linearize(r) is obtained from r by assigning index i to the i-th letter of r.\nConsider the regular expression: (ba | b)aa(a | ab)*\nIts linearized version: (b_1a_2 | b_3)a_4a_5(a_6 | a_7b_8)*"
    },
    {
        "question": "First Last Follow Set for Regular Expressions",
        "answer": "\n\nLet r be a regular expression. Then: The set First is the set of letters that a word from L(r) can begin with (if ε ∈ L(r), then it is formally added to First);\nThe set Last is the set of letters that a word from L(r) can end with\nThe set Follow(b) is the set of letters that b can precede. That is, { d ∈ ∑ | ∃ w_1, w_2 (w_1bdw_2 ∈ L(r)) }\n\nr = (ba | b)aa(a | ab)* is a regular expression First(r) = {b}\nLast(r) = {a, b}\nFollow_r(a) = {a, b}; Follow)_r(b) = {a}"
    },
    {
        "question": "First Last Next Set for Regular Expressions",
        "answer": "\n\nLet r be a regular expression. Then: The set First is the set of letters that a word in L(r) may begin with (if ε ∈ L(r), then it is formally added to First);\nThe set Last is the set of letters that a word in L(r) may end with\nThe set Follow(b) is the set of letters that may be preceded by b. That is, { d ∈ ∑ | ∃ w_1, w_2 (w_1bdw_2 ∈ L(r)) }\n\nr = (ba | b)aa(a | ab)* is a regular expression First(r) = {b}\nLast(r) = {a, b}\nFollow_r(a) = {a, b}; Follow)_r(b) = {a}"
    },
    {
        "question": "First Last Next Set for regular expressions",
        "answer": "\n\nLet r be a regular expression. Then: The set First is the set of letters that a word from L(r) may begin with (if ε ∈ L(r), then it is formally added to First);\nThe set Last is the set of letters that a word from L(r) may end with.\nThe set Follow(b) is the set of letters that may be preceded by the letter b. In other words, { d ∈ ∑ | ∃ w_1, w_2 (w_1bdw_2 ∈ L(r)) }\n\nr = (ba | b)aa(a | ab)* is a regular expression First(r) = {b}\nLast(r) = {a, b}\nFollow_r(a) = {a, b}; Follow)_r(b) = {a}"
    },
    {
        "question": "First Last Follow Together for Regular Expressions",
        "answer": "\n\nLet r be a regular expression. Then: The set First is the set of letters that a word from L(r) may begin with (if ε ∈ L(r), then it is formally added to First);\nThe set Last is the set of letters that a word from L(r) may end with.\nThe set Follow(b) is the set of letters that may be preceded by b. That is, { d ∈ ∑ | ∃ w_1, w_2 (w_1bdw_2 ∈ L(r)) }\n\nr = (ba | b)aa(a | ab)* is a regular expression First(r) = {b}\nLast(r) = {a, b}\nFollow_r(a) = {a, b}; Follow)_r(b) = {a}"
    },
    {
        "question": "Minimization of DFA Hopcroft algorithm",
        "answer": "\nHopcroft's algorithm for minimizing a DFA (Deterministic Finite Automaton) is an algorithm that finds the minimum DFA equivalent to a given DFA.\nThe Hopcroft algorithm works as follows:\nStep 1: Initialization\nThe initial partition consists of two sets:\nQ_accept (accepting states)\nQ_reject (non-accepting states)\nAll states of the DFA are divided into these two subsets. If there are states in the automaton that are neither accepting nor non-accepting, they are also included in the Q_reject set.\nStep 2: Set partitioning\nCreate a queue in which we place all sets from the current partition.\nWhile the queue is not empty:\nRemove set A from the queue.\nFor each symbol c in the alphabet:\nPartition A into two subsets:\nB_c: states that, upon reading c, go to a state in A.\nB_{~c}: states that, upon reading c, go to states not in A.\nIf B_c and B_{~c} are both nonzero, then replace A with two new sets in the partition.\nIf A has been partitioned, add the new subsets to the queue for further processing.\nStep 3: The algorithm stops when no more partitions can be performed.\nTheorem: Hopcroft's algorithm stops and produces a minimal DFA.\nHopcroft's algorithm is an algorithm for finding a minimal DFA equivalent to a given DFA, and is used in automata theory and regular languages."
    },
    {
        "question": "Minimization of Hopcroft's DFA Algorithm",
        "answer": "\nHopcroft's algorithm for minimizing a DFA (deterministic finite automaton) is an algorithm that finds the minimum DFA equivalent to a given DFA.\nThe Hopcroft algorithm works as follows:\nStep 1: Initialization\nThe initial partition consists of two sets:\nQ_accept (accepting states)\nQ_reject (non-accepting states)\nAll states of the DFA are divided into these two subsets. If there are states in the automaton that are neither accepting nor non-accepting, they are also included in the Q_reject set.\nStep 2: Partitioning the sets\nCreate a queue in which we place all sets from the current partition.\nWhile the queue is not empty:\nRemove set A from the queue.\nFor each symbol c in the alphabet:\nPartition A into two subsets:\nB_c: states that when reading c, go to a state in A.\nB_{~c}: states that when reading c, go to states not in A.\nIf B_c and B_{~c} are both nonzero, then replace A with the two new sets in the partition.\nIf A has been partitioned, add the new subsets to the queue for further processing.\nStep 3: The algorithm stops when no more partitions can be performed.\nTheorem: Hopcroft's algorithm stops and produces a minimal DFA.\nHopcroft's algorithm is an algorithm for finding a minimal DFA equivalent to a given DFA, and is used in automata theory and regular languages."
    },
    {
        "question": "Minimization of Hopcroft's DFA Algorithm",
        "answer": "\nHopcroft's deterministic finite automaton (DFA) minimization algorithm is an algorithm that finds the minimum DFA equivalent to a given DFA.\nHopcroft's algorithm works as follows:\nStep 1: Initialization\nThe initial score consists of two sets:\nQ_accept (accepting states)\nQ_reject (non-accepting states)\nAll states of the DFA are divided into these two subsets. If the automaton has states that are neither accepting nor non-accepting, they are also included in the Q_reject set.\nStep 2: Set the partition\nCreate a queue into which we place all ensembles from the current partition.\nWhile the queue is not empty:\nWe remove set A from the queue.\nFor each symbol c in the alphabet:\nPartition A into two subsets:\nB_c: indicates that reading c switches to state A.\nB_{~c}: indicates that reading c accesses states that are not in A.\nIf B_c and B_{~c} are both nonzero, replace A with the two new sets in the partition.\nIf A has been partitioned, queue the new subsets for further processing.\nStep 3: The algorithm stops when no further partitioning is possible.\nTheorem: Hopcroft's algorithm stops and produces a minimal DFA.\nHopcroft's algorithm is an algorithm for finding a minimal DFA equivalent to a given DFA, and is used in automata theory and regular languages."
    },
    {
        "question": "Minimization of Hopcroft's DFA Algorithm",
        "answer": "\nHopcroft's algorithm for minimizing an AFD (deterministic finite automaton) is an algorithm that finds the minimum AFD equivalent to a given AFD.\nThe Hopcroft algorithm works as follows:\nStep 1: Initialization\nThe initial score consists of two sets:\nQ_accept (accepting states)\nQ_reject (non-accepting states)\nAll AFD states are divided into these two subsets. If the automaton has non-accepting and non-rejecting states, they are also included in the Q_reject set.\nStep 2: Splitting the sets\nCreate a queue where we put all ensembles from the current partition.\nWhile the queue is not empty:\nRemove set A from the queue.\nFor each symbol c in the alphabet:\nPartition A into two subsets:\nB_c: indicates that reading c switches to state A.\nB_{~c}: indicates that reading c switches to states other than A.\nIf B_c and B_{~c} are nonzero, replace A with the two new sets in the partition.\nIf A has been split, add the new subsets to the queue for further processing.\nStep 3: The algorithm stops when no more splits can be made.\nTheorem: Hopcroft's algorithm stops and produces a minimal DFA.\nHopcroft's algorithm is an algorithm for finding a minimal DFA equivalent to a given DFA, and is used in automata theory and regular languages."
    },
    {
        "question": "what is LTS(Labelled Transition System)",
        "answer": "\nLTS is a triple <S,∑,Q> where S is the set of states, ∑ is the set of labels, Q is the set of transitions <triples from S x ∑ x S>. LTS are similar to automata but allow infinite sets of S and Q. They also do not have initial and final \nstates"
    },
    {
        "question": "what is LTS (Labeled Transition System)",
        "answer": "\nLTS is a triple <S,∑,Q>, where S is the set of states, ∑ is the set of labels, Q is the set of transitions <of a triple of S x ∑ x S>. LTS are similar to automata, but allow infinite sets of S and Q. They also do not have initial and final \nstates"
    },
    {
        "question": "what is LTS (Labeled Transition System)",
        "answer": "\nLTS is a triple <S,∑,Q>, where S is the set of states, ∑ is the set of labels, Q is the set of transitions <triple S x ∑ x S>. LTS are similar to automata, but allow infinite sets of S and Q. They also do not have initial and final values.\nstates"
    },
    {
        "question": "What is LTS (Labeled Transition System)",
        "answer": "\nLTS is a triple <S,∑,Q>, where S is a set of states, ∑ is a set of labels, Q is a set of transitions <triple S x ∑ x S>. LTS are similar to automata, but allow infinite sets of S and Q. They also have no initial and final states."
    },
    {
        "question": "If the language L is regular, then it is defined by a finite nondeterministic automaton. Prove",
        "answer": " \n\nLet an alphabet Σ of terminal symbols be given.\nThe theorem is proved in the same way as regular sets are constructed. Our task is to prove that any regular set can be defined by a finite nondeterministic automaton. First, we will find a scheme for constructing an automaton for constant regular sets, i.e. for letters of the alphabet ∑, then we will move on to complications.\nLet us construct an automaton that recognizes some letter a ∈ ∑:\nM = <Q, Σ, δ(x,y), q0, F>\nQ = {q0}\nF = {q0}\nδ(q0, a) = q0.\nThat is, this is an automaton that recognizes the language {a}. For the language {ε}, the function will be empty, everything else will be the same. And for the language ∅, the set of accepting states will also be empty, F = ∅.\nThere should have been a picture with a diagram here, but there isn't one. The diagram of the automaton is a regular directed graph, where the vertices are states from Q, and those from F are also highlighted in some color/outline/etc. The edge q1 → q2 should always be labeled with a letter from a ∈ ∑, which means that (q1, a) = q2. That's it.\nWe need to build an automaton that recognizes the language L1 + L2\nBy definition: If the languages ​​L1 and L2 are regular, then L1 + L2 is a regular language. Let L1 be recognized by the automaton M1=<Q1, Σ, δ1, q1, F1>, and L2 be recognized by the automaton M1=<Q2, Σ, δ2, q2, F2> Without loss of generality, we assume that Q1 ∩ Q2 = ∅ (achieved by regular renaming). Let us introduce a new state q0.\nLet M recognize L1+L2. M=<Q, Σ, δ, q0, F>\nQ=Q1UQ2U{q0}\nδ(q0, a) = δ1(q1,a) U δ2(q2,a)\nδ(q, a)= δ1(q, a); q∈Q1\nδ2(q, a), q∈Q2\nF = F1 U F2\nWe need to build an automaton that recognizes the language L1 L2\nLet M recognize L1L2. M=<Q, Σ, δ, q1, F>\nδ1(q, a), if q∈Q1\\F1\nδ(q, a)= δ1(q, a) U δ2(q2,a); q∈F1\nδ2(q, a), q∈Q2\nF=F2\n1)If 𝑞_2∈𝐹, then the automaton completes recognition at state 𝑞_2, and if 𝑞_1∈𝐹 (that is, the automaton for 𝐿_1 also completes recognition in an accepting state), then we can accept the string as correct, even if it ended in 𝑞_2. In this case: 𝐹=𝐹_1∪𝐹_2. \nThat is, the accepting states will be both the states of the automaton for 𝐿_1 and the states of the automaton for 𝐿_2, because after completing the recognition of 𝐿_1, the automaton can be ready to accept the strings of 𝐿_2, and vice versa.\n2) If 𝑞_2∉𝐹​, then the automaton for 𝐿_2 does not complete recognition in the accepting state. In this case, the only accepting state will be 𝐹_2 , since 𝐿_2 must complete recognition on states from 𝐹_2 that correspond to the completion of recognition of a string from 𝐿_2: 𝐹=𝐹_2\nThat is, in the case when 𝐿_1 is recognized in an accepting state, but 𝐿_2 is not, we cannot complete recognition in states 𝐹_1, but only in 𝐹_2\n\nWe need to build an automaton that recognizes the language L1*\nM=<Q, Σ, δ, q0, F>\nF=F1 U {q0}\nδ1(q1, a), if q=q0\nδ(q, a)= δ1(q, a); q∉F1\nδ1(q1,a) U δ1(q,a), q∈F"
    },
    {
        "question": "If the language L is regular, then it is defined by a finite nondeterministic automaton. Prove",
        "answer": " \n\nLet an alphabet Σ of terminal symbols be given.\nThe theorem is proved in the same way as regular sets are constructed. Our task is to prove that any regular set can be defined by a finite nondeterministic automaton. First, we will find a scheme for constructing an automaton for constant regular sets, i.e. for letters of the alphabet ∑, then we will move on to complications.\nLet us construct an automaton that recognizes some letter a ∈ ∑:\nM = <Q, Σ, δ(x,y), q0, F>\nQ = {q0}\nF = {q0}\nδ(q0, a) = q0.\nThat is, this is an automaton that recognizes the language {a}. For the language {ε}, the function will be empty, everything else will be the same. And for the language ∅, the set of accepting states will also be empty, F = ∅.\nThere should have been a picture with a diagram here, but there isn't one. The diagram of the automaton is a regular directed graph, where the vertices are states from Q, and the vertices from F are also highlighted with some color/outline/etc. The edge q1 → q2 must always be labeled with a letter from a ∈ ∑, which means that (q1, a) = q2. That's it.\nWe need to build an automaton that recognizes the language L1 + L2\nBy definition: if the languages ​​L1 and L2 are regular, then L1 + L2 is a regular language. Let L1 be recognized by the automaton M1=<Q1, Σ, δ1, q1, F1>, and L2 be recognized by the automaton M1=<Q2, Σ, δ2, q2, F2> Without loss of generality, we will assume that Q1 ∩ Q2 = ∅ (achieved by regular renaming). Let us introduce a new state q0.\nLet M recognize L1+L2. M=<Q, Σ, δ, q0, F>\nQ=Q1UQ2U{q0}\nδ(q0, a) = δ1(q1,a) U δ2(q2,a)\nδ(q, a)= δ1(q, a); q∈Q1\nδ2(q, a), q∈Q2\nF = F1 U F2\nWe need to build an automaton that recognizes the L1 L2 language\nLet M recognize L1L2. M=<Q, Σ, δ, q1, F>\nδ1(q, a), if q∈Q1\\F1\nδ(q, a)= δ1(q, a) U δ2(q2,a); q∈F1\nδ2(q, a), q∈Q2\nF=F2\n1)If 𝑞_2∈𝐹, then the automaton completes recognition in state 𝑞_2, and if 𝑞_1∈𝐹 (that is, the automaton for 𝐿_1 also completes recognition in the accepting state), then we can accept the string as correct, even if it ended in 𝑞_2. In this case: 𝐹=𝐹_1∪𝐹_2. \nThat is, the accepting states will be both the states of the automaton for 𝐿_1 and the states of the automaton for 𝐿_2, because after completing the recognition of 𝐿_1, the automaton can be ready to accept the strings of 𝐿_2, and vice versa.\n2) If 𝑞_2∉𝐹​, then the automaton for 𝐿_2 does not complete recognition in the accepting state. In this case, the only accepting state is 𝐹_2 , since 𝐿_2 must complete recognition on states from 𝐹_2 that correspond to the completion of recognition of a string from 𝐿_2: 𝐹=𝐹_2\nThat is, in the case when 𝐿_1 is recognized in the accepting state, but 𝐿_2 is not, we cannot complete recognition in the states of 𝐹_1, but only in 𝐹_2\n\nWe need to build an automaton that recognizes the language L1*\nM=<Q, Σ, δ, q0, F>\nF=F1 U {q0}\nδ1(q1, a), if q=q0\nδ(q, a)= δ1(q, a); q∉F1\nδ1(q1,a) U δ1(q,a), q∈F"
    },
    {
        "question": "If a language L is regular, then it is defined by a nondeterministic finite automaton. Prove",
        "answer": "\n\nConsider the alphabet of terminal symbols Σ.\nThe theorem is proved in the same way as the construction of regular sets. Our task is to prove that any regular set can be defined by a non-deterministic finite automaton. First, we find a scheme for constructing an automaton for constant regular sets, that is, for the letters of the alphabet ∑, and then we move on to complications.\nLet's construct an automaton that recognizes the letter a ∈ ∑:\nM = <Q, Σ, δ(x,y), q0, F>\nQ = {q0}\nF = {q0}\nδ(q0, a) = q0.\nIn other words, this is an automaton that recognizes the language {a}. For the language {ε}, the function will be empty, everything else is the same. And for the language ∅, the set of receptor states will also be empty, F = ∅.\nThere should be a picture with a scheme here, but there isn't one. The diagram of the automaton is a regular directed graph whose vertices are the states of Q, and the vertices of F are also highlighted in color/outline/etc. The edge q1 → q2 must always be labeled with a ∈ . ∑, which means that (q1,a) = q2. That's it.\nWe need to build an automaton that recognizes the language L1 + L2.\nBy definition: if languages ​​L1 and L2 are regular, then L1 + L2 is a regular language. Let L1 be recognized by the automaton M1 = <Q1, Σ, δ1, q1, F1>, and L2 be recognized by the automaton M1 = <Q2, Σ, δ2, q2, F2>. Without loss of generality, we assume that Q1 ∩ Q2 = ∅ (obtained by a regular change of name). Introduce a new state q0.\nLet M recognize L1 + L2. M=<Q, Σ, δ, q0, F>\nQ=Q1UQ2U{q0}\nδ(q0,a) = δ1(q1,a) U δ2(q2,a)\nδ(q, a)= δ1(q, a); qεQ1\nδ2(q, a), qεQ2\nФ = Ф1 У Ф2\nWe need to build an automaton that recognizes the L1 L2 language.\nLet M recognize L1L2. M=<Q, Σ, δ, q1, F>\nδ1(q, a), if qεQ1\\F1\nδ(q, a)= δ1(q, a) U δ2(q2,a); qεF1\nδ2(q, a), qεQ2\nФ=Ф2\n1) If 𝑞_2ε𝐹, then the automaton completes recognition in state 𝑞_2, and if 𝑞_1ε𝐹 (i.e. the automaton for 𝐿_1 also completes recognition in the accepting state), then we can accept the string as correct, even if it ended in 𝑞_2. In this case: 𝐹=𝐹_1∪𝐹_2. \nIn other words, the accepting states will be both the states of the automaton for 𝐿_1 and the states of the automaton for 𝐿_2, since as soon as recognition of 𝐿_1 is completed, the automaton can be ready to accept strings 𝐿_2, and vice versa. .\n2) If 𝑞_2∉𝐹​, then the automaton for 𝐿_2 does not complete recognition in the accepting state. In this case, the only accepting state will be 𝐹_2, since 𝐿_2 must complete recognition of the states of 𝐹_2, which correspond to the completion of recognition of the chain 𝐿_2: 𝐹=𝐹_2\nThat is, in the case when 𝐿_1 is recognized in the accepting state, but 𝐿_2 is not, we cannot complete recognition in the states of 𝐹_1, but only in 𝐹_2.\n\nWe must build an automaton that recognizes the language L1*.\nM=<Q, Σ, δ, q0, F>\nF = F1 U{q0}\nδ1(q1,a), if q=q0\nδ(q, a)= δ1(q, a); q∉F1\nδ1(q1,a) U δ1(q,a), qεF"
    },
    {
        "question": "If a language L is regular, then it is defined by a nondeterministic finite automaton. Demonstrate",
        "answer": "\n\nConsider the alphabet of terminal symbols Σ.\nThe theorem is proved in the same way as regular sets are constructed. Our task is to prove that any regular set can be defined by a non-deterministic finite automaton. First, we find a scheme for constructing an automaton for constant regular sets, that is, for letters of the alphabet ∑, then we move on to complications.\nLet's construct an automaton that recognizes the letter a ∈ ∑:\nM = <Q, Σ, δ(x,y), q0, F>\nQ = {q0}\nF = {q0}\nδ(q0, a) = q0.\nIn other words, this is an automaton that recognizes the language {a}. For the language {ε}, the function will be empty, everything else is the same. And for the language ∅, the set of acceptor states will also be empty, F = ∅.\nThere should have been a picture with a scheme here, but there isn't one. The diagram of the automaton is a regular directed graph, where the vertices are the states of Q, and the vertices of F are also highlighted in color/outline/etc. The edge q1 → q2 must always be labeled with a ∈ ∑, which means that (q1, a) = q2. That's it.\nWe must build an automaton that recognizes the language L1 + L2.\nBy definition: If languages ​​L1 and L2 are regular, then L1 + L2 is a regular language. Let L1 be recognized by the automaton M1 = <Q1, Σ, δ1, q1, F1>, and L2 be recognized by the automaton M1 = <Q2, Σ, δ2, q2, F2>. Without loss of generality, we assume that Q1 ∩ Q2 = ∅ (obtained by ordinary renaming). Introduce a new state q0.\nLet M recognize L1 + L2. M=<Q, Σ, δ, q0, F>\nQ=Q1UQ2U{q0}\nδ(q0,a) = δ1(q1,a) U δ2(q2,a)\nδ(q, a)= δ1(q, a); qεQ1\nδ2(q, a), qεQ2\nФ = Ф1 У Ф2\nWe need to build an automaton that recognizes the L1 L2 language.\nLet M recognize L1L2. M=<Q, Σ, δ, q1, F>\nδ1(q, a), if qεQ1\\F1\nδ(q, a)= δ1(q, a) U δ2(q2,a); qεF1\nδ2(q, a), qεQ2\nФ=Ф2\n1)If 𝑞_2ε𝐹, then the machine completes recognition in state 𝑞_2, and if 𝑞_1ε𝐹 (i.e. the machine for 𝐿_1 also completes recognition in the accepting state), then we can accept the string as correct, even if it ended in 𝑞_2. In this case: 𝐹=𝐹_1∪𝐹_2. \nThat is, the accepting states will be both the states of the machine for 𝐿_1 and the states of the machine for 𝐿_2, since after completing the recognition of 𝐿_1, the machine can be ready to accept the strings of 𝐿_2, and vice versa.\n2) If 𝑞_2∉𝐹​, then the automaton for 𝐿_2 does not complete recognition in the accepting state. In this case, the only accepting state will be 𝐹_2 , since 𝐿_2 must complete recognition of the states of 𝐹_2, which correspond to the completion of recognition of the string 𝐿_2: 𝐹=𝐹_2\nThat is, in the case when 𝐿_1 is recognized in the accepting state, but 𝐿_2 is not, we cannot complete recognition in the states of 𝐹_1, but only in 𝐹_2.\n\nWe must build an automaton that recognizes the language L1*.\nM=<Q, Σ, δ, q0, F>\nF=F1 U {q0}\nδ1(q1,a), if q=q0\nδ(q, a)= δ1(q, a); q∉F1\nδ1(q1,a) U δ1(q,a), qεF"
    },
    {
        "question": "If a language L is regular, then it is defined by a right-linear grammar. Prove",
        "answer": "\nLet M=(Q, Σ, δ, q0, F) be an automaton. Let us construct a grammar G=<Q, Σ, P, S> as follows:\n1) If q’=δ(q, a), then we include the rule q → q’a in G\n2) If qf=δ(q, a), then we include the rule qf→ ε in G\nWe compare the output of a word w in the grammar and the cycle of the automaton on w:\nIf a word w of length 1 is recognized by the automaton from state qi, then it is output in G with the initial state qi:\nrecognized ⇒ δ(qi, w) = qf. By construction, there is a rule qi → qf and qf → ε. Let the word w be output in G from qi. Thus,\nthere is a rule qi → qfw and qf → ε, then by construction δ(qi, w) = qf.\nLet a word w of length n be recognizable by an automaton M from initial states qi (derivable in a grammar G from\ninitial nonterminals qi). Let a word ajw of length n + 1 be recognized by an automaton M from state qk. Then δ(qk,\naj) =q’. Then the word w is recognized from state q’. By the induction hypothesis, there is a derivation for w from nonterminal q’. By construction, G has a rule qk → q’ aj. Then, adding qk and q’aj to the beginning of the derivation, we obtain\nthe derivation ajw.\nLet |ajw| have a derivation in G from qi. We show that the word is recognized by M from qj. Since ajw is derivable, G has a\nrule qj → q’ak. Then by construction δ(qj, ak) = q’. By the inductive hypothesis, w is recognizable from q’.\nThen ajw is recognizable from qj."
    },
    {
        "question": "If a language L is regular, then it is defined by a right-linear grammar. Prove",
        "answer": "\nLet M=(Q, Σ, δ, q0, F) be an automaton. Let us construct a grammar G=<Q, Σ, P, S> as follows:\n1) If q’=δ(q, a), then we include the rule q → q’a in G\n2) If qf=δ(q, a), then we include the rule qf→ ε in G\nCompare the output of the word w in the grammar and the cycle of the automaton on w:\nIf a word w of length 1 is recognized by the automaton from state qi, then it is output in G with the initial state qi:\nrecognized ⇒ δ(qi, w) = qf. By construction, there exists a rule qi → qf and qf → ε. Let the word w be output in G from qi. Thus,\nthere is a rule qi → qfw and qf → ε, then by construction δ(qi, w) = qf.\nLet a word w of length n be recognized by an automaton M from initial states qi (it is derivable in a grammar G from\ninitial nonterminals qi). Let a word ajw of length n + 1 be recognized by an automaton M from state qk. Then δ(qk,\naj) = q’. Then the word w is recognized from state q’. By the inductive hypothesis, there is a derivation for w from nonterminal q’. By construction, G has a rule qk → q’ aj. Then, adding qk and q’aj to the beginning of the derivation, we obtain the derivation ajw.\nLet |ajw| have a derivation in G from qi. We show that the word is recognized by M from qj. Since ajw is derivable, G has\na rule qj → q’ak. Then by construction δ(qj, ak) = q’. By the inductive hypothesis, w is recognizable from q'.\nThen ajw is recognizable from qj."
    },
    {
        "question": "If a language L is regular, then it is defined by a direct linear grammar. Prove",
        "answer": "\nLet M=(Q, Σ, δ, q0, F) be an automaton. Let us construct a grammar G=<Q, Σ, P, S> as follows:\n1) If q’=δ(q, a), then include the rule q → q’a in G\n2) If qf=δ(q, a), then include the rule qf→ ε in G\nLet us compare the output of a word w in the grammar and the automaton cycle for w:\nIf a word w of length 1 is recognized by the automaton from state qi, then it is sent back to G with the initial state qi:\nrecognized ⇒ δ(qi, w) = qf. By construction, there exists a rule qi → qf and qf → ε. Suppose that a word w comes from qi in G. Thus,\nthere is a rule qi → qfw and qf → ε, then by construction δ(qi, w) = qf.\nLet a word w of length n be recognized by an automaton M from initial states qi (obtained in the grammar G of the algorithm\ninitial nonterminals ci). Let a word ajw of length n + 1 be recognized by an automaton M from state qk. Then δ(qk,\naj) = q’. Then the word w is recognized from state q’. By the inductive hypothesis, there is a derivation of w from nonterminal q’. By construction, the rule qk → q’ aj is valid in G. Then, adding qk and q'aj at the beginning of the derivation, we obtain the derivation ajw.\nLet |ajw| have a derivation in G from ci. We show that the word M is recognized by qj. Since ajw is differentiable, G has\na rule qj → q’ak. Then by construction δ(qj, ak) = q’. By the inductive hypothesis, w is recognizable from q’.\nThen ajw is recognizable from qj."
    },
    {
        "question": "If a language L is regular, then it is defined by a right-linear grammar. Demonstrate",
        "answer": "\nLet M=(Q, Σ, δ, q0, F) be an automaton. Let us construct a grammar G=<Q, Σ, P, S> as follows:\n1) If q’=δ(q, a), then we include the rule q → q’a in G\n2) If qf=δ(q, a), then we include the rule qf→ ε in G\nCompare the output of the word w in the grammar and the cycle of the automaton on w:\nIf a word w of length 1 is recognized by the automaton from state qi, then it is output in G with the initial state qi:\nrecognized ⇒ δ(qi, w) = qf. By construction, there exists a rule qi → qf and qf → ε. Let the word w come out in G from ci. Thus,\nthere is a rule qi → qfw and qf → ε, then by construction δ(qi, w) = qf.\nLet a word w of length n be recognized by an automaton M by initial states qi (derived in a grammar G from\ninitial nonterminals ci). Let a word ajw of length n + 1 be recognized by an automaton M from state qk. Then δ(qk,\naj) = q'. Then the word w is recognized from state q'. By the induction hypothesis, there is a derivation of w from nonterminal q'. By construction, there is a rule qk → q' aj in G. Thus, adding qk and q'aj at the beginning of the derivation, we obtain\nthe derivation ajw.\nLet |ajw| have an origin in G from ci. We show that the word M is recognized by qj. Since ajw is differentiable, G has\nthe rule qj → q’ak. Then by construction δ(qj, ak) = q’. By the inductive hypothesis w is recognizable from q’.\nThen ajw is recognizable from qj."
    },
    {
        "question": "{a^nb^n| n>=0} is regular?",
        "answer": "\n\nFor proof, consider the language 𝑊={𝑎𝑘∣𝑘≥0} For any distinct words 𝑎^𝑖 and 𝑎^𝑗 (where 𝑖≠𝑗), if we take the concatenation with 𝑏^𝑖, then (𝑎^𝑖)(𝑏^𝑖) belongs to the language 𝐿 but (𝑎^𝑗)(𝑏^𝑖) will not belong to this language, since the number of symbols of 𝑏 does not coincide with the number of symbols of 𝑎"
    },
    {
        "question": "{a^nb^n| n>=0} is correct?",
        "answer": "\n\nFor proof, consider the language 𝑊={𝑎𝑘∣𝑘≥0} For any distinct words 𝑎^𝑖 and 𝑎^𝑗 (where 𝑖≠𝑗), if we take the concatenation with 𝑏^𝑖, then (𝑎^𝑖)(𝑏^𝑖) belongs to the language 𝐿, but (𝑎^𝑗)(𝑏^𝑖) will not belong to this language, since the number of symbols of 𝑏 does not coincide with the number of symbols of 𝑎"
    },
    {
        "question": "{un^nb^n| n>=0} is this correct?",
        "answer": "\n\nFor proof, consider the language 𝑊={𝑎𝑘∣𝑘≥0}. For any single word 𝑎^𝑖 and 𝑎^𝑗 (where 𝑖≠𝑗), if we take the concatenation with 𝑏^𝑖, then (𝑎^𝑖 )( 𝑏^𝑖 ) belongs to the language 𝐿, but (𝑎^𝑗)(𝑏^𝑖) does not belong to this language, since the number of symbols of 𝑏 does not coincide with the number of symbols of 𝑎"
    },
    {
        "question": "{a^nb^n| n>=0} is regular?",
        "answer": "\n\nFor proof, consider the language 𝑊={𝑎𝑘∣𝑘≥0}. For all distinct words 𝑎^𝑖 and 𝑎^𝑗 (where 𝑖≠𝑗), if we take the concatenation with 𝑏^𝑖, then (𝑎^𝑖)( 𝑏^𝑖) belongs to the language 𝐿, but (𝑎^𝑗)(𝑏^𝑖) will not belong to this language, since the number of symbols of 𝑏 does not coincide with the number of symbols of 𝑎"
    },
    {
        "question": "{www | Is w ∈ ∑* } regular?",
        "answer": "\n\nLet 𝑤 and 𝑣 be two different words belonging to 𝑊=Σ*, i.e. 𝑤≠𝑣. A string consisting of two identical parts 𝑤𝑤 belongs to the language 𝐿. That is, 𝑤𝑤∈𝐿. However, if we replace one of these parts with another different word 𝑣, then the string 𝑣𝑣 also belongs to the language 𝐿, since it represents two identical parts of 𝑣.\nWhat happens if we take one part from 𝑤 and the other from 𝑣?\nFor example, the string 𝑤𝑣, where 𝑤≠𝑣, will not belong to the language 𝐿, since it cannot be represented as two identical consecutive words (the first part of 𝑤 does not coincide with the second part of 𝑣).\nThus, the language L cannot be represented by a finite automaton, which proves its irregularity."
    },
    {
        "question": "{www | Is w ∈ ∑* } regular?",
        "answer": "\n\nLet 𝑤 and 𝑣 be two different words belonging to 𝑊=Σ*, i.e. 𝑤≠𝑣. A string consisting of two identical parts 𝑤𝑤 belongs to the language 𝐿. That is, 𝑤𝑤∈𝐿. However, if we replace one of these parts with another word 𝑣, then the string 𝑣𝑣 will also belong to the language 𝐿, since it represents two identical parts 𝑣.\nWhat happens if we take one part from 𝑤 and the other from 𝑣?\nFor example, the string 𝑤𝑣, where 𝑤≠𝑣, will not belong to the language 𝐿, since it cannot be represented as two identical consecutive words (the first part of 𝑤 does not coincide with the second part of 𝑣).\nThus, the language L cannot be represented by a finite automaton, which proves its irregularity."
    },
    {
        "question": "{www | Is w ∈ ∑* } regular?",
        "answer": "\n\nLet 𝑤 and 𝑣 be two different words belonging to 𝑊=Σ*, that is, 𝑤≠𝑣. A string consisting of two identical parts 𝑤𝑤 belongs to the language 𝐿. In other words, 𝑤𝑤ا𝐿. However, if we replace one of these parts with another word 𝑣, then the string 𝑣𝑣 will also belong to the language 𝐿, since it represents two identical parts of 𝑣.\nWhat happens if we take one part 𝑤 and the other part 𝑣?\nFor example, the string 𝑤𝑣, where 𝑤≠𝑣, does not belong to the language 𝐿, since it cannot be represented as two identical consecutive words (the first part of 𝑤 does not coincide with the second part of 𝑣).\nThus, the language L cannot be represented by a finite automaton, which proves its irregularity."
    },
    {
        "question": "{www | Is w ∈ ∑* } regular?",
        "answer": "\n\nLet 𝑤 and 𝑣 be two different words belonging to 𝑊=Σ*, that is, 𝑤≠𝑣. A string consisting of two identical parts 𝑤𝑤 belongs to the language 𝐿. That is, 𝑤𝑤ا𝐿. However, if we replace one of these parts with another word 𝑣, then the string 𝑣𝑣 will also belong to the language 𝐿, since it represents two identical parts of the word 𝑣.\nWhat happens if we take one part 𝑤 and the other part 𝑣?\nFor example, the string 𝑤𝑣, where 𝑤≠𝑣, will not belong to the language 𝐿, since it cannot be represented by two identical consecutive words (the first part of 𝑤 does not coincide with the second part of 𝑣).\nThus, the language L cannot be represented by a finite automaton, which proves its irregularity."
    },
    {
        "question": "Greibach Normal Form Theorem for LL(k)-Grammars",
        "answer": "\nFor any LL(k)-grammar without empty rules, one can construct an LL(k)-grammar in Greibach normal form defining the same language."
    },
    {
        "question": "Greibach's Normal Form Theorem for LL(k) Grammars",
        "answer": "\nFor any LL(k)-grammar without empty rules, one can construct an LL(k)-grammar in Greibach normal form defining the same language."
    },
    {
        "question": "Greibach's Normal Form Theorem for LL(k)-Grammars",
        "answer": "\nFor any LL(k)-grammar without empty rules, we can construct an LL(k)-grammar in Greibach normal form that defines the same language."
    },
    {
        "question": "Greibach's Normal Form Theorem for LL(k)-Grammars",
        "answer": "\nFor any LL(k)-grammar without empty rules, we can construct an LL(k)-grammar in Greibach normal form defining the same language."
    },
    {
        "question": "Theorem on the disjoint union of LL(k)-languages",
        "answer": "\nLet L1, . . . , Ln ⊆ Σ∗ be pairwise disjoint LL(k)-languages, and let their union L1 ∪ . . . ∪ Ln be regular. Then all languages ​​L1, . . . , Ln must be regular."
    },
    {
        "question": "Disjoint union theorem for LL(k) languages",
        "answer": "\nLet L1, . . . , Ln ⊆ Σ∗ be pairwise disjoint LL(k)-languages, and let their union L1 ∪ . . . ∪ Ln be regular. Then all languages ​​L1, . . . , Ln must be regular."
    },
    {
        "question": "Disjoint Union Theorem for LL(k) Languages",
        "answer": "\nLet L1, . . . , Ln ⊆ Σ∗ be pairwise disjoint LL(k) languages, and let their union be L1 ∪ . . . ∪ Ln regular. Then all languages ​​L1, . . . , Ln must be regular."
    },
    {
        "question": "Disjoint union theorem for LL(k) languages",
        "answer": "\nLet L1, . . . , Ln ⊆ Σ∗ be pairwise LL(k)-disjoint languages, and let their union be L1 ∪ . . . ∪ Regular Ln. Thus all languages ​​L1, . . . , Ln must be regular."
    },
    {
        "question": "Theorem on the existence of languages ​​not described by LL(k) grammars for k > 1",
        "answer": "\nFor every k > 1, there exists a language Lk that is determined by an LL(k)-grammar, but not determined by any LL(k)-grammar without empty rules—and hence not by any LL(k − 1)-grammar."
    },
    {
        "question": "Theorem on the existence of languages ​​not described by LL(k) grammars for k > 1",
        "answer": "\nFor every k > 1 there is a language Lk that is defined by an LL(k)-grammar, but is not defined by any LL(k)-grammar without empty rules—and hence not by any LL(k − 1)-grammar."
    },
    {
        "question": "Theorem on the existence of languages ​​not described by LL(k)-grammars for k > 1",
        "answer": "\nFor all k > 1 there exists a language Lk which is defined by an LL(k) grammar, but is not defined by any LL(k) grammar without empty rules and hence not by any LL(k − 1) grammar. ."
    },
    {
        "question": "Theorem on the existence of languages ​​not described by LL(k)-grammars for k > 1",
        "answer": "\nFor all k > 1, there exists a language Lk that is defined by an LL(k) grammar, but is not defined by any LL(k) grammar without empty rules—and hence is not defined by any LL(k − 1) grammar."
    },
    {
        "question": "What is a recursive language",
        "answer": "\nA language L is called recursive if L = L(M) for some Turing machine M satisfying the following conditions.\n1. If w belongs to L, then M gets to an accepting state (and hence halts).\n2. If w does not belong to L, then M eventually halts, although it does not get to an accepting state."
    },
    {
        "question": "What is a recursive language",
        "answer": "\nA language L is called recursive if L = L(M) for some Turing machine M satisfying the following conditions.\n1. If w belongs to L, then M goes to an accepting state (and hence halts).\n2. If w does not belong to L, then M eventually halts, although it does not go to an accepting state."
    },
    {
        "question": "What is a recursive language",
        "answer": "\nA language L is called recursive if L = L(M) for a Turing machine M satisfying the following conditions.\n1. If w is in L, then M goes to the accepting state (and hence halts).\n2. If w is not in L, then M eventually halts, although it does not go to the accepting state."
    },
    {
        "question": "What is a recursive language",
        "answer": "\nA language L is called recursive if L = L(M) for a Turing machine M satisfying the following conditions.\n1. If w belongs to L, then M reaches an accepting state (and hence halts).\n2. If w does not belong to L, then M eventually halts, although it does not reach an accepting state."
    },
    {
        "question": "Rice's theorem",
        "answer": "\nAny non-trivial property of recursively enumerable languages ​​is undecidable."
    },
    {
        "question": "Rice's theorem",
        "answer": "\nAny non-trivial property of recursively enumerable languages ​​is undecidable."
    },
    {
        "question": "Rice's theorem",
        "answer": "\nAny non-trivial property of recursively enumerable languages ​​is undecidable."
    },
    {
        "question": "Rice's theorem",
        "answer": "\nAny non-trivial property of recursively enumerable languages ​​is undecidable."
    },
    {
        "question": "What is right language",
        "answer": "\nA right language is a language Ld(q) recognized by an automaton A in which q is a unique initial state"
    },
    {
        "question": "What is correct language?",
        "answer": "\nA correct language is a language Ld(q) recognized by an automaton A, in which q is a unique initial state"
    },
    {
        "question": "Which language is correct?",
        "answer": "\nA correct language is a language Ld(q) recognized by an automaton A in which q is the only initial state"
    },
    {
        "question": "Which language is correct?",
        "answer": "\nA correct language is a language Ld(q) recognized by an automaton A in which q is the only initial state."
    },
    {
        "question": "What is left language",
        "answer": "\nA left language is a language Lg(q) recognized by an automaton A, in which q is a unique terminal state."
    },
    {
        "question": "What is left of the language?",
        "answer": "\nA left language is a language Lg(q) recognized by an automaton A in which q is a unique final state."
    },
    {
        "question": "What is left of the language?",
        "answer": "\nThe language on the left is the language Lg(q) recognized by the automaton A, in which q is the only final state."
    },
    {
        "question": "What's left of the language",
        "answer": "\nA left language is a language Lg(q) recognized by an automaton A in which q is the only terminal state."
    },
    {
        "question": "How to define a reverse automaton",
        "answer": "\nThe inverse automaton for the automaton A=⟨Σ,Q,S,T,δ⟩ is the automaton r(A)=⟨Σ,Q,T,I,r(δ)⟩ obtained from A by swapping the initial and final states and changing the directions of transitions."
    },
    {
        "question": "How to define a reverse automaton",
        "answer": "\nThe inverse automaton for the automaton A=⟨Σ,Q,S,T,δ⟩ is the automaton r(A)=⟨Σ,Q,T,I,r(δ)⟩ obtained from A by permuting the initial and final states and changing the directions of transitions."
    },
    {
        "question": "How to define a reverse automaton",
        "answer": "\nThe inverse of the automaton A=⟨Σ,Q,S,T,δ⟩ is the automaton r(A)=⟨Σ,Q,T,I,r(δ)⟩ obtained from A by permuting the initial values ​​and final states and changing the direction of transitions."
    },
    {
        "question": "How to define a reverse automaton",
        "answer": "\nThe inverse of the automaton A=⟨Σ,Q,S,T,δ⟩ is the automaton r(A)=⟨Σ,Q,T,I,r(δ)⟩, obtained from A by permuting the initial and final states and changing the directions of transitions."
    },
    {
        "question": "What is group language",
        "answer": "\nA group language is a language whose syntactic monoid is a group."
    },
    {
        "question": "What is group language?",
        "answer": "\nA group language is a language whose syntactic monoid is a group."
    },
    {
        "question": "What is group language?",
        "answer": "\nA group language is a language whose syntactic monoid is a group."
    },
    {
        "question": "What is group language",
        "answer": "\nA group language is a language whose syntactic monoid is a group."
    },
    {
        "question": "Transition monoid",
        "answer": "\nThe transition monoid M(A) is the set of mappings fω with the composition operation. fx⋅fy=fxy. The neutral element in this monoid is the mapping fε."
    },
    {
        "question": "Transitional monoid",
        "answer": "\nThe transition monoid M(A) is the set of mappings fω with the composition operation. fx⋅fy=fxy. The neutral element in this monoid is the mapping fε."
    },
    {
        "question": "Transitional monoid",
        "answer": "\nThe transition monoid M(A) is a set of mappings fω with the composition operation. fx⋅fy=fxy. The neutral element of this monoid is the mapping fε."
    },
    {
        "question": "Transitional monoid",
        "answer": "\nThe transition monoid M(A) is the set of mappings fω with the composition operation fx⋅fy=fxy. The neutral element of this monoid is the mapping fε."
    },
    {
        "question": "Non-deterministic Turing machine",
        "answer": "\nA non-deterministic Turing machine (NTM) differs from deterministic ones in that its δ(q, X) for each state q and tape symbol X is a set of triples {(q1, Y1, D1), (q2, Y2, D2), …, (qk, Yk, Dk)}, where k is a natural number. At each step, the NTM can choose any of the triples for the next transition."
    },
    {
        "question": "Non-deterministic Turing machine",
        "answer": "\nA nondeterministic Turing machine (NTM) differs from deterministic ones in that its δ(q, X) for each state q and tape symbol X is a set of triples {(q1, Y1, D1), (q2, Y2, D2), …, (qk, Yk, Dk)}, where k is a natural number. At each step, the NTM can choose any of the triples for the next transition."
    },
    {
        "question": "Non-deterministic Turing machine",
        "answer": "\nA nondeterministic Turing machine (NTM) differs from deterministic machines in that its δ(q, X) for each state q and zone symbol X is a set of triples {(q1, Y1, D1), (q2, Y2, D2), ..., (qk, Yk, Dk)}, where k is a natural number. At each step, the NTM can choose any of the triples for the next transition."
    },
    {
        "question": "Non-deterministic Turing machine",
        "answer": "\nA non-deterministic Turing machine (DTM) differs from deterministic machines in that its δ(q, X) for each state q and stripe symbol , D2), …, (qk, Yk, Dk)}, where k is a natural number. At each step, the DTM can choose one of the triplets for the next transition."
    },
    {
        "question": "Chain homomorphism",
        "answer": "\nA string homomorphism is a function on a set of strings that substitutes a specific string for each symbol of the string.\nExample: the function h defined as h(0) = ab and h(1) = ε is a homomorphism. In any string of symbols 0 and 1, h replaces all zeros with the string ab, and all ones with the empty string. For example, applying h to the string 0011 yields abab"
    },
    {
        "question": "Chain homomorphism",
        "answer": "\nA string homomorphism is a function on a set of strings that replaces a particular string with each character of the string.\nExample: The function h defined by h(0) = ab and h(1) = ε is a homomorphism. Given any string of characters 0 and 1, h replaces all 0s with the string ab and all 1s with the empty string. For example, applying h to the string 0011 yields abab"
    },
    {
        "question": "Chain homomorphism",
        "answer": "\nA string homomorphism is a function over a set of strings that replaces a particular string with every character in the string.\nExample: The function h defined by h(0) = ab and h(1) = ε is a homomorphism. Given any string of characters 0 and 1, h replaces all 0s with the string ab and all 1s with the empty string. For example, applying h to the string 0011 yields abab"
    },
    {
        "question": "String homomorphism",
        "answer": "\nA string homomorphism is a function over a set of strings that replaces each character in the string with a specific string.\nExample: The function h defined by h(0) = ab and h(1) = ε is a homomorphism. Given any string of characters 0 and 1, h replaces all 0s with the string ab and all 1s with the empty string. For example, applying h to the string 0011 yields abab"
    },
    {
        "question": "1) Pumping lemma for regular languages.",
        "answer": "\nLet L be a regular language. Then there exists an integer p ≥ 1 depending only on L such that strings w in L of length at least p (p is called the \"pumping length\") can be written as w = xyz such that:\n1. |y| ≥ 1\n2. |xy| ≤ p\n3. for all i ≥ 0, x(y^i)z ∈ L\ny is a substring that can be pumped (removed or repeated an arbitrary number of times, so that the result remains in L). (1) means that the cycle y must be pumped of at least length 1, (2) means that the cycle must be within the first p characters. There are no restrictions on x and z."
    },
    {
        "question": "1) Pumping lemma for regular languages.",
        "answer": "\nLet L be a regular language. Then there exists an integer p ≥ 1, depending only on L, such that strings w in L of length at least p (p is called the \"pumping length\") can be written as w = xyz such that:\n1. |y| ≥ 1\n2. |xy| ≤ p\n3. for all i ≥ 0, x(y^i)z ∈ L\ny is a substring that can be pumped (removed or repeated an arbitrary number of times, so that the result remains in L). (1) means that the cycle y must be pumped of length at least 1, (2) means that the cycle must be within the first p characters. There are no restrictions on x and z."
    },
    {
        "question": "1) Pumping lemma for regular languages.",
        "answer": "\nLet L be a regular language. Then there exists an integer p ≥ 1, depending only on L, such that for chains w in L of length at least p (p is called the \"pump length\") we can write w = xyz such that:\n1. |у| ≥ 1\n2. |ху| ≤p\n3. for all i ≥ 0 x(y^i)z ∈ L\ny is a substring that can be pumped (removed or repeated an arbitrary number of times so that the result remains in L). (1) means that the loop y must be pumped of length at least 1, (2) means that the loop must be within the first p symbols. There are no restrictions on x and z."
    },
    {
        "question": "1) Pumping lemma for regular languages.",
        "answer": "\nLet L be a regular language. Then there exists an integer p ≥ 1 depending only on L such that chains w in L of length at least p (p is called the \"pumping length\") can be written as w = xyz such that:\n1. |у| ≥ 1\n2. |ху| ≤p\n3. for all i ≥ 0 x(y^i)z ∈ L\ny is a substring that can be pumped (removed or repeated an arbitrary number of times so that the result remains in L). (1) means that the cycle y must be pumped of length at least 1, (2) means that the cycle must be in the first p symbols. There are no restrictions on x and z."
    },
    {
        "question": "2) An informal statement and explanation of the pumping lemma for regular languages.",
        "answer": "\n\nThe pumping lemma describes an essential property of regular languages. It states that a word w of a language L of length at least m (where m is a constant called the pumping length, depending only on L) can be divided into three substrings, w = xyz, such that the middle part, y (non-empty), can be repeated an arbitrary number of times (including zero, i.e., deleted) to yield a string in L. This process of repetition is called \"pumping\". Moreover, the pumping lemma guarantees that the length of xy will not exceed m, restricting the ways in which w can be divided. Note that finite languages ​​satisfy the requirements of the pumping lemma by trivially determining m to be the length of the largest string in the language plus one."
    },
    {
        "question": "2) An informal presentation and explanation of the pumping lemma for regular languages.",
        "answer": "\n\nThe pumping lemma describes an essential property of regular languages. It states that a word w of a language L of length at least m (where m is a constant, called the pumping length, depending only on L) can be divided into three substrings, w = xyz, such that the middle part, y (nonempty), can be repeated an arbitrary number of times (including zero, i.e., removed) to yield a string in L. This process of repetition is called \"pumping\". Moreover, the pumping lemma guarantees that the length of xy will not exceed m, constraining the ways in which w can be divided. Note that finite languages ​​satisfy the requirements of the pumping lemma by trivially defining m by the length of the largest string in the language plus one."
    },
    {
        "question": "2) An informal presentation and explanation of the pumping lemma for ordinary languages.",
        "answer": "\n\nThe pumping lemma describes an important property of regular languages. It states that a word w of a language L of length at least m (where m is a constant, called the pumping length, depending only on L) can be divided into three substrings w = xyz such that the middle part y (non-empty) can be repeated an arbitrary number of times (including zero, i.e., removed) to produce a string in L. This process of repetition is called \"pumping\". Furthermore, the pumping lemma ensures that the length of xy is at most m, which constrains the ways in which w can be divided. Note that finite languages ​​satisfy the requirements of the pumping lemma by trivially defining m by the length of the largest string of the language plus one."
    },
    {
        "question": "2) Informal formulation and explanation of the pumping lemma for ordinary languages.",
        "answer": "\n\nThe pumping lemma describes an important property of regular languages. It states that a word w of a language L of length at least m (where m is a constant, called the pumping length, depending only on L) can be partitioned into three substrings w = xyz such that the median of the part y (which is not empty) can be repeated an arbitrary number of times (including zero, i.e., removed) to produce a string in L. This process of repetition is called \"pumping\". Furthermore, the pumping lemma guarantees that the length of xy will not exceed m, which constrains the ways in which w can be partitioned. Note that finite languages ​​satisfy the requirements of the pumping lemma by trivially defining m by the length of the largest string of the language plus one."
    },
    {
        "question": "3) Using the pumping lemma for regular languages.",
        "answer": "\nThe pumping lemma is often used to prove that some language is not regular: a proof by contradiction (assuming the language is regular) can consist of finding a word (of the required length) in the language for which the property stated in the lemma does not hold.\n\nFor example, the irregularity of a language L = {(a^n)(b^n) : n ≥ 0} over an alphabet Σ = {a, b} can be shown as follows. Let w, x, y, z, p, and i be given as stated in the lemma above. Let w in L be given by w = (a^p)(b^p). By the pumping lemma, there exists a partition w = xyz, where |xy| ≤ p, |y| ≥ 1 such that x(y^i)z is in L for every i ≥ 0. If we assume that |xy|=p and |z|=p, then xy is the first part of w consisting of p consecutive instances of the symbol a. Since |y| ≥ 1, it contains at least one a, and x(y^2)z contains more a than b. Therefore, x(y^2)z is not in L (note that any value of i other than 1 will give a contradiction). We have reached a contradiction, since in this case the pumped word does not belong to L. Therefore, the assumption that L is regular is false. Therefore, L is not a regular language.\n\nThe proof of the irregularity of the language of balanced brackets follows the same idea. Given p, there exists a string of balanced brackets starting with more than p left brackets, so y will contain only left brackets. By repeating y, one can construct a string that does not contain an equal number of left and right parentheses, which cannot be balanced."
    },
    {
        "question": "3) Using the pumping lemma for regular languages.",
        "answer": "\nThe pumping lemma is often used to prove that some language is not regular: a proof by contradiction (assuming the language is regular) may consist of finding a word (of the required length) in the language for which the property specified in the lemma does not hold.\n\nFor example, the irregularity of a language L = {(a^n)(b^n) : n ≥ 0} over an alphabet Σ = {a, b} can be shown as follows. Let w, x, y, z, p, and i be given as in the lemma above. Let w in L be given by w = (a^p)(b^p). By the pumping lemma, there exists a partition w = xyz, where |xy| ≤ p, |y| ≥ 1 such that x(y^i)z belongs to L for every i ≥ 0. Assuming that |xy|=p and |z|=p, then xy is the first part of w consisting of p consecutive instances of a. Since |y| ≥ 1, it contains at least one a, and x(y^2)z contains more a than b. Therefore, x(y^2)z does not belong to L (note that any value of i other than 1 would lead to a contradiction). We have arrived at a contradiction, since in this case the pumped word does not belong to L. Therefore, the assumption that L is regular is false. Therefore, L is not a regular language.\n\nThe proof that the language of balanced brackets is not regular follows the same idea. Given p, there is a string of balanced brackets starting with more than p left brackets, so y will contain only left brackets. By repeating y, it is possible to construct a string that does not contain the same number of left and right parentheses, which cannot be balanced."
    },
    {
        "question": "3) Using the pumping lemma for regular languages.",
        "answer": "\nThe pumping lemma is often used to prove that a language is not regular: a proof by contradiction (assuming that the language is regular) can consist of finding a word (of the required length) in the language for which the property specified in the lemma does not hold.\n\nFor example, the linguistic imprecision L = {(a^n)(b^n): n ≥ 0} in the alphabet Σ = {a, b} can be mapped as follows. Let w, x, y, z, p, and i be as specified in the lemma above. Let w in L be given by w = (a^p)(b^p). By the pumping lemma, there exists a partition w = xyz, where |xy| ≤ p, |y| ≥ 1 such that x(y^i)z belongs to L for all i ≥ 0. Assuming |xy|=p and |z|=p, then xy is the first part of w consisting of p consecutive instances of the symbol a. . Since |y| ≥ 1, it contains at least one a, and x(y^2)z contains more a than b. Therefore, x(y^2)z does not belong to L (note that any value of i other than 1 would lead to a contradiction). We have arrived at a contradiction, since in this case the inflated word does not belong to L. Therefore, the assumption that L is regular is false. Therefore, L is not a regular language.\n\nThe proof that the language of balanced brackets is ill-formed follows the same idea. Given p, there is a string of balanced brackets starting with more than p left brackets, so y will contain only left brackets. By repeating y, you can build a string containing different numbers of left and right parentheses, which cannot be balanced."
    },
    {
        "question": "3) Using the pumping lemma for regular languages.",
        "answer": "\nThe Pumping Lemma is often used to prove that a language is not regular: a proof by contradiction (assuming that the language is regular) can consist of finding a word (of the required length) in the language for which the property claimed in the lemma is not verified.\n\nFor example, the irregularity of a language L = {(a^n)(b^n): n ≥ 0} over an alphabet Σ = {a, b} can be demonstrated as follows. Let w, x, y, z, p, and i be given as in the lemma above. Let w in L be given by w = (a^p)(b^p). By the Pumping Lemma, there exists a partition w = xyz, where |xy| ≤ p, |y| ≥ 1 such that x(y^i)z is in L for all i ≥ 0. If we assume that |xy|=p and |z|=p, then xy is the first part of w consisting of p consecutive instances of the symbol a. Since |y| ≥ 1, it contains at least one a, and x(y^2)z contains more a than b. Therefore, x(y^2)z is not in L (note that any value of i other than 1 will lead to a contradiction). We have arrived at a contradiction, since in this case the pumped word is not in L. Therefore, the assumption that L is regular is false. Therefore, L is not a regular language.\n\nThe proof that the language of balanced brackets is ill-formed follows the same idea. Given p, there is a string of balanced brackets starting with more than p left brackets, so y will contain only left brackets. By repeating y, we can construct a string that does not contain an equal number of left and right parentheses, and which cannot be balanced."
    },
    {
        "question": "4) Proof of the pumping lemma",
        "answer": "\nLet L be a regular language. Then, by the definition of regularity, there exists a deterministic finite automaton (DFA) for L that recognizes this language. Let this automaton have p states.\n\nNow consider any string w from L whose length |w| ≥ p. By the Dirichlet principle (or \"pocket principle\"), if we process the string w on this finite automaton, then when processing the first p characters of the string, the automaton, going through the states, will necessarily visit at least one state twice (since there are p states in total, and there are also p or more characters in the string).\n\nThus, the string w can be represented as three parts:\nw = xyz, where:\n1. x is the part of the string that leads the automaton from the initial state to some state;\n2. y is the part of the string that corresponds to a cycle in the automaton (a repeated visit to the same state);\n3. z is the remainder of the string after exiting the cycle.\nThis representation implies that the following conditions hold:\n\n1. |y| ≥ 1, because y is a non-empty part of the string that causes the cycle, otherwise there would be no repeated visit to the same state.\n2. |xy| ≤ p, because the cycle occurs within the first p characters of the string.\n3. For all i ≥ 0, a string of the form x(y^i)z also belongs to the language L. This means that we can repeat or remove the part y (which corresponds to the cycle in the automaton) an arbitrary number of times, and the resulting string will still be recognized by the automaton, and therefore belong to the language L.\nThus, the pumping lemma states that for any string w of length |w| ≥ p in a regular language, one can find a partition w = xyz that satisfies the above conditions, which completes the proof."
    },
    {
        "question": "4) Proof of the pumping lemma",
        "answer": "\nLet L be a regular language. Then, by the definition of regularity, there exists a deterministic finite automaton (DFA) for L that recognizes this language. Let this automaton have p states.\n\nNow consider any string w from L whose length |w| ≥ p. By the Dirichlet principle (or \"pocket principle\"), if we process the string w on this finite automaton, then when processing the first p characters of the string, the automaton, going through the states, will necessarily visit at least one state twice (since there are p states in total, and there are also p or more characters in the string).\n\nThus, the string w can be represented as three parts:\nw = xyz, where:\n1. x is the part of the string that takes the automaton from the initial state to some state;\n2. y is the part of the string that corresponds to a cycle in the automaton (repeated visit to the same state);\n3. z is the remainder of the string after exiting the cycle.\nThis representation implies that the following conditions hold:\n\n1. |y| ≥ 1, since y is a non-empty part of the string that causes the cycle, otherwise there would be no revisit to the same state.\n2. |xy| ≤ p, since the cycle occurs within the first p characters of the string.\n3. For all i ≥ 0, a string of the form x(y^i)z also belongs to the language L. This means that we can repeat or delete the part y (corresponding to the cycle in the automaton) an arbitrary number of times, and the resulting string will still be recognized by the automaton and, therefore, will belong to the language L.\nThus, the pumping lemma states that for any string w of length |w| ≥ p, a partition w = xyz can be found in a regular language that satisfies the above conditions, which completes the proof."
    },
    {
        "question": "4) Proof of the pumping lemma",
        "answer": "\nLet L be a regular language. Then, by the definition of regularity, there exists a deterministic finite automaton (DFA) for L that recognizes this language. Let this automaton have p states.\n\nNow consider any string w from L whose length |w| ≥п. According to the Dirichlet principle (or \"pocket principle\"), if we process a string w on this finite automaton, then when processing the first p symbols of the string, the automaton, passing through the states, will necessarily visit at least one state twice (since there are p states and p symbols in the string, and there are also p or more in the string).\n\nThus, the string w can be represented as three parts:\nw = xyz, where:\n1. х is the part of the chain that transfers the automaton from the initial state to the state;\n2. y is the part of the chain that corresponds to the machine's operating cycle (repeated visits to the same state);\n3. z is the remainder of the string after exiting the cycle.\nThis representation assumes that the following conditions are met:\n\n1. |у| ≥ 1, since y is a non-empty part of the string that causes the cycle, otherwise there would be no new visit to the same state.\n2. |ху| ≤ p, since the cycle occurs in the first p symbols of the string.\n3. For all i ≥ 0, a string of the form x(y^i)z also belongs to the language L. This means that we can repeat or delete the part y (corresponding to the cycle in the automaton) an arbitrary number of times, and the resulting string will always be recognized by the automaton and, therefore, will belong to the language L.\nThus, the pumping lemma states that for any string w of length |w| ≥ p in a regular language, we can find a partition w = xyz satisfying the above conditions, which completes the proof."
    },
    {
        "question": "4) Proof of the pumping lemma",
        "answer": "\nLet L be a regular language. Then, by the definition of regularity, there exists a deterministic finite automaton (DFA) for L that recognizes this language. Let this automaton have p states.\n\nNow consider a string w from L whose length |w| ≥п. By the Dirichlet principle (or \"pocket principle\"), if we process a string w on this finite automaton, then when processing the first p symbols of the string, the automaton, passing through the states, will necessarily visit minus one state twice. (since there are p states in total, and there are also p or more symbols in the string).\n\nThus, the string w can be represented by three parts:\nw = xyz, where:\n1. х is the part of the chain that brings the automaton from the initial state to the state;\n2. y is the part of the chain that corresponds to a cycle in the automaton (repeated visit to the same state);\n3. z is the remainder of the string after exiting the cycle.\nThis representation implies that the following conditions are satisfied:\n\n1. |у| ≥ 1, since y is a non-empty part of the string causing the cycle, otherwise there would be no revisit to the same state.\n2. |ху| ≤ p, since the cycle occurs in the first p symbols of the string.\n3. For all i ≥ 0, a string of the form x(y^i)z also belongs to the language L. This means that we can repeat or delete the part y (which corresponds to the cycle in the automaton) arbitrarily many times, and the resulting string will always be recognized by the automaton and therefore belong to the language L.\nThus, the pumping lemma states that for any string w of length |w| ≥ p in a regular language, we can find a partition w = xyz satisfying the above conditions, which completes the proof."
    },
    {
        "question": "5) Generalized pumping lemma for regular languages",
        "answer": "\nIf L is a regular language, then there exists a number p > 0 (the pumping length) such that for any string uwv in L with |w| ≥ p, the representation uwv = uxyzv holds\nwhere x, y, and z are strings such that |xy| ≤ p, |y| > 0, and ux(y^i)zv belongs to L for any integer i ≥ 0.\nThis version can be used to prove the irregularity of even more languages, since it imposes stronger restrictions."
    },
    {
        "question": "5) Generalized pumping lemma for regular languages",
        "answer": "\nIf L is a regular language, then there exists a number p > 0 (the pumping length) such that for any string uwv in L with |w| ≥ p, the representation uwv = uxyzv holds\nwhere x, y, and z are strings such that |xy| ≤ p, |y| > 0, and ux(y^i)zv is in L for every integer i ≥ 0.\nThis version can be used to prove the irregularity of even more languages, since it imposes stronger restrictions."
    },
    {
        "question": "5) Generalized pumping lemma for regular languages.",
        "answer": "\nIf l is a regular language, then there exists a number P> 0 (the pump length) such that for any UWV channel in L with W | ≥ p, the representation uwv = uxyzv holds\nwhere x, y, and z are strings such that |xy| ≤ p, |y| > 0, and ux (y^i) zv belongs to L for any integer i ≥ 0.\nThis version may be useful for proving an irregularity in more languages ​​than it requires."
    },
    {
        "question": "5) Generalized pumping lemma for regular languages.",
        "answer": "\nIf L is a regular language, then there exists a number p > 0 (the pumping length) such that for any string uwv in L with |w| ≥ p, we have uwv = uxyzv\nwhere x, y, and z are strings such that |xy| ≤ p, |y| > 0, and ux(y^i)zv is in L for every integer i ≥ 0.\nThis version can be used to prove the incorrectness of even more languages, since it imposes stronger restrictions."
    },
    {
        "question": "6) Insufficiency of the pumping lemma for regular languages",
        "answer": "\nThe pumping lemma for regular languages ​​is a powerful tool for proving that a language is not regular. However, it has limitations: even if a language violates the conditions of the pumping lemma, this does not always mean that it is irregular. The lemma is not always sufficient to prove the regularity or irregularity of a language. Let's consider this using the language L as an example:\nL={uvwxy:u,y∈{0,1,2,3}∗,vwx∈{0,1,2,3}∧(v=w∨v=x∨x=w)}∪{w:w∈{0,1,2,3}∗}\n\nLet's consider the structure of the language:\n1. The first part of the language:\nThese are the strings uvwxy, where:\nu, y are arbitrary strings from {0,1,2,3},\nvwx is a string of length 3 from {0,1,2,3},\none of the conditions is satisfied: v = w, v = x, or x = w (at least two symbols from vwx coincide).\n2. The second part of the language:\nThese are all the strings from {0,1,2,3}, that is, any possible set of symbols.\n\nLet's try to apply the lemma to the language L:\n1. Strings of the form uvwxy have strict restrictions on how symbols v, w, and x can be related to each other (they must contain at least two identical symbols). This may require the finite state machine to have \"memory\" to compare symbols v, w, and x, which may lead to irregularity of the language.\n2. However, because of the second part of the language (all strings in {0,1,2,3}), the pumping lemma will fail. Any string in this second part satisfies the pumping lemma, since it is completely regular (the finite state machine can recognize any strings in {0,1,2,3}). The lemma simply cannot prove irregularity, because the second part of the language covers all strings that can be \"pumped\", even if the first part is not regular.\n\nWhy the lemma is insufficient for L:\n1. If we try to apply the pumping lemma to L, it may \"fail\" on examples from the second part of the language (where any strings are regular). That is, the lemma will not be able to reveal the irregularity of the first part, since regular strings from the second part will satisfy the pumping conditions.\n2. The pumping lemma cannot prove the irregularity of the language if the language contains regular components that allow any strings to \"successfully pass\" the test for the lemma conditions.\n\nConclusion: The pumping lemma is insufficient to prove the irregularity of L, since the second part of the language ({w : w ∈ {0,1,2,3}^*}) consists of all strings from {0,1,2,3} that always satisfy the pumping conditions. This prevents the lemma from being applied to prove the irregularity of the first part of the language, which is related to the condition of coincidence of symbols in the string vwx."
    },
    {
        "question": "6) Insufficiency of the pumping lemma for regular languages",
        "answer": "\nThe pumping lemma for regular languages ​​is a powerful tool for proving that a language is not regular. However, it has limitations: even if a language violates the conditions of the pumping lemma, this does not always mean that it is irregular. The lemma is not always sufficient to prove whether a language is regular or irregular. Let's consider this using the language L as an example:\nL={uvwxy:u,y∈{0,1,2,3}∗,vwx∈{0,1,2,3}∧(v=w∨v=x∨x=w)}∪{w:w∈{0,1,2,3}∗}\n\nLet's consider the structure of the language:\n1. The first part of the language:\nThese are the strings uvwxy, where:\nu, y are arbitrary strings from {0,1,2,3},\nvwx is a string of length 3 from {0,1,2,3},\none of the conditions is satisfied: v = w, v = x, or x = w (at least two symbols from vwx coincide).\n2. The second part of the language:\nThese are all the strings from {0,1,2,3}, that is, any possible set of symbols.\n\nLet's try to apply the lemma to the language L:\n1. Strings of the form uvwxy have strict restrictions on how symbols v, w, and x can be related to each other (they must contain at least two identical symbols). This may require the FSM to have a \"memory\" for comparing symbols v, w, and x, which may lead to irregularity of the language.\n2. However, because of the second part of the language (all strings in {0,1,2,3}), the pumping lemma will not work. Any string in this second part satisfies the pumping lemma because it is completely regular (the FSM can recognize any strings in {0,1,2,3}). The lemma simply cannot prove irregularity, because the second part of the language covers all strings that can be \"pumped\", even if the first part is not regular.\n\nWhy the lemma is not sufficient for L:\n1. If we try to apply the pumping lemma to L, it may \"fail\" on examples from the second part of the language (where all strings are regular). That is, the lemma will not be able to reveal the irregularity of the first part, since regular strings from the second part will satisfy the pumping conditions.\n2. The pumping lemma cannot prove the irregularity of the language if the language contains regular components that allow any string to \"pass\" the test for the lemma's conditions.\n\nConclusion: the pumping lemma is not sufficient to prove the irregularity of L, since the second part of the language ({w : w ∈ {0,1,2,3}^*}) consists of all strings from {0,1,2,3} that always satisfy the pumping conditions. This does not allow us to apply the lemma to prove the irregularity of the first part of the language, which is related to the condition of matching symbols in the string vwx."
    },
    {
        "question": "6) Insufficiency of the pumping lemma for regular languages.",
        "answer": "\nThe pumping lemma for regular languages ​​is a powerful tool for proving that a language is not regular. However, it has its limits: even if a language violates the conditions of the pumping lemma, this does not always mean that it is not regular. The lemma is not always sufficient to prove the soundness or irregularity of a language. Let's look at this using the language L as an example:\nL={uvwxy:u,yε{0,1,2,3}∗,vwxε{0,1,2,3}∧(v=w∨v=x∨x=w)}∪{w: wε{0,1,2,3}∗}\n\nLet's look at the structure of the language:\n1. The first part of the language:\nThese are the strings uvwxy, where:\nu, y are arbitrary strings {0,1,2,3},\nvwx is a string of length 3 from {0,1,2,3},\none of the following is true: v = w, v = x, or x = w (at least two symbols of vwx are the same).\n2. The second part of the language:\nThese are all the strings {0,1,2,3}, that is, any possible set of symbols.\n\nLet's try to apply the lemma to the language L:\n1. Strings of the form uvwxy have strict restrictions on how symbols v, w, and x can be related to each other (they must contain at least two identical symbols). This may require the FSM to have \"memory\" to compare symbols v, w, and x, which may lead to violations of the language.\n2. However, because of the second part of the language (all strings in {0,1,2,3}), the pumping lemma will not work. Any string in the second part satisfies the pumping lemma because it is completely regular (the FSM can recognize any string in {0,1,2,3}). The lemma simply cannot prove irregularity, since the second part of the language covers all strings that can be \"pumped\", even if the first part is not regular.\n\nWhy the lemma is not sufficient for L:\n1. If we try to apply the pumping lemma to L, it may \"fail\" on examples from the second part of the language (where all strings are regular). In other words, the lemma will not be able to reveal the irregularity of the first part, since regular strings of the second part will satisfy the pumping conditions.\n2. The pumping lemma cannot prove the irregularity of the language if the language contains regular components that allow any string to \"pass\" the test of the lemma conditions.\n\nConclusion: the pumping lemma is not sufficient to prove the irregularity of L, since the second part of the language ({w : w ∈ {0,1,2,3}^*}) consists of all strings {0,1 ,2,3} that always satisfy the pumping conditions. This does not allow us to apply the lemma to prove the inaccuracy of the first part of the language, related to the condition of matching symbols in the string vwx."
    },
    {
        "question": "6) Insufficiency of the pumping lemma for regular languages.",
        "answer": "\nThe pumping lemma for regular languages ​​is a powerful tool for proving that a language is not regular. However, it has its limits: even if a language violates the conditions of the pumping lemma, this does not always mean that it is not regular. The lemma is not always sufficient to prove the soundness or irregularity of a language. Let's look at this using the language L as an example:\nL={uvwxy:u,yε{0,1,2,3}∗,vwxε{0,1,2,3}∧(v=w∨v=x∨x=w)}∪{w: wε{0,1,2,3}∗}\n\nLet's look at the structure of the language:\n1. The first part of the language:\nThese are the strings uvwxy, where:\nu, y are arbitrary strings from {0,1,2,3},\nvwx is a string of length 3 from {0,1,2,3},\none of the conditions is satisfied: v = w, v = x, or x = w (at least two symbols of vwx coincide).\n2. The second part of the language:\nThese are all the strings {0,1,2,3}, that is, any possible set of symbols.\n\nLet's try to apply the lemma to the language L:\n1. Strings of the form uvwxy have strict restrictions on how symbols v, w, and x can be related to each other (they must contain at least two identical symbols). This may require the FSM to have \"memory\" to compare symbols v, w, and x, which may lead to imprecision in the language.\n2. However, because of the second part of the language (all strings in {0,1,2,3}), the Pumping Lemma will fail. Any string in the second part satisfies the Pumping Lemma because it is completely regular (the FSM can recognize all strings in {0,1,2,3}). The lemma simply cannot prove irregularity, since the second part of the language covers all strings that can be \"pumped\", even if the first part is not regular.\n\nWhy the lemma is not sufficient for L:\n1. If we try to apply the pumping lemma to L, it may \"fail\" on examples from the second part of the language (where all strings are regular). That is, the lemma will not be able to reveal the irregularity of the first part, since regular chains of the second part will satisfy the pumping conditions.\n2. The pumping lemma cannot prove the irregularity of a language if it contains regular components that allow all strings to \"pass\" the test of the lemma conditions.\n\nConclusion: the pumping lemma is not sufficient to prove the irregularity of L, since the second part of the language ({w : w ∈ {0,1,2,3}^*}) consists of all strings {0,1,2,3} that always satisfy the pumping conditions. This does not allow us to apply the lemma to prove the irregularity of the first part of the language, related to the condition of coincidence of symbols in the string vwx."
    },
    {
        "question": "7) Delinearization of regular expressions",
        "answer": "\nRegular expression delinearization is the reverse process of linearization, which involves converting regular expressions from a linear, simplified form to a more compact or structured form, typically using grouping and repetition to reduce the amount of code and better represent the logic of the regular expression. Delinearization helps to restore the original structure of the regular expression, making it more organized and easier to maintain."
    },
    {
        "question": "7) Delinearization of regular expressions",
        "answer": "\nRegular expression delinearization is the reverse process of linearization, which involves transforming regular expressions from a linear, simplified form to a more compact or structured form, typically using grouping and repetition to reduce the amount of code and better represent the logic of the regular expression. Delinearization helps restore the original structure of a regular expression, making it more organized and easier to maintain."
    },
    {
        "question": "7) Delinearization of regular expressions",
        "answer": "\nRegular expression delinearization is the reverse process of linearization, converting regular expressions from a linear, simplified form to a more compact or structured form, typically using grouping and repetition to reduce the code size and better represent the logic of the regular expression. Delinearization helps restore the original structure of a regular expression, making it more organized and easier to maintain."
    },
    {
        "question": "7) Delinearization of regular expressions",
        "answer": "\nRegular expression delinearization is the reverse process of linearization, which involves transforming regular expressions from a linear, simplified form to a more compact or structured form, typically using grouping and repetition to reduce the amount of code and better represent the logic of the regular expression. Delinearization restores the original structure of the regular expression, making it more organized and easier to maintain."
    },
    {
        "question": "8) Why is delinearization needed?",
        "answer": "\n1. Compactness: Linear regular expressions, especially if they contain many alternatives or repeated parts, can become unwieldy. Delinearization helps reduce the length of the expression and improve its readability.\n2. Improved readability: Delinearization adds structure to the regular expression. Grouping parts can help make the logic of the expression easier to understand, especially when the different parts are separated into logical components.\n3. Maintainability: A regular expression consisting of many linear alternatives is difficult to maintain and modify. Grouping common parts and using shorthand makes the expression easier to modify in the future."
    },
    {
        "question": "8) Why is delinearization needed?",
        "answer": "\n1. Compactness: Linear regular expressions, especially if they contain many alternatives or repeated parts, can become unwieldy. Delinearization helps reduce the length of the expression and improve its readability.\n2. Improved readability: Delinearization adds structure to the regular expression. Grouping parts can help make the logic of the expression clearer, especially when the different parts are separated into logical components.\n3. Maintainability: A regular expression consisting of many linear alternatives is difficult to maintain and change. Grouping common parts and using shorthand makes it easier to change the expression in the future."
    },
    {
        "question": "8) Why is delinearization necessary?",
        "answer": "\n1. Compactness: Linear regular expressions, especially if they contain many alternatives or repeated parts, can become unwieldy. Delinearization can reduce the length of the expression and improve its readability.\n2. Improved readability: Delinearization adds structure to the regular expression. Grouping parts can help make the logic of the expression clearer, especially when the different parts are separated into logical components.\n3. Maintainability: A regular expression consisting of many linear alternatives is difficult to maintain and change. Grouping common areas and using shorthand makes it easier to change the expression in the future."
    },
    {
        "question": "8) Why is delinearization necessary?",
        "answer": "\n1. Compactness: Linear regular expressions, especially if they contain many alternatives or repeated parts, can become unwieldy. Delinearization can reduce the length of the expression and improve its readability.\n2. Improved readability: Delinearization adds structure to the regular expression. Grouping parts can help make the logic of the expression easier to understand, especially when the different parts are separated into logical components.\n3. Maintainability: A regular expression consisting of many linear alternatives is difficult to maintain and change. Grouping common parts and using shortcuts makes it easier to change the expression in the future."
    },
    {
        "question": "9) Stages of delinearization",
        "answer": "\n1. Finding common elements: Find parts of the regular expression that are repeated in the alternative paths and factor them out of brackets. For example, in the expression abd|acd, you can factor out parts a and d, since they are repeated in both alternative branches.\n2. Using groupings: After finding common elements, group the different parts of the expression using brackets. This will allow you to reduce redundant parts of the expression.\n3. Optimizing repetitions: If the regular expression contains similar repetitions of characters or groups, use quantifiers (*, +, {n}, etc.) to compactly express the repetition."
    },
    {
        "question": "9) Stages of delinearization",
        "answer": "\n1. Finding common elements: Find parts of the regular expression that are repeated in the alternative paths and factor them out of parentheses. For example, in the expression abd|acd, you can factor out parts a and d, since they are repeated in both alternative branches.\n2. Using groupings: After finding common elements, group different parts of the expression using parentheses. This allows you to reduce redundant parts of the expression.\n3. Optimizing repetitions: If the regular expression contains similar repetitions of characters or groups, use quantifiers (*, +, {n}, etc.) to compactly express the repetition."
    },
    {
        "question": "9) Stages of delinearization",
        "answer": "\n1. Find common elements. Find parts of the regular expression that are repeated in the alternative paths and enclose them in parentheses. For example, in the expression abd|acd, you can remove parts a and d because they are repeated in two alternative branches.\n2. Use groupings. Once you find common elements, group different parts of the expression using parentheses. This allows you to reduce redundant parts of the expression.\n3. Optimize repetitions. If the regular expression contains similar repetitions of characters or groups, use quantifiers (*, +, {n}, etc.) to tighten up the repetition expression."
    },
    {
        "question": "9) Stages of delinearization",
        "answer": "\n1. Find common elements: Find parts of the regular expression that are repeated in the alternative paths and factor them by removing them from the parentheses. For example, in the expression abd|acd, we can factor parts a and d because they are repeated in two alternative branches.\n2. Use groupings: Once you find common elements, group different parts of the expression using parentheses. This allows you to reduce redundant parts of the expression.\n3. Optimize repetitions: If the regular expression contains similar repetitions of characters or groups, use quantifiers (*, +, {n}, etc.) to compactly express the repetition."
    },
    {
        "question": "10) Advantages of delinearization",
        "answer": "\n1. Improved readability: A delinearized expression is easier to read and understand because it displays the logic in a more organized form.\n2. Smaller code footprint: After delinearization, the expression is often shorter because many elements can be factored out and shortened.\n3. Easier to modify: A delinearized regular expression is easier to modify and maintain because its structure better reflects the logic of the check."
    },
    {
        "question": "10) Advantages of delinearization",
        "answer": "\n1. Improved readability: A delinearized expression is easier to read and understand because it displays the logic in a more organized form.\n2. Smaller code: After delinearization, the expression is often shorter because many elements can be factored out and shortened.\n3. Easier to modify: A delinearized regular expression is easier to modify and maintain because its structure better reflects the logic of the check."
    },
    {
        "question": "10) Advantages of delinearization",
        "answer": "\n1. Improved readability. A delinearized expression is easier to read and understand because it displays the logic in a more organized form.\n2. Less code. After delinearization, an expression is often shorter because many elements can be enclosed in square brackets and abbreviated.\n3. Easier to modify. A delinearized regular expression is easier to modify and maintain because its structure better reflects the logic of the check."
    },
    {
        "question": "10) Advantages of delinearization",
        "answer": "\n1. Improved readability. A delinearized expression is easier to read and understand because it displays the logic in a more organized form.\n2. Smaller code. After delinearization, the expression is often shorter because many elements can be factored and reduced.\n3. Easier to modify. A delinearized regular expression is easier to modify and maintain because its structure better reflects the logic of the check."
    },
    {
        "question": "11) Disadvantages of delinearization",
        "answer": "\n1. May make it harder to understand: In some cases, overly complex groupings can make a regular expression less understandable, especially for those unfamiliar with the use of quantifiers and groupings.\n2. Not always applicable: Not every linear expression can be delinearized effectively. If there are no obvious repetitions or common parts in the expression, attempting delinearization may not provide much improvement."
    },
    {
        "question": "11) Disadvantages of delinearization",
        "answer": "\n1. May make it harder to understand: In some cases, overly complex groupings can make a regular expression harder to understand, especially for those unfamiliar with the use of quantifiers and groupings.\n2. Not always applicable: Not every linear expression can be delinearized effectively. If there are no obvious repetitions or common parts in the expression, attempting delinearization may not provide much improvement."
    },
    {
        "question": "11) Disadvantages of delinearization",
        "answer": "\n1. May make it difficult to understand. In some cases, overly complex groupings can make a regular expression difficult to understand, especially for those unfamiliar with the use of quantifiers and groupings.\n2. Not always applicable: Not all linear expressions can be delinearized effectively. If there are no obvious repetitions or common parts in the expression, attempting delinearization may not provide much improvement."
    },
    {
        "question": "11) Disadvantages of delinearization",
        "answer": "\n1. May make it difficult to understand. In some cases, overly complex groupings can make a regular expression difficult to understand, especially for those unfamiliar with the use of quantifiers and groupings.\n2. Not always applicable: Not all linear expressions can be delinearized effectively. If there are no obvious repetitions or common parts in an expression, attempting to delinearize it may not yield much improvement."
    },
    {
        "question": "12) Formal languages",
        "answer": "\nA formal language is a set M of words over an alphabet Σ (denoted by M ⊆ Σ∗, where the sign ∗ is the Kleene iteration). It usually implies the presence of formal rules that determine the correctness of the form (i.e., syntax) of words from M."
    },
    {
        "question": "12) Formal languages",
        "answer": "\nA formal language is a set M of words in an alphabet Σ (denoted by M ⊆ Σ∗, where the sign ∗ is the Kleene iteration). It is usually assumed that there are formal rules that determine the correctness of the form (i.e., the syntax) of words in M."
    },
    {
        "question": "12) Formal languages",
        "answer": "\nA formal language is a set M of words of an alphabet Σ (denoted by M ⊆ Σ∗, where the sign ∗ is the Kleene iteration). It is usually assumed that there are formal rules that determine the correct form (i.e., syntax) of the words M."
    },
    {
        "question": "12) Formal languages",
        "answer": "\nA formal language is a set M of words in an alphabet Σ (denoted M ⊆ Σ∗, where the sign ∗ is the Kleene iteration). This usually implies the presence of formal rules that determine the correctness of the form (i.e., the syntax) of the words M."
    },
    {
        "question": "13) Enumerability and decidability of formal languages",
        "answer": "\nThe language M is decidable ⇔ for any word w, there is an algorithm for checking whether w belongs to M (always terminating and giving an exact, either positive or negative, answer).\n\nThe language M is enumerable ⇔ for any word w, there is an algorithm that answers the question of whether w belongs to M positively in finite time (but may loop if w /∈ M).\n\nEnumerable but not decidable: the language of programs that terminate on input 0 (in any sufficiently powerful PL). Further, decidable languages ​​can be classified by the minimum required complexity of the decision algorithm."
    },
    {
        "question": "13) Enumerability and decidability of formal languages",
        "answer": "\nThe language M is decidable ⇔ for any word w, there is an algorithm for checking whether w belongs to M (which always terminates and gives an exact answer, positive or negative).\n\nThe language M is enumerable ⇔ for any word w, there is an algorithm that answers the question of whether w belongs to M positively in finite time (but may loop if w /∈ M).\n\nEnumerable but not decidable: the language of programs that terminate on input 0 (in any sufficiently powerful PL). In addition, decidable languages ​​can be classified by the minimum required complexity of the decision algorithm."
    },
    {
        "question": "13) Enumerability and decidability of formal languages.",
        "answer": "\nA language M is decidable ⇔ for any word w, there is an algorithm for checking whether w belongs to M (terminating and always giving an exact answer, positive or negative).\n\nEnumerating a language M ⇔ for any word w, there is an algorithm that answers the question of whether w belongs to a positive word M in finite time (but may loop if w /ε M).\n\nEnumerable but undecidable: a language for programs that terminate on input 0 (in any sufficiently powerful language). Additionally, decidable languages ​​can be classified by the minimum required complexity of the decision algorithm."
    },
    {
        "question": "13) Enumerability and decidability of formal languages.",
        "answer": "\nThe language M is decidable ⇔ for any word w, there is an algorithm for checking whether w belongs to M (always terminating and giving a precise answer, positive or negative).\n\nThe language M is enumerable ⇔ for any word w, there is an algorithm that answers the question of whether w belongs to M positively in finite time (but may loop if w /ε M).\n\nEnumerable but undecidable: the language of programs that terminate on input 0 (in any sufficiently powerful PL). Additionally, decidable languages ​​can be classified by the minimum required complexity of the solution algorithm."
    },
    {
        "question": "14) Term rewriting systems",
        "answer": "\nSignature is a set of pairs ⟨f, n⟩ consisting of a constructor name f and its arrondissement n.\nLet V be a set of variables, F a set of constructors; a set of terms T(F) over F is defined recursively:\n1. all elements of V are terms;\n2. if ⟨f, n⟩ is a constructor and t_1,. . . , t_n are terms, then f(t_1, . . . , t_n) is a term;\n3. there are no other terms.\n\nLet V be a set of variables, F a set of constructors (signature); T(F) a set of terms over the set of constructors F. TRS is a set of rewriting rules of the form Φ_i → Ψ_i, where Φ_i, Ψ_i are terms in T(F). A rewriting rule Φ_i → Ψ_i applies to a term t if t contains a subterm that can be mapped (unified) to Φ_i.\n\nIf no TRS rewriting rule applies to a term t, the term is called normalized. Given rewriting rules of the form f(g(x)) → g(g(f(x))) and g(g(x)) → f(x), each of them can be applied to a term f(g(g(f(g(f(g(g(g(Z))))))))) in three different ways."
    },
    {
        "question": "14) Term rewriting systems",
        "answer": "\nA signature is a set of pairs ⟨f, n⟩ consisting of the name of a constructor f and its constituency n.\nLet V be a set of variables, F a set of constructors; a set of terms T(F) over F is defined recursively:\n1. all elements of V are terms;\n2. if ⟨f, n⟩ is a constructor and t_1,. . . , t_n are terms, then f(t_1,. . . , t_n) are terms;\n3. there are no other terms.\n\nLet V be a set of variables, F a set of constructors (a signature); T(F) a set of terms over the set of constructors F. A TRS is a set of rewriting rules of the form Φ_i → Ψ_i, where Φ_i, Ψ_i are terms in T(F). A rewriting rule Φ_i → Ψ_i applies to a term t if t contains a subterm that can be mapped (unified) to Φ_i.\n\nIf no TRS rewriting rule applies to a term t, the term is called normalized. Given rewriting rules of the form f(g(x)) → g(g(f(x))) and g(g(x)) → f(x), each of them can be applied to a term f(g(g(f(g(f(g(g(g(Z))))))))) in three different ways."
    },
    {
        "question": "14) Term rewriting systems",
        "answer": "\nA signature is a set of pairs ⟨f, n⟩ consisting of a constructor name f and its neighborhood n.\nLet V be a set of variables, F a set of constructors; the set of terms T(F) on F is defined recursively:\n1. all elements of V are terms;\n2. if ⟨f, n⟩ is a constructor and t_1,. . . , t_n are terms, then f(t_1,..., t_n) are terms;\n3. there are no other conditions.\n\nLet V be a set of variables, F a set of constructors (signature); T(F) a set of terms on the set of constructors of F. TRS is a set of rewriting rules of the form Φ_i → Ψ_i, where Φ_i, Ψ_i are terms from T(F). A rewriting rule Φ_i → Ψ_i applies to a term t if t contains a subterm that can be mapped (unified) to Φ_i.\n\nIf a term t is not subject to a TRS rewriting rule, the term is called normalized. Given rewriting rules of the form f(g(x)) → g(g(f(x))) and g(g(x)) → f(x), each of them can be applied to a term f( g (g( f(g(f(g(g(Z)))))))) in three different ways."
    },
    {
        "question": "14) Term rewriting systems",
        "answer": "\nA signature is a set of pairs ⟨f, n⟩ consisting of the name of a constructor f and its round n.\nLet V be a set of variables, F a set of constructors; a set of terms T(F) on F is defined recursively:\n1. all elements of V are terms;\n2. if ⟨f, n⟩ is a constructor and t_1,. . . , t_n are terms, then f(t_1,..., t_n) is a term;\n3. there are no other conditions.\n\nLet V be a set of variables, F a set of constructors (a signature); T(F) a set of terms on the set of constructors of F. TRS is a set of rewriting rules of the form Φ_i → Ψ_i, where Φ_i, Ψ_i are terms from T(F). A rewriting rule Φ_i → Ψ_i applies to a term t if t contains a subterm that can be mapped (unified) to Φ_i.\n\nIf a term t is not subject to a TRS rewriting rule, the term is called normalized. Given rewriting rules of the form f(g(x)) → g(g(f(x))) and g(g(x)) → f(x), each of them can be applied to a term f(g (g(f(g(f(g(g(g(Z))))))))) in three different ways."
    },
    {
        "question": "15) Confluency",
        "answer": "\nA TRS is called confluent if for any two terms t, s that are obtained by rewriting the same term u, there exists a term v such that t, s both rewrite to v.\nFormally:\n∀u, t, s(u →∗t & u →∗s ⇒ ∃v(t →∗ v & s →∗ v))\n\nConfluent systems are parallelizable and easy to optimize.\n→ — rewriting in 1 step;\n→∗ — rewriting in an arbitrary number of steps,\nstarting from 0."
    },
    {
        "question": "15) Merger",
        "answer": "\nA TRS is called confluent if for any two terms t, s obtained by rewriting the same term u, there exists a term v such that t, s both rewrite to v.\nFormally:\n∀u, t, s(u →∗t & u →∗s ⇒ ∃v(t →∗ v & s →∗ v))\n\nConfluent systems are parallelizable and easy to optimize.\n→ — rewriting in 1 step;\n→∗ — rewriting in an arbitrary number of steps,\nstarting from 0."
    },
    {
        "question": "15) Merger",
        "answer": "\nA TRS is called confluent if for any two terms t, s obtained by rewriting the same term u, there exists a term v such that both t, s are rewritten to v.\nOfficially:\n∀u, t, s(u →∗t & u →∗s ⇒ ∃v(t →∗ v & s →∗ v))\n\nConfluent systems are parallelizable and easy to optimize.\n→ — rewriting in 1 step;\n→∗ — rewriting in an arbitrary number of steps,\nfrom 0."
    },
    {
        "question": "15) Merger",
        "answer": "\nTRS is called confluent if for any two terms t, s obtained by rewriting the same term u, there exists a term v such that both terms t, s are rewritten as v.\nFormally:\n∀u, t, s(u →∗t & u →∗s ⇒ ∃v(t →∗ v & s →∗ v))\n\nConfluent systems are parallelizable and easy to optimize.\n→ — 1-stage rewriting;\n→∗ — rewriting in an arbitrary number of steps,\nfrom 0."
    },
    {
        "question": "16) Syntactic monoid",
        "answer": "\nA monoid is an algebraic structure consisting of a set MM with a binary operation ⋅ (multiplication) that:\n1. is associative: (a⋅b)⋅c=a⋅(b⋅c) for any a, b ,c ∈ M;\n2. has an identity element e ∈ M such that for any a ∈ M we have e⋅a=a⋅e =a.\n\nFactorization with respect to language: To construct a syntactic monoid for a language, we first define a factorization relation with respect to the language L⊆Σ∗ (where Σ∗ is the set of all strings over the alphabet Σ).\n\nTwo strings u and v are said to be equivalent with respect to a language L if for all strings x,y ∈ Σ∗ the condition xuy ∈ L is equivalent to xvy ∈ L. This equivalence relation is called syntactic equivalence and is denoted by u ∼_L v.\n\nRelation to Finite Automata:\nThe syntactic monoid can be used to describe regular languages. For any regular language, its syntactic monoid is finite, and the language can be recognized by a finite automaton built on top of this monoid. This means that the syntactic monoid is a powerful tool for analyzing the structure of regular languages ​​and their behavior.\n\nMikan's Theorem: A language is regular if and only if its syntactic monoid is finite.\n\nApplications:\n1. Classification of Regular Languages: The syntactic monoid allows us to describe regular languages ​​in terms of their algebraic structure. This is especially useful in analyzing and classifying languages.\n2. Determining the complexity of languages: The complexity of a syntactic monoid (e.g., the number of its elements) can give an idea of ​​the complexity of describing the language. 3. Minimizing finite automata: Using syntactic monoids, one can construct minimal finite automata that efficiently recognize regular languages."
    },
    {
        "question": "16) Syntactic monoid",
        "answer": "\nA monoid is an algebraic structure consisting of a set MM with a binary operation ⋅ (multiplication) that:\n1. is associative: (a⋅b)⋅c=a⋅(b⋅c) for any a, b ,c ∈ M;\n2. has an identity element e ∈ M such that for any a ∈ M we have e⋅a=a⋅e =a.\n\nFactorization with respect to a language: To construct a syntactic monoid for a language, we first define a factorization relation with respect to the language L⊆Σ∗ (where Σ∗ is the set of all strings over the alphabet Σ).\n\nTwo strings u and v are said to be equivalent with respect to a language L if for all strings x,y ∈ Σ∗ the condition xuy ∈ L is equivalent to xvy ∈ L. This equivalence relation is called syntactic equivalence and is denoted by u ∼_L v.\n\nRelation to finite automata:\nThe syntactic monoid can be used to describe regular languages. For any regular language, its syntactic monoid is finite, and the language can be recognized by a finite automaton built on top of this monoid. This means that the syntactic monoid is a powerful tool for analyzing the structure of regular languages ​​and their behavior.\n\nMikan's Theorem: A language is regular if and only if its syntactic monoid is finite.\n\nApplications:\n1. Classification of regular languages: The syntactic monoid allows us to describe regular languages ​​in terms of their algebraic structure. This is especially useful in the analysis and classification of languages.\n2. Determining the complexity of languages: The complexity of a syntactic monoid (e.g., the number of its elements) can give an idea of ​​the complexity of describing the language. 3. Minimizing finite automata: Using syntactic monoids, one can construct minimal finite automata that efficiently recognize regular languages."
    },
    {
        "question": "16) Syntactic monoid",
        "answer": "\nA monoid is an algebraic structure consisting of a set MM with a binary operation ⋅ (multiplication) that:\n1. is associative: (a⋅b)⋅c=a⋅(b⋅c) for all a, b, c ∈ M;\n2. has an identity element e ∈ M such that for all a ∈ M we have e⋅a=a⋅e =a.\n\nFactorization with respect to language. To construct a syntactic monoid for a language, we first define a factorization relation with respect to the language L⊆Σ∗ (where Σ∗ is the set of all strings in the alphabet Σ).\n\nTwo strings u and v are said to be equivalent with respect to a language L if for all strings x,y ∈ Σ∗ the condition xuy ∈ L is equivalent to the condition xvy ∈ L. This equivalence relation is called syntactic equivalence and is denoted by u ∼_L v.\n\nRelation to Finite Automata:\nThe syntactic monoid can be used to describe regular languages. For any regular language, its syntactic monoid is finite, and the language can be recognized by a finite automaton built on this monoid. This means that the syntactic monoid is a powerful tool for analyzing the structure of regular languages ​​and their behavior.\n\nMikan's Theorem: A language is regular if and only if its syntactic monoid is finite.\n\nApplications:\n1. Classification of Regular Languages. The syntactic monoid allows us to describe regular languages ​​by their algebraic structure. This is especially useful for analyzing and classifying languages.\n2. Determine the complexity of languages: The complexity of a syntactic monoid (e.g., the number of its elements) can give an idea of ​​the complexity of describing the language. 3. Minimize finite automata: Using syntactic monoids, one can construct minimal finite automata that efficiently recognize regular languages."
    },
    {
        "question": "16) Syntactic monoid",
        "answer": "\nA monoid is an algebraic structure consisting of a set MM with a binary operation ⋅ (multiplication) that:\n1. is associative: (a⋅b)⋅c=a⋅(b⋅c) for all a, b, c ∈ M;\n2. has an identity element e ∈ M such that for all a ∈ M we have e⋅a=a⋅e =a.\n\nLanguage factorization. To construct a syntactic monoid for a language, we first define a language factorization relation L⊆Σ∗ (where Σ∗ is the set of all strings in the alphabet Σ).\n\nTwo strings u and v are said to be equivalent with respect to a language L if for all strings x,y ∈ Σ∗ the condition xuy ∈ L is equivalent to xvy ∈ L. This equivalence relation is called syntactic equivalence and is denoted by u ∼ _Л в.\n\nRelation to finite automata:\nThe syntactic monoid can be used to describe regular languages. For any regular language, its syntactic monoid is finite, and the language can be recognized by a finite automaton built on top of this monoid. This means that the syntactic monoid is a powerful tool for analyzing the structure of regular languages ​​and their behavior.\n\nMikan's theorem: A language is regular if and only if its syntactic monoid is finite.\n\nApplications:\n1. Classification of regular languages. The syntactic monoid allows us to describe regular languages ​​in terms of their algebraic structure. This is especially useful for analyzing and classifying languages.\n2. Determine the complexity of languages: The complexity of a syntactic monoid (e.g., the number of its elements) can give an idea of ​​the difficulty of describing the language. 3. Minimize finite automata: Using syntactic monoids, we can construct minimal finite automata that efficiently recognize regular languages."
    },
    {
        "question": "17) Construction of a syntactic monoid",
        "answer": "\nA syntactic monoid can be constructed as follows:\n1. Set of equivalence classes: Consider the set of all equivalence classes of strings with respect to syntactic equivalence. Each equivalence class of strings corresponds to one element of the syntactic monoid.\n2. Operation on equivalence classes: Let [u][u] and [v][v] be the equivalence classes of strings uu and vv, respectively. Then the operation in the syntactic monoid is defined as [u]⋅[v]=[uv], where uv is the concatenation of the strings u and v.\n3. Identity element: The identity element of the syntactic monoid is the equivalence class of the empty string ϵ, which is usually denoted as [ϵ].\n\nThus, a syntactic monoid consists of all equivalence classes of strings with respect to syntactic equivalence with the operation of string concatenation."
    },
    {
        "question": "17) Construction of a syntactic monoid",
        "answer": "\nA syntactic monoid can be constructed as follows:\n1. A set of equivalence classes: Consider the set of all equivalence classes of strings with respect to syntactic equivalence. Each equivalence class of strings corresponds to one element of the syntactic monoid.\n2. An operation on equivalence classes: Let [u][u] and [v][v] be the equivalence classes of strings uu and vv, respectively. Then the operation on the syntactic monoid is defined as [u]⋅[v]=[uv], where uv is the concatenation of the strings u and v.\n3. An identity element: The identity element of a syntactic monoid is the equivalence class of the empty string ϵ, which is usually denoted by [ϵ].\n\nThus, a syntactic monoid consists of all equivalence classes of strings with respect to syntactic equivalence with the operation of string concatenation."
    },
    {
        "question": "17) Construction of a syntactic monoid",
        "answer": "\nA syntactic monoid can be constructed as follows:\n1. A set of equivalence classes. Consider the set of all string equivalence classes with respect to syntactic equivalence. Each string equivalence class corresponds to an element of the syntactic monoid.\n2. An operation on equivalence classes. Let [u][u] and [v][v] be the equivalence classes of the chains uu and vv, respectively. Then the operation in the syntactic monoid is defined as [u]⋅[v]=[uv], where uv is the union of the strings u and v.\n3. An identity element. The identity element of a syntactic monoid is the equivalence class of the empty string ϵ, which is usually denoted by [ϵ].\n\nThus, a syntactic monoid consists of all equivalence classes of strings with respect to syntactic equivalence with the operation of string concatenation."
    },
    {
        "question": "17) Construction of a syntactic monoid",
        "answer": "\nA syntactic monoid can be constructed as follows:\n1. A set of equivalence classes. Consider the set of all string equivalence classes with respect to syntactic equivalence. Each string equivalence class corresponds to an element of the syntactic monoid.\n2. An operation on equivalence classes. Let [u][u] and [v][v] be the equivalence classes of the strings uu and vv, respectively. Then the operation in the syntactic monoid is defined as [u]⋅[v]=[uv], where uv is the union of the strings u and v.\n3. An identity element. The identity element of a syntactic monoid is the equivalence class of the empty string ϵ, which is usually denoted by [ϵ].\n\nThus, a syntactic monoid consists of all equivalence classes of strings with respect to syntactic equivalence with the operation of string concatenation."
    },
    {
        "question": "Grammar in Chomsky normal form is",
        "answer": "\nA grammar in Chomsky normal form is a context-free grammar that can contain rules only of the following form:\nA→BC,\nA→a,\nS→ε,\nwhere:\na is a terminal,\nA,B,C are non-terminals,\nS is the starting node,\nε is an empty string, the starting node is not contained in the right-hand sides of the rules."
    },
    {
        "question": "Grammar in Chomsky normal form:",
        "answer": "\nA grammar in Chomsky normal form is a context-free grammar that can contain rules only of the following form:\nA→BC,\nA→a,\nS→ε,\nwhere:\na is a terminal,\nA,B,C are nonterminals,\nS is the initial node,\nε is the empty string, the initial node is not contained in the right-hand sides of the rules."
    },
    {
        "question": "Chomsky's normal form grammar:",
        "answer": "\nA Chomsky normal form grammar is a context-free grammar that can only contain rules of the following form:\nA → BK,\nA → one,\nS → ε,\nOr:\na is a terminal,\nA, B, C are nonterminals,\nS is a start node,\nε is the empty string, the start node is not contained in the right-hand sides of the rules."
    },
    {
        "question": "The grammar in Chomsky normal form is",
        "answer": "\nA Chomsky normal form grammar is a context-free grammar that can only contain rules of the following form:\nA→BC,\nA→a,\nS→ε,\nOr:\na is a terminal,\nA,B,C are nonterminals,\nS is the initial node,\nε is the empty string, the initial node is not contained in the right-hand sides of the rules."
    },
    {
        "question": "Prove the theorem: Any context-free grammar can be reduced to Chomsky normal form.",
        "answer": "\nConsider a context-free grammar Γ. To reduce it to Chomsky normal form, we need to perform five steps. At each step, we construct a new Γi that admits the same language as Γ.\n\nRemove long rules.\nUse the algorithm for removing long rules from a grammar. We obtain a grammar Γ1 equivalent to the original, containing rules of length 0.1 and 2.\nRemove ε - rules.\nUse the algorithm for removing ε - rules from a grammar. We obtain a grammar Γ2 equivalent to the original, but which does not contain ε - rules.\nRemove chain rules.\nUse the algorithm for removing chain rules from a grammar. The algorithm works in such a way that new ε - rules are not formed. We obtain a grammar Γ3 equivalent to Γ.\nRemove useless symbols.\nLet's use an algorithm for removing useless symbols from the grammar. Since Γ3\nis equivalent to Γ, useless symbols could not cease to be useless. Moreover, we only remove rules, new ε-rules and chain rules could not appear.\nLet's remove situations when a rule contains several terminals.\nFor all rules of the form A→u1u2\n(where ui is a terminal or non-terminal), replace all terminals ui with new non-terminals Ui and add rules Ui→ui.\nNow the rules contain either a single terminal or a string of two non-terminals.\nThus, we have obtained a grammar in Chomsky normal form, which admits the same language as Γ.\n\nIt is worth noting that the order of operations is important. The first rule must be executed before the second, otherwise the normalization time will degrade to O(2|Γ|). The third rule comes after the second, because after removing ε-rules, new chain rules may be formed. Also, the fourth rule must be executed after the third and second, because they may generate useless symbols.\n\nWith this order of actions, the size of the grammar increases polynomially.\n\nAfter removing long rules from each rule of length k⩾3, k−1 new rules could appear, and their length does not exceed two. At this step, the size of the grammar increases no more than twice.\n\nWhen removing ε-rules from a grammar containing rules of length 0.1 and 2, the size of the grammar could grow no more than 3 times.\n\nIn total, there are no more than n2 chain rules in the grammar, where n is the number of nonterminals. When removing chain rules, we take each of the chain pairs and add non-chain rules derived from the second nonterminal in the pair. If the maximum total length of all rules derived from some non-terminal is k, then the size of the grammar will increase by no more than k⋅n2.\n\nFinally, at the last step, no more than |Σ|(Σ is the alphabet of the grammar) new rules can be added, and all of them will be of length 1."
    },
    {
        "question": "Prove the theorem: Any context-free grammar can be reduced to Chomsky normal form.",
        "answer": "\nConsider a context-free grammar Γ. To reduce it to Chomsky normal form, we need to perform five steps. At each step, we construct a new Γi that admits the same language as Γ.\n\nRemove long rules.\nUse the algorithm for removing long rules from a grammar. We obtain a grammar Γ1 equivalent to the original, containing rules of length 0, 1, and 2.\nRemove ε - rules.\nUse the algorithm for removing ε - rules from a grammar. We obtain a grammar Γ2 equivalent to the original, but containing no ε - rules.\nRemove chain rules.\nUse the algorithm for removing chain rules from a grammar. The algorithm works in such a way that new ε - rules are not formed. We obtain a grammar Γ3 equivalent to Γ.\nRemove useless symbols.\nWe use an algorithm for removing useless symbols from the grammar. Since Γ3\nis equivalent to Γ, useless symbols could not cease to be useless. Moreover, we only remove rules, new ε-rules and chain rules could not appear.\nRemove situations when a rule contains several terminals.\nFor all rules of the form A→u1u2\n(where ui is a terminal or non-terminal), replace all terminals ui with new non-terminals Ui and add rules Ui→ui.\nNow the rules contain either a single terminal or a string of two non-terminals.\nThus, we have obtained a grammar in Chomsky normal form, which admits the same language as Γ.\n\nIt is worth noting that the order of operations is important. The first rule must be executed before the second, otherwise the normalization time will be reduced to O(2|Γ|). The third rule follows the second, since removing ε-rules may result in new chain rules. Also, the fourth rule must be executed after the third and second, since they may generate useless symbols.\n\nWith this order of actions, the size of the grammar increases polynomially.\n\nAfter removing long rules, k−1 new rules may appear from each rule of length k⩾3, and their length does not exceed two. At this step, the size of the grammar increases by no more than two times.\n\nWhen removing ε-rules from a grammar containing rules of length 0.1 and 2, the size of the grammar can grow by no more than three times.\n\nIn total, there are no more than n2 chain rules in the grammar, where n is the number of nonterminals. When removing chain rules, we take each pair of chains and add non-chain rules obtained from the second nonterminal in the pair. If the maximum total length of all rules obtained from some nonterminal is k, then the size of the grammar will increase by no more than k⋅n2.\n\nFinally, at the last step no more than |Σ|(Σ is the alphabet of the grammar) new rules can be added, and all of them will have length 1."
    },
    {
        "question": "Prove the theorem: any context-free grammar can be reduced to Chomsky normal form.",
        "answer": "\nConsider a context-free grammar Γ. To reduce it to Chomsky normal form, we need to perform five steps. At each step, we construct a new Γi admitting the same language as Γ.\n\nLet's remove long rules.\nWe use the algorithm to remove long rules from the grammar. We get a grammar Γ1 equivalent to the original, containing rules of length 0, 1, and 2.\nLet's remove ε-rules.\nWe use the algorithm to remove ε-rules from the grammar. We get a grammar Γ2 equivalent to the original, but containing no ε-rules.\nLet's remove chain rules.\nWe use the algorithm to remove string rules from the grammar. The algorithm works in such a way that no new ε-rules are formed. We get a grammar Γ3 equivalent to Γ.\nLet's remove unnecessary symbols.\nWe use an algorithm to remove unnecessary symbols from the grammar. Since Γ3\nare equivalent to Γ, useless symbols could not cease to be useless. Also, we only remove rules; new ε-rules and chain rules cannot appear.\nRemove situations where a rule contains multiple terminals.\nFor all rules of the form A→u1u2\n(where ui is a terminal or a non-terminal), replace all terminals ui with new non-terminals Ui and add rules Ui→ui.\nThe rules now contain either a terminal or a chain of two non-terminals.\nThus, we have a grammar with Chomsky normal form that admits the same language as Γ.\n\nNote that the order of operations is important. The first rule must be executed before the second, otherwise the normalization time will be reduced to O(2|Γ|). The third rule follows the second, since new chain rules can be formed after removing ε rules. In addition, the fourth rule should be executed after the third and second, since they can generate unnecessary symbols.\n\nThis procedure increases the size of the grammar polynomially.\n\nAfter removing long rules, each rule of length k⩾3 may produce k−1 new rules whose length does not exceed two. At this stage, the size of the grammar increases by no more than a factor of two.\n\nWhen removing ε rules from a grammar containing rules of length 0, 1, and 2, the size of the grammar cannot increase by more than a factor of three.\n\nThere are no more than n2 chain rules in the grammar, where n is the number of nonterminals. When removing linked rules, we take each of the linked pairs and add unlinked rules obtained from the second nonterminal of the pair. If the maximum total length of all rules obtained from a nonterminal is k, then the size of the grammar will not increase by more than k⋅n2.\n\nFinally, in the last step, at most |Σ|(Σ is the grammatical alphabet) new rules can be added, and they will all have length 1."
    },
    {
        "question": "Prove the theorem: any grammar without context can be reduced to Chomsky normal form.",
        "answer": "\nConsider an a-context grammar Γ. To reduce it to Chomsky normal form, we need to perform five steps. At each step, we construct a new Γi admitting the same language as Γ.\n\nRemove long rules.\nUse the algorithm for removing long rules from a grammar. We obtain a grammar Γ1 equivalent to the original, containing rules of length 0.1 and 2.\nRemove ε - rules.\nUse the algorithm for removing ε - rules from a grammar. We obtain a grammar Γ2 equivalent to the original, but containing no ε - rules.\nRemove channel rules.\nUse the algorithm for removing chains of rules from a grammar. The algorithm works in such a way that new ε-rules are not formed. We obtain a grammar Γ3 equivalent to Γ.\nRemove unnecessary symbols.\nUse the algorithm for removing unnecessary symbols from a grammar. Since Γ3\nis equivalent to Γ, useless symbols cannot cease to be useless. In this case, we only remove rules, new ε-rules and chain rules cannot appear.\nLet us remove the cases when a rule contains multiple endpoints.\nFor all rules of the form A→u1u2\n(where ui is a terminal or a non-terminal), let us replace all terminals ui with new non-terminals Ui and add rules Ui→ui.\nNow the rules contain either a single terminal or a chain of two non-terminals.\nThus, we have a grammar in Chomsky normal form that admits the same language as Γ.\n\nNote that the order of operations is important. The first rule must be executed before the second, otherwise the normalization time will degrade to O(2|Γ|). The third rule follows the second, since new chain rules can be formed after removing ε-rules. In addition, the fourth rule should be executed after the third and second, since they can generate unnecessary symbols.\n\nWith this order of actions, the size of the grammar increases polynomially.\n\nAfter removing long rules, each rule of length k⩾3 may produce k−1 new rules whose length does not exceed two. At this stage, the size of the grammar increases by no more than a factor of two.\n\nWhen removing ε-rules from a grammar containing rules of length 0.1 and 2, the size of the grammar cannot increase by more than a factor of three.\n\nIn total, the grammar has no more than n2 chain rules, where n is the number of nonterminals. When removing chain rules, we take each pair of strings and add rules not related to the chain, obtained from the second nonterminal of the pair. If the maximum total length of all rules obtained from a nonterminal is k, then the size of the grammar will not increase by more than k⋅n2.\n\nFinally, in the last step, no more |Σ|(Σ is the grammatical alphabet) new rules can be added, and they will all have length 1."
    },
    {
        "question": "Grammar in weakened normal form Greibach",
        "answer": "\nA grammar in Greibach weak normal form is a context-free grammar that can contain only rules of one of the following types:\nA→aγ\nS→ε\nwhere a is a terminal, A is a nonterminal (possibly a starting one), S is a starting nonterminal (and it should not appear in the right-hand sides of the rules), ε is the empty string, γ is a string of an arbitrary number of terminals, and a nonterminal"
    },
    {
        "question": "Grammar in weakened normal form Greibach",
        "answer": "\nA grammar in Greibach weak normal form is a context-free grammar that can only contain rules of one of the following types:\nA→aγ\nS→ε\nwhere a is a terminal, A is a nonterminal (possibly an initial one), S is an initial nonterminal (and it must not appear on the right-hand side of rules), ε is the empty string, γ is a string of any number of terminals, and the nonterminal"
    },
    {
        "question": "Weakened Grammar of Greibach Normal Form",
        "answer": "\nA weak Greibach normal form grammar is a context-free grammar that can contain rules of only one of the following types:\nA→aγ\nC → ε\nwhere a is a terminal, A is a nonterminal (possibly the first), S is the initial nonterminal (and it must not appear to the right of the rules), ε is the empty string, γ is a string, an arbitrary number of terminals, and a nonterminal"
    },
    {
        "question": "Grammar in Greibach's Weak Normal Form",
        "answer": "\nA Greibach weak normal form grammar is a context-free grammar that can contain rules of only one of the following types:\nA→aγ\nC→ε\nwhere a is a terminal, A is a nonterminal (possibly a starting terminal), S is a starting nonterminal (and it must not appear on the right-hand side of rules), ε is the empty string, γ is a string of any number of terminals and one nonterminal"
    },
    {
        "question": "Prove the terminal deletion lemma:",
        "answer": "\nAssign each terminal a a new symbol a′, which is not in N∪Σ, such that a′≠b′ for different terminals a and b.\n\nLet N′=N∪{a′∣a∈Σ}\n\nLet α=x1x2…xn be a part of the rule, then α′=y1y2…yn, where yi={xix′i; xi∈N; xi∈Σ for 1⩽i⩽n.\n\nLet us construct a grammar G′=⟨Σ,N′,S,P′⟩, where P′={α′→β′∣α→β∈P}∪{a′→a∣a∈Σ}\n\nLet us show that L(Γ′)=L(Γ)\n\nLet w∈L(Γ)\nThen in Γ there exists a derivation S=w0⇒w1⇒…⇒wn⇒w\n\nBy the construction of P′, in Γ′ there exists a derivation S=w′0⇒w′1⇒w′2⇒…⇒w′n=v0⇒v1⇒v2⇒…⇒vm=w\n\nFor 0⩽i⩽n−1 in transitions w′i⇒w′i+1 we use the rule α′→β′, since the rule α→β was used in the derivation of wi⇒wi+1\n\nFor 0⩽j⩽m−1 in transitions vj⇒vj+1 we use rules of the form a′→a\n\nWe replace the symbols allowed in w′ with new ones and obtain that w∈L(Γ′)\nThen L(Γ)⊆L(Γ′)\n\nLet x∈L(Γ′). Then in Γ′ there is an inference S⇒∗x\nWe can change the order of applying the rules in this inference: first we apply only the rules of the form α′→β′, and then only the rules of the form a′→a.\n\nFrom the construction: after applying a rule of the form a′→a, the resulting a cannot be used when applying rules from P′\n\nChanging the order of inference does not change the language, that is, in Γ′ there is an inference: S=x′0⇒x′1⇒…⇒x′r⇒x′⇒y1⇒y2⇒…⇒ys=x, where for 0⩽i⩽r−1 x′i+1∈(N′)∗ and in the transition x′i→x′i+1, the inference rule α′→β′ was used and for 1⩽j⩽s, the rule a′→a was used to obtain yj→yj+1\n\nWe obtain an inference in Γ: S=x0⇒x1⇒…⇒xn=x\n\nThen L(Γ′)⊆L(Γ)\n\nThus, L(Γ′)=L(Γ)\n\nObviously, if the grammar was non-shortening, it will remain so."
    },
    {
        "question": "Prove the terminal deletion lemma:",
        "answer": "\nAssign to each terminal a a new symbol a′ that is not in N∪Σ such that a′≠b′ for distinct terminals a and b.\n\nLet N′=N∪{a′∣a∈Σ}\n\nLet α=x1x2…xn be part of the rule, then α′=y1y2…yn, where yi={xix′i; xi∈N; xi∈Σ for 1⩽i⩽n.\n\nLet us construct a grammar G′=⟨Σ,N′,S,P′⟩, where P′={α′→β′∣α→β∈P}∪{a′→a∣a∈Σ}\n\nLet us show that L(Γ′)=L(Γ)\n\nLet w∈L(Γ)\nThen in Γ there exists a derivation S=w0⇒w1⇒…⇒wn⇒w\n\nBy the construction of P′, in Γ′ there exists a derivation S=w′0⇒w′1⇒w′2⇒…⇒w′n=v0⇒v1⇒v2⇒…⇒vm=w\n\nFor 0⩽i⩽n−1 in transitions w′i⇒w′i+1 we use the rule α′→β′, since the rule α→β was used in the derivation of wi⇒wi+1\n\nFor 0⩽j⩽m−1 in transitions vj⇒vj+1 we use rules of the form a′→a\n\nWe replace the symbols allowed in w′ with new ones and obtain that w∈L(Γ′)\nThen L(Γ)⊆L(Γ′)\n\nLet x∈L(Γ′). Then Γ′ has an inference S⇒∗x\nWe can change the order of applying the rules in this inference: first we apply only rules of the form α′→β′, and then only rules of the form a′→a.\n\nFrom the construction: after applying a rule of the form a′→a, the resulting a cannot be used when applying rules from P′\n\nChanging the order of inference does not change the language, that is, in Γ′ there is an inference: S=x′0⇒x′1⇒…⇒x′r⇒x′⇒y1⇒y2⇒…⇒ys=x, where for 0⩽i⩽r−1 x′i+1∈(N′)∗ and for the transition x′i→x′i+1, the inference rule α′→β′ was used, and for 1⩽j⩽s, the rule a′→a was used to obtain yj→yj+1\n\nWe obtain an inference in Γ: S=x0⇒x1⇒…⇒xn=x\n\nThen L(Γ′)⊆L(Γ)\n\nThus, L(Γ′)=L(Γ)\n\nObviously, if the grammar was not abbreviated, it will remain so."
    },
    {
        "question": "Prove the terminal deletion lemma:",
        "answer": "\nAssign to each terminal a a new symbol a' that is not in N∪Σ, such that a'≠b' for distinct terminals a and b.\n\nLet N′=N∪{a′∣aεΣ}\n\nLet α=x1x2…xn be part of the rule, then α′=y1y2…yn, where yi={xix′i; ксеN; xiεΣ for 1⩽i⩽n.\n\nLet us construct a grammar G′=⟨Σ,N′,S,P′⟩, where P′={α′→β′∣α→βاP}∪{a′→a∣aاΣ}\n\nLet us show that L(Γ′)=L(Γ)\n\nLet wεL(Γ)\nThen Γ has a derivation S=w0⇒w1⇒…⇒wn⇒w\n\nBy the construction of P′, Γ′ has a derivation S=w′0⇒w′1⇒w′2⇒…⇒w′n=v0⇒v1⇒v2⇒…⇒vm=w\n\nFor 0⩽i⩽n−1 in transitions w′i⇒w′i+1 we use the rule α′→β′, since the rule α→β was used in the derivation of wi⇒wi+1\n\nFor 0⩽j⩽m−1 in transitions vj⇒vj+1 we use rules of the form a′→a\n\nWe replace the allowed symbols in w′ with new ones and obtain that wεL(Γ′)\nThen L(Γ)⊆L(Γ′)\n\nLet xεL(Γ′). Then in Γ′ there is a derivation S⇒∗x\nWe can change the order of applying the rules in this derivation: first we apply only the rules of the form α′→β′, then only the rules of the form a′→a.\n\nFrom the construction: after applying a rule of the form a′→a, the resulting a cannot be used when applying the rules P′\n\nChanging the order of inference does not change the language, i.e. in Γ′ the conclusion holds: S=x′0⇒x′1⇒…⇒x′r⇒x′⇒y1⇒ y2⇒…⇒ys=x , where for 0⩽i⩽ r−1 x′i+1ا(N′)∗ and for the transition x′i→x′i+1 the inference rule α′→β′ was used, and for 1⩽j⩽s the rule a′ →a was used to obtain yj→yj+1\n\nWe obtain the conclusion in Γ: S=x0⇒x1⇒…⇒xn=x\n\nThen L(Γ′)⊆L(Γ)\n\nThus, L(Γ′)=L(Γ)\n\nObviously, if the grammar were integral, it would remain so."
    },
    {
        "question": "Prove the terminal deletion lemma:",
        "answer": "\nAssign to each terminal a a new symbol a' that is not in N∪Σ, such that a'≠b' for distinct terminals a and b.\n\nLet N′=N∪{a′∣aεΣ}\n\nLet α=x1x2…xn be part of the rule, then α′=y1y2…yn, where yi={xix′i; ксеN; xiεΣ for 1⩽i⩽n.\n\nLet us construct a grammar G′=⟨Σ,N′,S,P′⟩, where P′={α′→β′∣α→βاP}∪{a′→a∣aاΣ}\n\nLet us show that L(Γ′)=L(Γ)\n\nLet wεL(Γ)\nThen in Γ there exists a derivation S=w0⇒w1⇒…⇒wn⇒w\n\nBy the construction of P′ in Γ′ there exists a derivation S=w′0⇒w′1⇒w′2⇒…⇒w′n=v0⇒v1⇒v2⇒…⇒vm=w\n\nFor 0⩽i⩽n−1 in transitions w′i⇒w′i+1 we use the rule α′→β′, since the rule α→β was used in the derivation of wi⇒wi+1\n\nFor 0⩽j⩽m−1 in transitions vj⇒vj+1 we use rules of the form a′→a\n\nReplacing the allowed symbols in w′ with new ones we get that wεL(Γ′)\nThen L(Γ)⊆L(Γ′)\n\nLet xεL(Γ′). Then in Γ′ there is a derivation S⇒∗x\nWe can change the order of applying the rules in this derivation: first we apply only the rules of the form α’→β’, then only the rules of the form a’→a.\n\nFrom the construction: after applying a rule of the form a′→a, the result a cannot be used when applying the rules P′\n\nChanging the order of inference does not change the language, that is, in Γ′ there is an inference: S=x′0⇒x′1⇒…⇒x′r⇒x′⇒y1 ⇒y2⇒…⇒ys =x, where for 0⩽i⩽r−1 x′i+1ا(N′)∗ and for the transition x′i→x′i+1, the inference rule α′→β′ was used, and for 1⩽j⩽s, the rule a′ →a was used to obtain yj→yj+1\n\nWe obtain the inference in Γ: S=x0⇒x1⇒…⇒xn=x\n\nThen L(Γ′)⊆L(Γ)\n\nThus, L(Γ′)=L(Γ)\n\nObviously, if the grammar was not abbreviated, it will remain so."
    },
    {
        "question": "Lemma (on the removal of short rules) proof",
        "answer": "\nFirst, given Γ, we construct a grammar Γ′′=⟨Σ,N′′,S,P′′⟩, as in the proof of Lemma 1. Given Γ′′, we construct a grammar Γ′ in which:\n\nN′=N′′∪{D}, where D is a new symbol, P′ is obtained from P′′ by replacing all rules of the form α→β∈P′′, where |α|>|β|, with rules of the form α→βD|α|−|β|, and adding the rule D→ε.\nNow all rules in P′ have the required form.\n\nLet us show that L(Γ′)=L(Γ)\n\nNote that replacing the rule α→β with α→βD|α|−|β| does not change the language of the grammar, because D is only mapped to ε, and there are no other rules for D.\n\nThen we get that L(Γ)⊆L(Γ′), similarly, the reverse changes do not change the language, that is, L(Γ′)⊆L(Γ)."
    },
    {
        "question": "Lemma (on the removal of short rules) proof",
        "answer": "\nFirst, given Γ, we construct a grammar Γ′′=⟨Σ,N′′,S,P′′⟩, as in the proof of Lemma 1. Given Γ′′, we construct a grammar Γ′ in which:\n\nN′=N′′∪{D}, where D is a new symbol, P′ is obtained from P′′ by replacing all rules of the form α→β∈P′′, where |α|>|β|, with rules of the form α→βD|α|−|β|, and adding the rule D→ε.\nNow all rules in P′ have the required form.\n\nLet us show that L(Γ′)=L(Γ)\n\nNote that replacing the rule α→β with α→βD|α|−|β| does not change the language of the grammar, since D is mapped only to ε, and there are no other rules for D.\n\nThen we get that L(Γ)⊆L(Γ′), similarly, the reverse changes do not change the language, i.e. L(Γ′)⊆L(Γ)."
    },
    {
        "question": "Proof of the lemma (on removing short rules)",
        "answer": "\nFirst, for a given Γ, we construct a grammar Γ''=⟨Σ,N'',S,P''⟩, as in the proof of Lemma 1. For a given Γ'', we construct a grammar Γ' in which:\n\nN′=N′′∪{D}, where D is a new symbol, P′ is obtained from P′′ by replacing all rules of the form α→βاP′′, where |α|>β| , with rules of the form α→ βD|α|−|β| and adding the rule D→ε.\nNow all rules of P' have the required form.\n\nLet us show that L(Γ′)=L(Γ)\n\nNote that replacing the rule α→β with the rule α→βD|α|−|β| does not change the language of the grammar, since D corresponds only to ε and there are no other rules for D.\n\nThen we get that L(Γ)⊆L(Γ′), and also the inverse replacements do not change the language, i.e. L(Γ′)⊆L(Γ)."
    },
    {
        "question": "Proof of the lemma (on removing short rules)",
        "answer": "\nFirst, given Γ, we construct a grammar Γ''=⟨Σ,N'',S,P''⟩, as in the proof of Lemma 1. Given Γ'', we construct a grammar Γ ' in which:\n\nN′=N′′∪{D}, where D is a new symbol, P′ is obtained from P′′ by replacing all rules of the form α→βاP′′, where |α|>β| , with rules of the form α→βD|α|−|β| and adding the rule D→ε.\nNow all rules of P' have the required form.\n\nLet us show that L(Γ′)=L(Γ)\n\nNote: replacing the rule α→β with α→βD|α|−|β| does not change the language of the grammar, since D is mapped only to ε, and there are no other rules for D.\n\nThen we get that L(Γ)⊆L(Γ′), similarly the reverse substitutions do not change the language, i.e. L(Γ′)⊆L(Γ)."
    },
    {
        "question": "Grammar order",
        "answer": "\nFor any grammar Γ=⟨Σ,N,S,P⟩ of order n⩾3 such that: any rule from P′ has the form α→β, where α∈(N′)+ and β∈(N′)+ and |α|⩽|β| or A→a or A→ε, where A∈N′ and a∈T, a grammar Γ′=⟨Σ,N′,S,P′⟩ of order n−1 can be constructed such that L(Γ′)=L(Γ)"
    },
    {
        "question": "Grammatical order",
        "answer": "\nFor any grammar Γ=⟨Σ,N,S,P⟩ of order n⩾3 such that: any rule in P′ has the form α→β, where α∈(N′)+ and β∈(N′)+ and |α|⩽|β| or A→a or A→ε, where A∈N′ and a∈T, the grammar Γ′=⟨Σ,N′,S,P′⟩ of order n−1 can be constructed such that L(Γ′)=L(Γ)"
    },
    {
        "question": "Grammatical order",
        "answer": "\nFor any grammar Γ=⟨Σ,N,S,P⟩ of order n⩾3 such that: each rule P′ has the form α→β, where αε(N′)+ and βε(N′) + and |α|⩽|β| or A→a or A→ε, where AεN′ and aεT, one can construct a grammar Γ′=⟨Σ,N′,S,P′⟩ of order n−1 such that L(Γ′ ) = L(Γ)"
    },
    {
        "question": "Grammatical order",
        "answer": "\nFor any grammar Γ=⟨Σ,N,S,P⟩ of order n⩾3 such that: each rule P′ has the form α→β, where αε(N′)+ and βε(N′) + and |α|⩽|β| or A→a or A→ε, where AεN′ and aεT, one can construct a grammar Γ′=⟨Σ,N′,S,P′⟩ of order n−1 such that L(Γ′ ) =L(Γ)"
    },
    {
        "question": "Instant description",
        "answer": "\nAn instantaneous description is a set ⟨q,α,γ⟩, where q is the current state, α is the remainder of the string, and γ is the stack contents."
    },
    {
        "question": "Instant description",
        "answer": "\nAn instantaneous description is a tuple ⟨q,α,γ⟩, where q is the current state, α is the remainder of the string, and γ is the stack contents."
    },
    {
        "question": "Instant description",
        "answer": "\nA snapshot description is a tuple ⟨q,α,γ⟩, where q is the current state, α is the remainder of the row, and γ is the stack contents."
    },
    {
        "question": "Instant description",
        "answer": "\nAn instantaneous description is a tuple ⟨q,α,γ⟩, where q is the current state, α is the remainder of the string, and γ is the stack contents."
    },
    {
        "question": "Deterministic pushdown automaton",
        "answer": "\nA deterministic pushdown automaton is a pushdown automaton for which the following conditions are satisfied:\n8q∈Q,a∈Σ∪{ε},X∈Γ⇒δ(q,a,X) has at most one element — δ:Q×Σ∪{ε}×Γ→Q×Γ∗\nIf δ(q,a,X) is nonempty for some a∈Σ, then δ(q,ε,X) must be empty."
    },
    {
        "question": "Deterministic pushdown automaton",
        "answer": "\nA deterministic pushdown automaton is a pushdown automaton for which the following conditions are satisfied:\n8q∈Q,a∈Σ∪{ε},X∈Γ⇒δ(q,a,X) has at most one element — δ:Q×Σ∪{ε}×Γ→Q×Γ∗\nIf δ(q,a,X) is nonempty for some a∈Σ, then δ(q,ε,X) must be empty."
    },
    {
        "question": "Deterministic Automatic Store",
        "answer": "\nA deterministic pushdown automaton is a pushdown automaton for which the following conditions are satisfied:\n8qεQ,aεΣ∪{ε},XεΓ⇒δ(q,a,X) has at most one element - δ:Q×Σ∪{ε}×Γ→Q×Γ∗\nIf δ(q,a,X) is not empty for some aεΣ, then δ(q,ε,X) must be empty."
    },
    {
        "question": "Deterministic feedback automaton",
        "answer": "\nA deterministic battery automaton is a battery automaton for which the following conditions are satisfied:\n8qεQ,aεΣ∪{ε},XεΓ⇒δ(q,a,X) has at most one element — δ:Q×Σ∪{ε}×Γ→Q×Γ∗\nIf δ(q,a,X) is not empty for some aεΣ, then δ(q,ε,X) must be empty."
    },
    {
        "question": "Deterministic pushdown automaton, empty stack tolerance",
        "answer": "\nWe define a deterministic pushdown automaton accepting by empty stack (PDA accepting by empty stack) as a deterministic pushdown automaton that does not have a set T of accepting states. The automaton terminates its operation as soon as the stack becomes empty.\nWe define for it a set of accepting words N={ω∣(q0,a0,Z0)⊢∗(p,ϵ,ϵ)}, where p is an arbitrary state."
    },
    {
        "question": "Deterministic pushdown automaton, empty stack tolerance",
        "answer": "\nWe define a deterministic pushdown automaton accepting an empty stack (PDA accepting an empty stack) as a deterministic pushdown automaton that does not have a set T of accepting states. The automaton terminates as soon as the stack becomes empty.\nWe define for it a set of accepting words N={ω∣(q0,a0,Z0)⊢∗(p,ϵ,ϵ)}, where p is an arbitrary state."
    },
    {
        "question": "Deterministic warehouse automation, low battery tolerance",
        "answer": "\nWe define a stack-accepting deterministic automaton (a stack-accepting PDA) as a stack-accepting deterministic automaton that does not have a set T of acceptor states. The machine terminates as soon as the accumulator is discharged.\nTo do this, we define a set of acceptor words N={ω∣(q0,a0,Z0)⊢∗(p,ϵ,ϵ)}, where p is an arbitrary state."
    },
    {
        "question": "Deterministic stack automaton, empty stack tolerance",
        "answer": "\nWe define an accepting deterministic automaton with an empty stack (PDA accepting an empty stack) as a deterministic automaton with an empty stack that does not have a set T of accepting states. The automaton terminates its operation as soon as the battery is discharged.\nWe define for it a set of accepting words N={ω∣(q0,a0,Z0)⊢∗(p,ϵ,ϵ)}, where p is an arbitrary state."
    },
    {
        "question": "The right context is",
        "answer": "\nThe right context C_R_L(y) of a word y in a language L is the set {z∣yz∈L}"
    },
    {
        "question": "Correct context:",
        "answer": "\nThe correct context C_R_L(y) of a word y in a language L is the set {z∣yz∈L}"
    },
    {
        "question": "Correct context:",
        "answer": "\nThe correct context C_R_L(y) of a word y in a language L is the set {z∣yzاL}"
    },
    {
        "question": "Correct context",
        "answer": "\nThe correct context C_R_L(y) of a word y in a language L is the set {z∣yzاL}"
    },
    {
        "question": "Left context is",
        "answer": "\nThe left context C_L_L(y) of a word y in a language L is the set {z∣zy∈L}"
    },
    {
        "question": "Left context",
        "answer": "\nThe left context C_L_L(y) of a word y in a language L is the set {z∣zy∈L}"
    },
    {
        "question": "Context on the left",
        "answer": "\nThe left context C_L_L(y) of a word y in a language L is the set {z∣zyاL}"
    },
    {
        "question": "Left context",
        "answer": "\nThe left context C_L_L(y) of a word y in a language L is the set {z∣zyاL}"
    },
    {
        "question": "Two-way context is",
        "answer": "\nA two-sided context C_L(y) of a word y in a language L is the set {⟨x,z⟩∣xyz∈L}"
    },
    {
        "question": "Two-way context",
        "answer": "\nThe two-sided context C_L(y) of a word y in a language L is the set {⟨x,z⟩∣xyz∈L}"
    },
    {
        "question": "Bidirectional context",
        "answer": "\nThe bidirectional context C_L(y) of a word y in a language L is the set {⟨x,z⟩∣xyzاL}"
    },
    {
        "question": "Bidirectional context",
        "answer": "\nThe two-way context C_L(y) of a word y in a language L is the set {⟨x,z⟩∣xyzاL}"
    },
    {
        "question": "4. Design of Glushkov's machine gun.",
        "answer": "\nAlgorithm for constructing Glushkov(r)\n1. Build a linearized version of r: r(Lin)=Linearize(r).\n2. Find First{r(Lin)}, Last{r(Lin)}, and [Follow{r(Lin)}(q)] for all q ∈ Σ{r(Lin)}.\nAll states of the automaton, except the initial one (let's call it S),\ncorrespond to the letters q ∈ Σ{r(Lin)}.\n3. From the initial state, we construct transitions to those states for which q ∈ First{r(Lin)}. The transitions are of the form S → q by q.\n4. Transitions from state q correspond to elements d of the set [Follow{r(Lin)}(q)] and are of the form\nq → d by d.\n5. Final states are such that q ∈ Last{r(Lin)}, and also S if ε ∈ L(R).\n6. Now we erase the marking constructed by linearization on the transitions of the automaton. The construction is complete."
    },
    {
        "question": "4. Design of the Glushkov machine gun.",
        "answer": "\nGlushkov(r) construction algorithm\n1. Construct a linearized version of r: r(Lin)=Linearize(r).\n2. Find First{r(Lin)}, Last{r(Lin)}, and [Follow{r(Lin)}(q)] for all q ∈ Σ{r(Lin)}.\nAll states of the automaton, except the initial one (let's call it S),\ncorrespond to the letters q ∈ Σ{r(Lin)}.\n3. From the initial state, construct transitions to those states for which q ∈ First{r(Lin)}. The transitions are of the form S → q by q.\n4. Transitions from state q correspond to elements d of the set [Follow{r(Lin)}(q)] and are of the form\nq → d by d.\n5. The final states are such that q ∈ Last{r(Lin)}, and also S if ε ∈ L(R).\n6. Now we erase the marking constructed by linearization on the transitions of the automaton. The construction is complete."
    },
    {
        "question": "4. Design of the Glushkov machine gun.",
        "answer": "\nGlushkov's algorithm for constructing r\n1. Construct a linearized version of r: r(Lin)=Linearize(r).\n2. Find First{r(Lin)}, Last{r(Lin)}, and [Follow{r(Lin)}(q)] for all q ∈ Σ{r(Lin)}.\nAll states of the automaton except the initial one (let's call it S)\ncorrespond to letters q ∈ Σ{r(Lin)}.\n3. From the initial state, construct transitions to states for which q ∈ First{r(Lin)}. The transitions are of the form S → q by q.\n4. The transitions of state q correspond to elements d of the set [Follow{r(Lin)}(q)] and are of the form\nq → d by d.\n5. The final states are such that q ∈ Last{r(Lin)}, and also S if ε ∈ L(R).\n6. Now we erase the markings constructed by linearization on the machine transitions. The construction site is finished."
    },
    {
        "question": "4. Design of the Glushkov machine gun.",
        "answer": "\nGlushkov's algorithm for constructing r\n1. Construct a linearized version of r: r(Lin)=Linearize(r).\n2. Find First{r(Lin)}, Last{r(Lin)}, and [Follow{r(Lin)}(q)] for all q ∈ Σ{r(Lin)}.\nAll states of the automaton, except the initial state (let's call it S),\ncorrespond to letters q ∈ Σ{r(Lin)}.\n3. From the initial state, we construct transitions to states for which q ∈ First{r(Lin)}. The transitions are of the form S → q by q.\n4. The transitions from state q correspond to elements d of the set [Follow{r(Lin)}(q)] and are of the form\nq → d by d.\n5. The final states are such that q ∈ Last{r(Lin)}, and also S if ε ∈ L(R).\n6. Now we erase the marking constructed by linearization on the transitions of the automaton. The construction is complete."
    }
]